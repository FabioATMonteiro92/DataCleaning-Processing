{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5509bc",
   "metadata": {},
   "source": [
    "## What this notebook does\n",
    "This notebook demonstrates a reproducible **data-processing pipeline** for a study on **time-of-day (ToD) effects in working memory (WM)**. It ingests multi-source data, performs cleaning and harmonization, and outputs analysis-ready tables for downstream statistical modeling.\n",
    "\n",
    "This script produces an excel file, containing multiple sheets/databases. These databases/sheets contain the following information:\n",
    "\n",
    "**1. Screening database**: includes information that was used to evaluate whether participants met the study’s inclusion and exclusion criteria.\n",
    "\n",
    "**2. Sleep Diary database**: includes information that was used to evaluate whether participants complied with the study’s mandatory sleep schedule.\n",
    "\n",
    "**3. Acitivity Diary database**: includes information that was used to evaluate whether participants complied with the study’s mandatory restrictions.\n",
    "\n",
    "**4. Actigraphy database**: \n",
    "\n",
    "**5. WM tasks Practice database**:\n",
    "\n",
    "**6. WM tasks Experimental database**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf633239",
   "metadata": {},
   "source": [
    "# 0 - Imports librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, datetime\n",
    "from pathlib import Path\n",
    "from datetime import timedelta, datetime\n",
    "import time, re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa18e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce803a0",
   "metadata": {},
   "source": [
    "# 1. Screening database\n",
    "\n",
    "The dataset produced includes information that was used to evaluate whether participants met the study’s inclusion and exclusion criteria. \n",
    "\n",
    "It contains:\n",
    "\n",
    "- **Sociodemographic data.**  \n",
    "  *e.g., age, nationality, educational background*\n",
    "- **Morningness–Eveningness Questionnaire (MEQ)** scores.  \n",
    "- **Brief Symptom Inventory (BSI)** scores.\n",
    "\n",
    "---\n",
    "\n",
    "This section:\n",
    "\n",
    "- Loads the **raw screening data** (Excel file + MEQ item files).  \n",
    "- Removes redundant variables.  \n",
    "- Standardizes column names.  \n",
    "- Performs extensive **data cleaning and recoding**.  \n",
    "- Converts numeric categorical codes into **meaningful labels**.  \n",
    "  *(e.g., sex, education level, caffeine/alcohol consumption)*.  \n",
    "- Computes and integrates:  \n",
    "  - **MEQ chronotype classification.**  \n",
    "  - **BSI symptom dimensions.**  \n",
    "  - **Global indice**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d35945",
   "metadata": {},
   "source": [
    "## 1.1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469270a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Opens the excel file where data from participants are stored.\n",
    "# Go up one folder (from /notebook to project root)\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Build the path safely\n",
    "excel_part_data_path = PROJ_ROOT / \"Screening_Data\" / \"data.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "df_excel_Screening = pd.read_excel(excel_part_data_path)\n",
    "##Drops redundant columns (empty columns regarding Q1,Q7,Q14,Q15) in the dataframe df_excel_Screening.\n",
    "df_excel_Screening.drop(df_excel_Screening.columns[[33, *range(104,108)]], axis=1, inplace=True)\n",
    "df_excel_Screening.columns = [\n",
    "    'subject_nr', 'Idade', 'Sexo', 'Nacionalidade', 'FreqEnsSup',\n",
    "    'CicloEstudosEnsSup_1','CicloEstudosEnsSup_2','CicloEstudosEnsSup_3','CicloEstudosEnsSup_4',\n",
    "    'HabAcademicas_1','HabAcademicas_2','HabAcademicas_3','HabAcademicas_4','HabAcademicas_5','HabAcademicas_6',\n",
    "    'Curso','SituacaoLaboral','SituacaoLaboralTurnos_1','SituacaoLaboralTurnos_2',\n",
    "    'PresencaDoencasAnter','ListaDoencasAnter','MedicacaoPsicotropica','ListaMedicacaoPsicotropica',\n",
    "    'Fumador','NrCigarros','ProdutosSessaoTabagica_1','ProdutosSessaoTabagica_2',\n",
    "    'ConsomeCafeina','QuantidadeDiaCafeina','ConsomeAlcool','QuantidadeDiaAlcool','ConsomeDrogas','Viagens',\n",
    "    'Pergunta1','Pergunta2','Pergunta3','Pergunta4','Pergunta5','Pergunta6','Pergunta7','Pergunta8',\n",
    "    'Pergunta9','Pergunta10','Pergunta11','Pergunta12','Pergunta13','Pergunta14','Pergunta15','Pergunta16',\n",
    "    'MEQscore',\n",
    "    *[f'BSI:{i}' for i in range(1,54)],\n",
    "    'TIME_start','TIME_end','TIME_total'\n",
    "]\n",
    "\n",
    "# Sort participants by completion time (earliest to latest)\n",
    "df_excel_Screening.sort_values(by='TIME_end', kind='mergesort', ascending=True, inplace=True)\n",
    "\n",
    "# Reset the index after sorting\n",
    "df_excel_Screening.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display all columns when printing the DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Show the cleaned screening dataset\n",
    "df_excel_Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a11a62",
   "metadata": {},
   "source": [
    "## 1.2. Transformation of raw codes into meaningful attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_Screening[['Pergunta1', 'Pergunta7','Pergunta14','Pergunta15','Sexo','Nacionalidade','FreqEnsSup','CicloEstudosEnsSup_1','CicloEstudosEnsSup_2','CicloEstudosEnsSup_3','CicloEstudosEnsSup_4','HabAcademicas_1','HabAcademicas_2','HabAcademicas_3','HabAcademicas_4','HabAcademicas_5','HabAcademicas_6','SituacaoLaboral','SituacaoLaboralTurnos_1','SituacaoLaboralTurnos_2','PresencaDoencasAnter','MedicacaoPsicotropica','Fumador','ProdutosSessaoTabagica_1','ProdutosSessaoTabagica_2','ConsomeCafeina','ConsomeAlcool','ConsomeDrogas','Viagens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64e491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resolve project root and point to MEQ item files directory\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "meq_dir = PROJ_ROOT / \"Screening_Data\" / \"experiment_data\"\n",
    "\n",
    "# Initialize mapping: filename -> integer response value\n",
    "file_to_value = {}\n",
    "# Read each MEQ *.txt file, extract first token as int, and store in mapping (skip unreadable files)\n",
    "for fp in sorted(meq_dir.glob(\"*.txt\")):\n",
    "    try:\n",
    "        # Plain-text read is most robust for these tiny files\n",
    "        token = fp.read_text(encoding=\"utf-8\", errors=\"ignore\").strip().split()[0]\n",
    "        file_to_value[fp.name] = int(token)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {fp.name}: {e}\")\n",
    "\n",
    "# Replace filenames in MEQ question columns with their numeric answers (fallback to original if missing)\n",
    "for col in ['Pergunta1','Pergunta7','Pergunta14','Pergunta15']:\n",
    "    # Map filenames in the column to their numeric value; keep original if not found\n",
    "    df_excel_Screening[col] = df_excel_Screening[col].map(file_to_value).fillna(df_excel_Screening[col])\n",
    "\n",
    "# Recode sex codes to labels (1=Masculino, 2=Feminino, else=Outro)\n",
    "for i in range(0,len(df_excel_Screening['Sexo'])):\n",
    "        if df_excel_Screening.loc[i,'Sexo'] == 1:\n",
    "            df_excel_Screening.loc[i,'Sexo'] = \"Masculino\"\n",
    "        elif df_excel_Screening.loc[i,'Sexo'] == 2:\n",
    "            df_excel_Screening.loc[i,'Sexo'] = \"Feminino\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'Sexo'] = \"Outro\"\n",
    "\n",
    "# Recode nationality (1=PT, else=Outra)\n",
    "for i in range(0,len(df_excel_Screening['Nacionalidade'])):\n",
    "        if df_excel_Screening.loc[i,'Nacionalidade'] == 1:\n",
    "            df_excel_Screening.loc[i,'Nacionalidade'] = \"PT\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'Nacionalidade'] = \"Outra\"\n",
    "\n",
    "# Recode higher-education attendance (1=Sim, 0/other=Não)\n",
    "for i in range(0,len(df_excel_Screening['FreqEnsSup'])):\n",
    "        if df_excel_Screening.loc[i,'FreqEnsSup'] == 1:\n",
    "            df_excel_Screening.loc[i,'FreqEnsSup'] = \"Sim\"\n",
    "        else :\n",
    "            df_excel_Screening.loc[i,'FreqEnsSup'] = \"Não\"\n",
    "\n",
    "# Collapse one-hot study-cycle columns into a single 'CicloEstudosEnsSup' label\n",
    "CicloEstudosEnsSup = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "        indexx = 0\n",
    "        if df_excel_Screening.loc[i,'CicloEstudosEnsSup_1'] == 1:\n",
    "            aaa = \"Licenciatura\"\n",
    "            CicloEstudosEnsSup.append(aaa)\n",
    "            indexx += 1\n",
    "        elif df_excel_Screening.loc[i,'CicloEstudosEnsSup_2'] == 1:\n",
    "            aaa = \"Mestrado\"\n",
    "            CicloEstudosEnsSup.append(aaa)\n",
    "            indexx += 1\n",
    "        elif df_excel_Screening.loc[i,'CicloEstudosEnsSup_3'] == 1:\n",
    "            aaa = \"Doutoramento\"\n",
    "            CicloEstudosEnsSup.append(aaa)\n",
    "            indexx += 1\n",
    "        elif df_excel_Screening.loc[i,'CicloEstudosEnsSup_4'] == 1:\n",
    "            aaa = \"Outro Ciclo\"\n",
    "            CicloEstudosEnsSup.append(aaa)\n",
    "            indexx +=1\n",
    "        elif indexx == 0:\n",
    "            aaa = \"\"\n",
    "            CicloEstudosEnsSup.append(aaa)\n",
    "\n",
    "# Drop study-cycle one-hot columns\n",
    "df_excel_Screening.drop(columns=['CicloEstudosEnsSup_1','CicloEstudosEnsSup_2','CicloEstudosEnsSup_3','CicloEstudosEnsSup_4'],inplace=True)\n",
    "# Attach consolidated study-cycle label\n",
    "df_excel_Screening['CicloEstudosEnsSup'] = CicloEstudosEnsSup\n",
    "# Move study-cycle column to position 5\n",
    "TransCol = df_excel_Screening.pop('CicloEstudosEnsSup')\n",
    "df_excel_Screening.insert(5,\"CicloEstudosEnsSup\",TransCol)\n",
    "\n",
    "# Derive highest academic qualification based on attendance + cycle indicators\n",
    "HabAcademicas = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "        if df_excel_Screening.loc[i,\"FreqEnsSup\"] == \"Não\" and df_excel_Screening.loc[i,'HabAcademicas_1'] == 1:\n",
    "            aaa = \"Ensino Obrigatório\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Não\" and df_excel_Screening.loc[i,'HabAcademicas_2'] == 1:\n",
    "            aaa = \"Licenciatura\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Não\" and df_excel_Screening.loc[i,'HabAcademicas_3'] == 1:\n",
    "            aaa = \"Pós-graduação\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Não\" and df_excel_Screening.loc[i,'HabAcademicas_4'] == 1:\n",
    "            aaa = \"Mestrado\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Não\" and df_excel_Screening.loc[i,'HabAcademicas_5'] == 1:\n",
    "            aaa = \"Doutoramento\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Não\" and df_excel_Screening.loc[i,'HabAcademicas_6'] == 1:\n",
    "            aaa = \"Nenhuma das opções anteriores se aplica\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Sim\" and df_excel_Screening.loc[i,'CicloEstudosEnsSup'] == \"Licenciatura\":\n",
    "            aaa = \"Ensino Obrigatório\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Sim\" and df_excel_Screening.loc[i,'CicloEstudosEnsSup'] == \"Mestrado\":\n",
    "            aaa = \"Licenciatura\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Sim\" and df_excel_Screening.loc[i,'CicloEstudosEnsSup'] == \"Doutoramento\":\n",
    "            aaa = \"Mestrado\"\n",
    "            HabAcademicas.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, \"FreqEnsSup\"] == \"Sim\" and df_excel_Screening.loc[i,'CicloEstudosEnsSup'] == \"Outro Ciclo\":\n",
    "            aaa = \"Outro Ciclo\"\n",
    "            HabAcademicas.append(aaa)\n",
    "\n",
    "# Drop academic one-hot columns\n",
    "df_excel_Screening.drop(columns=['HabAcademicas_1','HabAcademicas_2','HabAcademicas_3','HabAcademicas_4','HabAcademicas_5','HabAcademicas_6'],inplace=True)\n",
    "# Attach derived academic label\n",
    "df_excel_Screening['HabAcademicas'] = HabAcademicas\n",
    "# Move academic column to position 6\n",
    "TransCol = df_excel_Screening.pop('HabAcademicas')\n",
    "df_excel_Screening.insert(6,\"HabAcademicas\",TransCol)\n",
    "\n",
    "# Recode employment status (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['SituacaoLaboral'])):\n",
    "        if df_excel_Screening.loc[i,'SituacaoLaboral'] == 1:\n",
    "            df_excel_Screening.loc[i,'SituacaoLaboral'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'SituacaoLaboral'] = \"Não\"\n",
    "\n",
    "# Derive shift-work indicator from two binary columns\n",
    "SituacaoLaboralTurnos = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "        if df_excel_Screening.loc[i,'SituacaoLaboralTurnos_1'] == 1:\n",
    "            aaa = \"Sim\"\n",
    "            SituacaoLaboralTurnos.append(aaa)\n",
    "        elif df_excel_Screening.loc[i, 'SituacaoLaboralTurnos_1'] == 0:\n",
    "            aaa = \"Não\"\n",
    "            SituacaoLaboralTurnos.append(aaa)\n",
    "        elif df_excel_Screening.loc[i,'SituacaoLaboralTurnos_2'] == 1 or df_excel_Screening.loc[i,'SituacaoLaboralTurnos_2'] == 0:\n",
    "            aaa = \"Não\"\n",
    "            SituacaoLaboralTurnos.append(aaa)\n",
    "\n",
    "# Drop shift-work source columns\n",
    "df_excel_Screening.drop(columns=['SituacaoLaboralTurnos_1','SituacaoLaboralTurnos_2'],inplace=True)\n",
    "# Attach derived shift-work label and place at column 9\n",
    "df_excel_Screening['SituacaoLaboralTurnos'] = SituacaoLaboralTurnos\n",
    "TransCol = df_excel_Screening.pop('SituacaoLaboralTurnos')\n",
    "df_excel_Screening.insert(9,\"SituacaoLaboralTurnos\",TransCol)\n",
    "\n",
    "# Recode prior disease presence (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['PresencaDoencasAnter'])):\n",
    "        if df_excel_Screening.loc[i,'PresencaDoencasAnter'] == 1:\n",
    "            df_excel_Screening.loc[i,'PresencaDoencasAnter'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'PresencaDoencasAnter'] = \"Não\"\n",
    "\n",
    "# Recode psychotropic medication use (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['MedicacaoPsicotropica'])):\n",
    "        if df_excel_Screening.loc[i,'MedicacaoPsicotropica'] == 1:\n",
    "            df_excel_Screening.loc[i,'MedicacaoPsicotropica'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'MedicacaoPsicotropica'] = \"Não\"\n",
    "\n",
    "# Recode smoking status into descriptive labels\n",
    "for i in range(0,len(df_excel_Screening['Fumador'])):\n",
    "        if df_excel_Screening.loc[i,'Fumador'] == 1:\n",
    "            df_excel_Screening.loc[i,'Fumador'] = \"Sou fumador\"\n",
    "        elif df_excel_Screening.loc[i,'Fumador'] == 2:\n",
    "            df_excel_Screening.loc[i,'Fumador'] = \"Deixei de fumar há menos de 3 meses\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'Fumador'] = \"Não sou fumador nem deixei de fumar há menos de 3 meses\"\n",
    "\n",
    "# Merge two tabacco-session flags into a single labeled column\n",
    "ProdutosSessaoTabagica = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "        if df_excel_Screening.loc[i,'ProdutosSessaoTabagica_1'] == 1:\n",
    "            aaa = \"Sim\"\n",
    "            ProdutosSessaoTabagica.append(aaa)\n",
    "        elif df_excel_Screening.loc[i,'ProdutosSessaoTabagica_2'] == 1:\n",
    "            aaa = \"Não\"\n",
    "            ProdutosSessaoTabagica.append(aaa)\n",
    "        elif df_excel_Screening.loc[i,'ProdutosSessaoTabagica_1'] == 0 and df_excel_Screening.loc[i,'ProdutosSessaoTabagica_2'] == 0:\n",
    "            aaa = None\n",
    "            ProdutosSessaoTabagica.append(aaa)\n",
    "\n",
    "# Drop tobacco-session source columns\n",
    "df_excel_Screening.drop(columns=['ProdutosSessaoTabagica_1','ProdutosSessaoTabagica_2'],inplace=True)\n",
    "\n",
    "# Attach consolidated tobacco-session label and place at column 16\n",
    "df_excel_Screening['ProdutosSessaoTabagica'] = ProdutosSessaoTabagica\n",
    "TransCol = df_excel_Screening.pop('ProdutosSessaoTabagica')\n",
    "df_excel_Screening.insert(16,\"ProdutosSessaoTabagica\",TransCol)\n",
    "\n",
    "# Recode caffeine consumption (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['ConsomeCafeina'])):\n",
    "        if df_excel_Screening.loc[i,'ConsomeCafeina'] == 1:\n",
    "            df_excel_Screening.loc[i,'ConsomeCafeina'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'ConsomeCafeina'] = \"Não\"\n",
    "\n",
    "# Recode alcohol consumption (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['ConsomeAlcool'])):\n",
    "        if df_excel_Screening.loc[i,'ConsomeAlcool'] == 1:\n",
    "            df_excel_Screening.loc[i,'ConsomeAlcool'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'ConsomeAlcool'] = \"Não\"\n",
    "\n",
    "# Recode drug use (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['ConsomeDrogas'])):\n",
    "        if df_excel_Screening.loc[i,'ConsomeDrogas'] == 1:\n",
    "            df_excel_Screening.loc[i,'ConsomeDrogas'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'ConsomeDrogas'] = \"Não\"\n",
    "\n",
    "# Recode recent travel (1=Sim, else=Não)\n",
    "for i in range(0,len(df_excel_Screening['Viagens'])):\n",
    "        if df_excel_Screening.loc[i,'Viagens'] == 1:\n",
    "            df_excel_Screening.loc[i,'Viagens'] = \"Sim\"\n",
    "        else:\n",
    "            df_excel_Screening.loc[i,'Viagens'] = \"Não\"\n",
    "\n",
    "# Final QC preview: key recoded and derived screening fields\n",
    "df_excel_Screening[['Pergunta1', 'Pergunta7','Pergunta14','Pergunta15','Sexo','Nacionalidade','FreqEnsSup','CicloEstudosEnsSup',\n",
    "                    'HabAcademicas','SituacaoLaboral','SituacaoLaboralTurnos','PresencaDoencasAnter','MedicacaoPsicotropica',\n",
    "                    'Fumador','ProdutosSessaoTabagica','ConsomeCafeina','ConsomeAlcool','ConsomeDrogas','Viagens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff74cf8",
   "metadata": {},
   "source": [
    "## 1.3. Calculating MEQ score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute chronotype category for each participant based on MEQ score\n",
    "colChronotype = []\n",
    "for i in range(0,len(df_excel_Screening['MEQscore'])):\n",
    "    if df_excel_Screening.loc[i,'MEQscore'] < 31:\n",
    "        temp_hold_chrono = 'Definitivamente Vespertino'\n",
    "    elif df_excel_Screening.loc[i,'MEQscore'] >=  31 and df_excel_Screening.loc[i,'MEQscore'] <= 42:\n",
    "        temp_hold_chrono = 'Moderadamente Vespertino'\n",
    "    elif df_excel_Screening.loc[i,'MEQscore'] >=  43 and df_excel_Screening.loc[i,'MEQscore'] <= 53:\n",
    "        temp_hold_chrono = 'Intermédio'\n",
    "    elif df_excel_Screening.loc[i,'MEQscore'] >=  54 and df_excel_Screening.loc[i,'MEQscore'] <= 59:\n",
    "        temp_hold_chrono = 'Moderadamente Matutino'\n",
    "    elif df_excel_Screening.loc[i,'MEQscore'] > 59:\n",
    "        temp_hold_chrono = 'Definitivamente Matutino'\n",
    "    colChronotype.append(temp_hold_chrono)\n",
    "\n",
    "# Add chronotype column to the dataframe\n",
    "df_excel_Screening['Cronotipo'] = colChronotype\n",
    "\n",
    "# Move chronotype column to position 40\n",
    "TransCol = df_excel_Screening.pop('Cronotipo')\n",
    "df_excel_Screening.insert(40,\"Cronotipo\",TransCol)\n",
    "\n",
    "# Display chronotype column\n",
    "df_excel_Screening[['Cronotipo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880882d",
   "metadata": {},
   "source": [
    "## 1.4. Calculating symptom dimensions and global indeces for the BSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703a9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############Somatizacao\n",
    "# Compute Somatization factor (mean of selected BSI items) per participant\n",
    "FatorSomatizacao = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_somatizacao = (df_excel_Screening.loc[i,'BSI:2'] + df_excel_Screening.loc[i,'BSI:7'] + df_excel_Screening.loc[i,'BSI:23'] + df_excel_Screening.loc[i,'BSI:29'] + df_excel_Screening.loc[i,'BSI:30'] + df_excel_Screening.loc[i,'BSI:33'] + df_excel_Screening.loc[i,'BSI:37'])/7\n",
    "    FatorSomatizacao.append(temp_somatizacao)\n",
    "\n",
    "# Add Somatizacao column and position it at index 94\n",
    "df_excel_Screening['Somatizacao'] = FatorSomatizacao\n",
    "TransCol = df_excel_Screening.pop('Somatizacao')\n",
    "df_excel_Screening.insert(94,\"Somatizacao\",TransCol)\n",
    "\n",
    "##############FatorObsessoesCompulsoes\n",
    "# Compute Obsessions–Compulsions factor (mean of selected BSI items)\n",
    "FatorObsessoesCompulsoes = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorObsessoesCompulsoes = (df_excel_Screening.loc[i,'BSI:5'] + df_excel_Screening.loc[i,'BSI:15'] + df_excel_Screening.loc[i,'BSI:26'] + df_excel_Screening.loc[i,'BSI:27'] + df_excel_Screening.loc[i,'BSI:32'] + df_excel_Screening.loc[i,'BSI:36'])/6\n",
    "    FatorObsessoesCompulsoes.append(temp_FatorObsessoesCompulsoes)\n",
    "\n",
    "# Add factor column and place it at index 95\n",
    "df_excel_Screening['FatorObsessoesCompulsoes'] = FatorObsessoesCompulsoes\n",
    "TransCol = df_excel_Screening.pop('FatorObsessoesCompulsoes')\n",
    "df_excel_Screening.insert(95,\"FatorObsessoesCompulsoes\",TransCol)\n",
    "\n",
    "##############FatorSensibilidadeInterpessoal\n",
    "# Compute Interpersonal Sensitivity factor (mean of selected BSI items)\n",
    "FatorSensibilidadeInterpessoal = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorSensibilidadeInterpessoal = (df_excel_Screening.loc[i,'BSI:20'] + df_excel_Screening.loc[i,'BSI:21'] + df_excel_Screening.loc[i,'BSI:22'] + df_excel_Screening.loc[i,'BSI:42'])/4\n",
    "    FatorSensibilidadeInterpessoal.append(temp_FatorSensibilidadeInterpessoal)\n",
    "\n",
    "# Add factor column and place it at index 96\n",
    "df_excel_Screening['FatorSensibilidadeInterpessoal'] = FatorSensibilidadeInterpessoal\n",
    "TransCol = df_excel_Screening.pop('FatorSensibilidadeInterpessoal')\n",
    "df_excel_Screening.insert(96,\"FatorSensibilidadeInterpessoal\",TransCol)\n",
    "\n",
    "##############FatorDepressao\n",
    "# Compute Depression factor (mean of selected BSI items)\n",
    "FatorDepressao = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorDepressao = (df_excel_Screening.loc[i,'BSI:9'] + df_excel_Screening.loc[i,'BSI:16'] + df_excel_Screening.loc[i,'BSI:17'] + df_excel_Screening.loc[i,'BSI:18'] + df_excel_Screening.loc[i,'BSI:35'] + df_excel_Screening.loc[i,'BSI:50'])/6\n",
    "    FatorDepressao.append(temp_FatorDepressao)\n",
    "\n",
    "# Add factor column and place it at index 97\n",
    "df_excel_Screening['FatorDepressao'] = FatorDepressao\n",
    "TransCol = df_excel_Screening.pop('FatorDepressao')\n",
    "df_excel_Screening.insert(97,\"FatorDepressao\",TransCol)\n",
    "\n",
    "##############FatorAnsiedade\n",
    "# Compute Anxiety factor (mean of selected BSI items)\n",
    "FatorAnsiedade = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorAnsiedade = (df_excel_Screening.loc[i,'BSI:1'] + df_excel_Screening.loc[i,'BSI:12'] + df_excel_Screening.loc[i,'BSI:19'] + df_excel_Screening.loc[i,'BSI:38'] + df_excel_Screening.loc[i,'BSI:45'] + df_excel_Screening.loc[i,'BSI:49'])/6\n",
    "    FatorAnsiedade.append(temp_FatorAnsiedade)\n",
    "\n",
    "# Add factor column and place it at index 98\n",
    "df_excel_Screening['FatorAnsiedade'] = FatorAnsiedade\n",
    "TransCol = df_excel_Screening.pop('FatorAnsiedade')\n",
    "df_excel_Screening.insert(98,\"FatorAnsiedade\",TransCol)\n",
    "\n",
    "##############FatorHostilidade\n",
    "# Compute Hostility factor (mean of selected BSI items)\n",
    "FatorHostilidade = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorHostilidade = (df_excel_Screening.loc[i,'BSI:6'] + df_excel_Screening.loc[i,'BSI:13'] + df_excel_Screening.loc[i,'BSI:40'] + df_excel_Screening.loc[i,'BSI:41'] + df_excel_Screening.loc[i,'BSI:46'])/5\n",
    "    FatorHostilidade.append(temp_FatorHostilidade)\n",
    "\n",
    "# Add factor column and place it at index 99\n",
    "df_excel_Screening['FatorHostilidade'] = FatorHostilidade\n",
    "TransCol = df_excel_Screening.pop('FatorHostilidade')\n",
    "df_excel_Screening.insert(99,\"FatorHostilidade\",TransCol)\n",
    "\n",
    "##############FatorAnsiedadeFobica\n",
    "# Compute Phobic Anxiety factor (mean of selected BSI items)\n",
    "FatorAnsiedadeFobica = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorAnsiedadeFobica = (df_excel_Screening.loc[i,'BSI:8'] + df_excel_Screening.loc[i,'BSI:28'] + df_excel_Screening.loc[i,'BSI:31'] + df_excel_Screening.loc[i,'BSI:43'] + df_excel_Screening.loc[i,'BSI:47'])/5\n",
    "    FatorAnsiedadeFobica.append(temp_FatorAnsiedadeFobica)\n",
    "\n",
    "# Add factor column and place it at index 100\n",
    "df_excel_Screening['FatorAnsiedadeFobica'] = FatorAnsiedadeFobica\n",
    "TransCol = df_excel_Screening.pop('FatorAnsiedadeFobica')\n",
    "df_excel_Screening.insert(100,\"FatorAnsiedadeFobica\",TransCol)\n",
    "\n",
    "##############FatorIdeacaoParanoide\n",
    "# Compute Paranoid Ideation factor (mean of selected BSI items)\n",
    "FatorIdeacaoParanoide = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorIdeacaoParanoide = (df_excel_Screening.loc[i,'BSI:4'] + df_excel_Screening.loc[i,'BSI:10'] + df_excel_Screening.loc[i,'BSI:24'] + df_excel_Screening.loc[i,'BSI:48'] + df_excel_Screening.loc[i,'BSI:51'])/5\n",
    "    FatorIdeacaoParanoide.append(temp_FatorIdeacaoParanoide)\n",
    "\n",
    "# Add factor column and place it at index 101\n",
    "df_excel_Screening['FatorIdeacaoParanoide'] = FatorIdeacaoParanoide\n",
    "TransCol = df_excel_Screening.pop('FatorIdeacaoParanoide')\n",
    "df_excel_Screening.insert(101,\"FatorIdeacaoParanoide\",TransCol)\n",
    "\n",
    "##############FatorPsicoticismo\n",
    "# Compute Psychoticism factor (mean of selected BSI items)\n",
    "FatorPsicoticismo = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_FatorPsicoticismo = (df_excel_Screening.loc[i,'BSI:3'] + df_excel_Screening.loc[i,'BSI:14'] + df_excel_Screening.loc[i,'BSI:34'] + df_excel_Screening.loc[i,'BSI:44'] + df_excel_Screening.loc[i,'BSI:53'])/5\n",
    "    FatorPsicoticismo.append(temp_FatorPsicoticismo)\n",
    "\n",
    "# Add factor column and place it at index 102\n",
    "df_excel_Screening['FatorPsicoticismo'] = FatorPsicoticismo\n",
    "TransCol = df_excel_Screening.pop('FatorPsicoticismo')\n",
    "df_excel_Screening.insert(102,\"FatorPsicoticismo\",TransCol)\n",
    "\n",
    "##############IGS\n",
    "# Compute Global Severity Index (IGS) as the mean of all 53 BSI items\n",
    "IGS = []\n",
    "for i in range(0,len(df_excel_Screening['subject_nr'])):\n",
    "    temp_IGS = (df_excel_Screening.loc[i,'BSI:1'] +\tdf_excel_Screening.loc[i,'BSI:2'] +\tdf_excel_Screening.loc[i,'BSI:3'] +\tdf_excel_Screening.loc[i,'BSI:4'] +\tdf_excel_Screening.loc[i,'BSI:5'] +\tdf_excel_Screening.loc[i,'BSI:6'] +\tdf_excel_Screening.loc[i,'BSI:7'] +\tdf_excel_Screening.loc[i,'BSI:8'] +\tdf_excel_Screening.loc[i,'BSI:9'] +\tdf_excel_Screening.loc[i,'BSI:10'] + df_excel_Screening.loc[i,'BSI:11'] + df_excel_Screening.loc[i,'BSI:12'] + df_excel_Screening.loc[i,'BSI:13'] +\tdf_excel_Screening.loc[i,'BSI:14'] +\tdf_excel_Screening.loc[i,'BSI:15'] +\tdf_excel_Screening.loc[i,'BSI:16'] +\tdf_excel_Screening.loc[i,'BSI:17'] +\tdf_excel_Screening.loc[i,'BSI:18'] +\tdf_excel_Screening.loc[i,'BSI:19'] +\tdf_excel_Screening.loc[i,'BSI:20'] +\tdf_excel_Screening.loc[i,'BSI:21'] +\tdf_excel_Screening.loc[i,'BSI:22'] +\tdf_excel_Screening.loc[i,'BSI:23'] +\tdf_excel_Screening.loc[i,'BSI:24'] +\tdf_excel_Screening.loc[i,'BSI:25'] +\tdf_excel_Screening.loc[i,'BSI:26'] +\tdf_excel_Screening.loc[i,'BSI:27'] +\tdf_excel_Screening.loc[i,'BSI:28'] +\tdf_excel_Screening.loc[i,'BSI:29'] +\tdf_excel_Screening.loc[i,'BSI:30'] +\tdf_excel_Screening.loc[i,'BSI:31'] +\tdf_excel_Screening.loc[i,'BSI:32'] +\tdf_excel_Screening.loc[i,'BSI:33'] +\tdf_excel_Screening.loc[i,'BSI:34'] +\tdf_excel_Screening.loc[i,'BSI:35'] +\tdf_excel_Screening.loc[i,'BSI:36'] +\tdf_excel_Screening.loc[i,'BSI:37'] +\tdf_excel_Screening.loc[i,'BSI:38'] +\tdf_excel_Screening.loc[i,'BSI:39'] +\tdf_excel_Screening.loc[i,'BSI:40'] +\tdf_excel_Screening.loc[i,'BSI:41'] +\tdf_excel_Screening.loc[i,'BSI:42'] +\tdf_excel_Screening.loc[i,'BSI:43'] +\tdf_excel_Screening.loc[i,'BSI:44'] +\tdf_excel_Screening.loc[i,'BSI:45'] +\tdf_excel_Screening.loc[i,'BSI:46'] +\tdf_excel_Screening.loc[i,'BSI:47'] +\tdf_excel_Screening.loc[i,'BSI:48'] +\tdf_excel_Screening.loc[i,'BSI:49'] +\tdf_excel_Screening.loc[i,'BSI:50'] + df_excel_Screening.loc[i,'BSI:51'] + df_excel_Screening.loc[i,'BSI:52'] + df_excel_Screening.loc[i,'BSI:53'])/53\n",
    "    IGS.append(temp_IGS)\n",
    "\n",
    "# Add IGS column and place it at index 103\n",
    "df_excel_Screening['IGS'] = IGS\n",
    "TransCol = df_excel_Screening.pop('IGS')\n",
    "df_excel_Screening.insert(103,\"IGS\",TransCol)\n",
    "\n",
    "# Round selected BSI factor scores and IGS to two decimals\n",
    "cols_to_round = [\n",
    "    \"Somatizacao\",\n",
    "    \"FatorObsessoesCompulsoes\",\n",
    "    \"FatorSensibilidadeInterpessoal\",\n",
    "    \"FatorDepressao\",\n",
    "    \"FatorAnsiedade\",\n",
    "    \"FatorHostilidade\",\n",
    "    \"FatorAnsiedadeFobica\",\n",
    "    \"FatorIdeacaoParanoide\",\n",
    "    \"FatorPsicoticismo\",\n",
    "    \"IGS\"\n",
    "]\n",
    "df_excel_Screening[cols_to_round] = df_excel_Screening[cols_to_round].round(2)\n",
    "\n",
    "# Preview selected factor scores and IGS\n",
    "df_excel_Screening[['Somatizacao','FatorObsessoesCompulsoes','FatorSensibilidadeInterpessoal','FatorDepressao',\n",
    "                    'FatorAnsiedade','FatorHostilidade','FatorAnsiedadeFobica','FatorIdeacaoParanoide','FatorPsicoticismo',\n",
    "                    'IGS']]\n",
    "\n",
    "# Sort participants by subject number for consistent ordering\n",
    "df_excel_Screening.sort_values(by=['subject_nr'], kind='mergesort', inplace=True,ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4ef61",
   "metadata": {},
   "source": [
    "## 1.5. View final screening database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15547e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display all dataframe columns without truncation\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Show the full dataframe\n",
    "df_excel_Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b4e7d",
   "metadata": {},
   "source": [
    "# 2. Activity and sleep diaries databases\n",
    "The following sections load, clean, transform, and preprocess the data collected through the participants’ sleep and activity diaries. These steps ensure that all variables are standardized, structured, and ready for subsequent analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f630c7a",
   "metadata": {},
   "source": [
    "## **2.1. Sleep diary database**\n",
    "\n",
    "In this subsection, we load, clean, and preprocess the data collected through the sleep diaries. These daily logs were used to verify compliance with the study’s mandatory sleep schedule: sleeping between 23:00h and 01:00h and waking between 07:00h and 09:00h.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sleep Diary Questions**\n",
    "\n",
    "The sleep diary contained the following questions: \n",
    "\n",
    "SD_Q1. A que hora foi para a cama?\n",
    "\n",
    "SD_Q2. A que horas começou a tentar adormecer\n",
    "\n",
    "SD_Q3. Quanto tempo acha que demorou a adormecer?\n",
    "\n",
    "SD_Q4. Quantas vezes acordou e voltou a adormecer?\n",
    "\n",
    "SD_Q5. Caso tenha acordado a meio da noite e voltado a adormecer quanto tempo é que esteve acordado até voltar a adormecer? Se experienciou mais do que um episódio destes some o tempo total que esteve acordado durante estes episódios. Por favor, indique esta estimativa em minutos (p. ex., 75).\n",
    "\n",
    "SD_Q6a. A que horas acordou? Se acordou a meio da noite e voltou a adormecer indique a hora em que acordou pela última vez antes de se levantar. Por favor, utilize o formato hh:mm (p. ex., 08:30).\n",
    "\n",
    "SD_Q6b. Depois de acordar quanto tempo passou na cama a tentar adormecer novamente? Por favor, indique esta estimativa em minutos (p. ex., 75).\n",
    "\n",
    "SD_Q6c. Acordou antes da hora que tinha planificado?\n",
    "\n",
    "SD_Q6d. Caso esta situação tenha ocorrido, indique quanto tempo antes da hora planificada acordou? Por favor, indique esta estimativa em minutos (p. ex., 75)Se isto não lhe aconteceu avance para a próxima pergunta.\n",
    "\n",
    "SD_Q7. A que horas se levantou/saiu da cama? Se acordou a meio da noite, levantou-se, voltou para a cama e voltou a adormecer, indique qual foi a hora em que se levantou/saiu da cama pela última vez antes de iniciar a sua rotina. Por favor, utilize o formato hh:mm (p. ex., 08:30).\n",
    "\n",
    "SD_Q8. No total, estima que dormiu quanto tempo (p. ex., 6h30)?\n",
    "\n",
    "SD_Q9. Como classifica a qualidade do seu sono?\n",
    "\n",
    "SD_Q10. O quão descansado(a)/restabelecido(a) se sentiu quando acordou e iniciou a sua rotina?\n",
    "\n",
    "SD_Q11. Comentários. Adicione qualquer informação adicional que considere que possa ter condicionado o seu sono (p. ex., estar doente, ter experienciado alguma emergência). Caso não queira adicionar nenhuma informação deixe a caixa de texto em branco.\n",
    "\n",
    "---\n",
    "\n",
    "### **Derived Sleep-Efficiency Indices**\n",
    "\n",
    "The script derives a set of sleep-efficiency indices from the raw outputs, which are standard in sleep research and allow quantitative evaluation of sleep continuity and restoration:\n",
    "\n",
    "- **SOL (Sleep Onset Latency):** Time elapsed between going to bed and falling asleep.\n",
    "\n",
    "- **TASAFA (Time Attempting to Sleep After Final Awakening):** Minutes spent trying to return to sleep after the final awakening.\n",
    "\n",
    "- **WASO (Wake After Sleep Onset):** Total time awake during the sleep period after initially falling asleep.\n",
    "\n",
    "- **TST (Total Sleep Time):** Total number of minutes actually spent sleeping during the sleep episode.\n",
    "\n",
    "- **DSE (Duration of the Sleep Episode):** Total length of the sleep period, including SOL, WASO, TST, and TASAFA.\n",
    "\n",
    "- **SE (Sleep Efficiency):** Ratio between total sleep time (TST) and the full sleep episode duration (DSE), expressed as a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1501b8",
   "metadata": {},
   "source": [
    "### 2.1.1. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b34b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resolve project root directory\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Build full path to the sleep diary Excel file\n",
    "SD_excel_part_data_path = PROJ_ROOT / \"DiariosSonoAtividade\" / \"dataSono.xlsx\"\n",
    "\n",
    "# Load Excel sleep diary data into a dataframe\n",
    "SD_df_excel_data_part = pd.read_excel(SD_excel_part_data_path)\n",
    "\n",
    "## Drop redundant (empty) session-related columns\n",
    "SD_df_excel_data_part.drop(SD_df_excel_data_part.columns[3:11],axis=1,inplace=True)\n",
    "\n",
    "## Replace original column names with correct labels\n",
    "SD_df_excel_data_part.columns = ['participant', 'Subject_Nr', 'Session_Nr', 'Data', 'SD_Q1', 'SD_Q2', 'SD_Q3', 'SD_Q4', 'SD_Q5', 'SD_Q6a',\n",
    "                               'SD_Q6b', 'SD_Q6c', 'SD_Q6d', 'SD_Q7', 'SD_Q8', 'SD_Q9', 'SD_Q10', 'SD_Q11', 'TIME_start', 'TIME_end','TIME_total']\n",
    "\n",
    "## Sort diary entries by participant and session number\n",
    "SD_df_excel_data_part.sort_values(by=['Subject_Nr','Session_Nr'], kind='mergesort', inplace=True,ascending=True)\n",
    "SD_df_excel_data_part.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Display all dataframe columns and show the result\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "SD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc78c3",
   "metadata": {},
   "source": [
    "### 2.1.2. Transformation of raw codes into meaningful attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_df_excel_data_part[['SD_Q6c', 'SD_Q9','SD_Q10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc917e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Substitui valores numéricos por labels correspondentes nas colunas selecionadas\n",
    "for i in range(0,len(SD_df_excel_data_part['SD_Q6c'])):\n",
    "        if SD_df_excel_data_part.loc[i,'SD_Q6c'] == 1:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q6c'] = \"Sim\"\n",
    "        else :\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q6c'] = \"Não\"\n",
    "\n",
    "# Map numeric sleep quality ratings (SD_Q9) to descriptive labels\n",
    "for i in range(0,len(SD_df_excel_data_part['SD_Q9'])):\n",
    "        if SD_df_excel_data_part.loc[i,'SD_Q9'] == 1:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q9'] = \"Muito Pobre\"\n",
    "        elif SD_df_excel_data_part.loc[i,'SD_Q9'] == 2:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q9'] = \"Pobre\"\n",
    "        elif SD_df_excel_data_part.loc[i,'SD_Q9'] == 3:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q9'] = \"Aceitável\"\n",
    "        elif SD_df_excel_data_part.loc[i,'SD_Q9'] == 4:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q9'] = \"Boa\"\n",
    "        else :\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q9'] = \"Muito Boa\"\n",
    "\n",
    "# Convert numeric restfulness ratings (SD_Q10) to descriptive labels\n",
    "for i in range(0,len(SD_df_excel_data_part['SD_Q10'])):\n",
    "        if SD_df_excel_data_part.loc[i,'SD_Q10'] == 1:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q10'] = \"Nada descansado(a)/restabelecido(a)\"\n",
    "        elif SD_df_excel_data_part.loc[i,'SD_Q10'] == 2:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q10'] = \"Ligeiramente descansado(a)/restabelecido(a)\"\n",
    "        elif SD_df_excel_data_part.loc[i,'SD_Q10'] == 3:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q10'] = \"Aceitavelmente descansado(a)/restabelecido(a)\"\n",
    "        elif SD_df_excel_data_part.loc[i,'SD_Q10'] == 4:\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q10'] = \"Bem descansado(a)/restabelecido(a)\"\n",
    "        else :\n",
    "            SD_df_excel_data_part.loc[i,'SD_Q10'] = \"Muito bem descansado(a)/restabelecido(a)\"\n",
    "\n",
    "# Display updated categorical columns\n",
    "SD_df_excel_data_part[['SD_Q6c', 'SD_Q9','SD_Q10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff09779",
   "metadata": {},
   "source": [
    "### 2.1.3. Calculates and appends sleep efficiency indices (e.g., Total Sleep Time (TST), Duration of Sleep Episode (DSE), and Sleep Efficiency (SE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9990f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import standard libraries used for time calculations and timestamps\n",
    "import os, datetime, time\n",
    "\n",
    "# Create SOL (sleep onset latency) column from SD_Q3\n",
    "SD_df_excel_data_part['SOL'] = SD_df_excel_data_part['SD_Q3']\n",
    "\n",
    "# Create TASAFA (time attempting to sleep after final awakening) column from SD_Q6b\n",
    "SD_df_excel_data_part['TASAFA'] = SD_df_excel_data_part['SD_Q6b']\n",
    "\n",
    "# Create WASO (wake after sleep onset) column from SD_Q5\n",
    "SD_df_excel_data_part['WASO'] = SD_df_excel_data_part['SD_Q5']\n",
    "\n",
    "# Replace missing WASO values with 0 (participants who skipped the question)\n",
    "for i in range(0,len(SD_df_excel_data_part['WASO'])):\n",
    "    if pd.isnull(SD_df_excel_data_part.loc[i,'WASO']):\n",
    "        SD_df_excel_data_part.loc[i,'WASO'] = 0\n",
    "\n",
    "# Prepare to compute Total Sleep Time (TST) from sleep attempt to rise time\n",
    "temp_sleep_time = SD_df_excel_data_part['SD_Q2'].copy(deep=True)\n",
    "\n",
    "# Parse bedtime strings to datetime\n",
    "for i in range(0,len(temp_sleep_time)):\n",
    "    temp_sleep_time[i] = pd.to_datetime(str(temp_sleep_time[i]))\n",
    "\n",
    "# Copy rise times for aligned processing\n",
    "temp_rising_time = SD_df_excel_data_part['SD_Q6a'].copy(deep=True)\n",
    "\n",
    "# Parse rise time strings to datetime\n",
    "for i in range(0,len(temp_rising_time)):\n",
    "    temp_rising_time[i] = pd.to_datetime(str(temp_rising_time[i]))\n",
    "\n",
    "# Compute elapsed time between rise time and bedtime\n",
    "temp_diff_ris_sleep = temp_rising_time - temp_sleep_time\n",
    "\n",
    "# Keep only the HH:MM portion (drop date component)\n",
    "for i in range(0,len(temp_diff_ris_sleep)):\n",
    "    temp_diff_ris_sleep[i] = str(temp_diff_ris_sleep[i])\n",
    "    aaa = (str(temp_diff_ris_sleep[i]))\n",
    "    aaa = aaa[-8:-3]\n",
    "    temp_diff_ris_sleep[i] = aaa\n",
    "\n",
    "# Compute TST in minutes: (Wakeup - Bedtime) - WASO - SOL, and also format HHhMM for display\n",
    "temp_TST = []\n",
    "TST_for_DSE = []\n",
    "for i in range(0,len(temp_diff_ris_sleep)):\n",
    "    ##Converte o formato hh:mm em minutos e subtrai o valor do WASo e do SOL para calcular o TST.\n",
    "    ##Volta a converter o TST num forato HHhMM\n",
    "    aaa = float(temp_diff_ris_sleep[i][0:2])\n",
    "    aaa = aaa*60\n",
    "    bbb = float(temp_diff_ris_sleep[i][3:6])\n",
    "    aaa = aaa + bbb\n",
    "    aaa = aaa - SD_df_excel_data_part.loc[i,'WASO'] - SD_df_excel_data_part.loc[i,'SOL']\n",
    "    TST_for_DSE.append(aaa)\n",
    "    aaa = aaa/60\n",
    "    aaa = '{0:02.0f}:{1:02.0f}'.format(*divmod(aaa * 60, 60))\n",
    "    aaa = str(aaa)\n",
    "    aaa = aaa.replace(\":\",\"h\")\n",
    "    temp_TST.append(aaa)\n",
    "\n",
    "# Store formatted TST (HHhMM)\n",
    "SD_df_excel_data_part['TST'] = temp_TST\n",
    "\n",
    "# Compute DSE (duration of sleep episode) = SOL + WASO + TST + TASAFA; store formatted HHhMM\n",
    "# The DSE is calculating by suming the SOL, WASO, TST, and the TASAFA.\n",
    "temp_DSE = []\n",
    "DSE_for_SE = []\n",
    "for i in range(0,len(SD_df_excel_data_part['participant'])):\n",
    "    aaa = TST_for_DSE[i] + SD_df_excel_data_part.loc[i,'WASO'] + SD_df_excel_data_part.loc[i,'SOL'] + SD_df_excel_data_part.loc[i,'TASAFA']\n",
    "    DSE_for_SE.append(aaa)\n",
    "    aaa = aaa/60\n",
    "    aaa = '{0:02.0f}:{1:02.0f}'.format(*divmod(aaa * 60, 60))\n",
    "    aaa = str(aaa)\n",
    "    aaa = aaa.replace(\":\",\"h\")\n",
    "    temp_DSE.append(aaa)\n",
    "\n",
    "# Store formatted DSE (HHhMM)\n",
    "SD_df_excel_data_part['DSE'] = temp_DSE\n",
    "\n",
    "# Compute SE (sleep efficiency) as TST/DSE * 100\n",
    "# The SE is the ratio of total sleep time (TST) to duration of sleep episode (SDE).\n",
    "temp_SE = []\n",
    "for i in range(0,len(SD_df_excel_data_part['participant'])):\n",
    "    aaa = round((TST_for_DSE[i]/DSE_for_SE[i])*100,2)\n",
    "    temp_SE.append(aaa)\n",
    "\n",
    "# Store SE (%)\n",
    "SD_df_excel_data_part['SE'] = temp_SE\n",
    "\n",
    "# Define helper constants (hours and time thresholds)\n",
    "twentythree = 82800\n",
    "twentythree = datetime.time(23,0,0)\n",
    "one = 3600\n",
    "one = datetime.time(1,0,0)\n",
    "seven = 25200\n",
    "seven = datetime.time(7,0,0)\n",
    "nine = datetime.time(9,0,0)\n",
    "TSTsix = 360\n",
    "TSTnine = 540\n",
    "\n",
    "# Show all columns and preview key derived sleep variables\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "SD_df_excel_data_part[['SOL', 'TASAFA', 'WASO', 'TST', 'DSE', 'SE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37eb89",
   "metadata": {},
   "source": [
    "### 2.1.4. View final sleep diary database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912ed2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reorder columns to produce the final structured sleep diary dataset\n",
    "SD_df_excel_data_part = SD_df_excel_data_part[['Subject_Nr', 'Session_Nr', 'Data', 'SD_Q1', 'SD_Q2',\n",
    "       'SD_Q3', 'SD_Q4', 'SD_Q5', 'SD_Q6a', 'SD_Q6b', 'SD_Q6c', 'SD_Q6d',\n",
    "       'SD_Q7', 'SD_Q8', 'SD_Q9', 'SD_Q10', 'SD_Q11', 'SOL', 'TASAFA', 'WASO', 'TST', 'DSE', 'SE', 'TIME_start', 'TIME_end',\n",
    "       'TIME_total']]\n",
    "\n",
    "# Display all columns and show the final dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "SD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa806b",
   "metadata": {},
   "source": [
    "## **2.1. Acitivity diary database**\n",
    "\n",
    "In this subsection, we load, clean, and preprocess the data collected through the activity diaries. These daily logs were used to verify compliance with the study’s mandatory restrictions: participants were asked to limit their daily intake of caffeinated beverages (≤3) and alcoholic drinks (≤2) and to refrain from using any sleep aids, including herbal supplements like valerian.\n",
    "\n",
    "---\n",
    "\n",
    "### **Activity diary questions**\n",
    "\n",
    "The activity diary contained the following questions: \n",
    "\n",
    "AD_Q1.1 Quantas sestas fez hoje (p. ex., 1)?\n",
    "\n",
    "AD_Q1.2 Caso tenha feito sesta(s), quanto tempo dormiu no total (p. ex., 02h15)? Se fez mais do que uma sesta, some o tempo total que dormiu em todas as sestas.Caso não tenha feito sesta avance para a questão seguinte.\n",
    "\n",
    "AD_Q3. Quantas bebidas alcoólicas consumiu hoje (p. ex., 0)?\n",
    "\n",
    "AD_Q4. Sinalize o(s) horário(s) em que consumiu bebidas alcoólicas.<br>Caso não tenha consumido bebidas alcoólicas avance para a questão seguinte.\n",
    "\n",
    "AD_Q5. Quantas bebidas cafeinadas (café, bebidas energéticas, chá) consumiu hoje (p. ex., 2)?\n",
    "\n",
    "AD_Q6. Sinalize o(s) horário(s) em que consumiu bebidas cafeinadas.<br>Caso não tenha consumido bebidas cafeinadas avance para a questão seguinte.\n",
    "\n",
    "AD_Q7. Tomou alguma medicação para o(a) ajudar a dormir?\n",
    "\n",
    "AD_Q8. Caso tenha respondido afirmativamente na última questão liste a medicação que tomou.<br>Se não tomou nenhuma medicação avance para a questão seguinte.\n",
    "\n",
    "AD_Q9. Sinalize o(s) horário(s) em que tomou medicação para dormir.<br>Caso não tenha tomado nenhuma medicação avance para a questão seguinte.\n",
    "\n",
    "AD_Q10. Realizou algum tipo de desporto ou atividade física intensa?\n",
    "\n",
    "AD_Q11. Sinalize o(s) horário(s) em que realizou desporto ou atividade física intensa.<br>Caso não tenha realizado desporto ou atividade física intensa avance para a questão seguinte.\n",
    "\n",
    "AD_Q12. Comentários. Adicione qualquer informação adicional que considere que possa ter condicionado/alterado a sua rotina (p. ex., estar doente, ter experienciado alguma emergência). Caso não queira adicionar mais nenhuma informação deixe a caixa de texto em branco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95adbce",
   "metadata": {},
   "source": [
    "### 2.2.1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2efd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Resolve the project root directory (one level above the notebook)\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Build the full path to the activity diary Excel file\n",
    "AD_excel_part_data_path = PROJ_ROOT / \"DiariosSonoAtividade\" / \"dataAtividade.xlsx\"\n",
    "\n",
    "# Load the raw activity diary dataset\n",
    "AD_df_excel_data_part = pd.read_excel(AD_excel_part_data_path)\n",
    "\n",
    "# Drop redundant/empty columns related to unused session fields\n",
    "AD_df_excel_data_part.drop(AD_df_excel_data_part.columns[3:11], axis=1, inplace=True)\n",
    "\n",
    "# Replace default column names with the correct questionnaire labels\n",
    "AD_df_excel_data_part.columns = [\"participant\",\"Subject_Nr\",\"Session_Nr\",\"Data\",\"AD_Q1\",\"AD_Q2\",\"AD_Q3\",\"AD_Q4:1\",\"AD_Q4:2\",\"AD_Q4:3\",\n",
    "                                 \"AD_Q4:4\",\"AD_Q4:5\",\"AD_Q4:6\",\"AD_Q4:7\",\"AD_Q4:8\",\"AD_Q4:9\",\"AD_Q4:10\",\"AD_Q4:11\",\"AD_Q4:12\",\"AD_Q4:13\",\n",
    "                                 \"AD_Q4:14\",\"AD_Q4:15\",\"AD_Q4:16\",\"AD_Q4:17\",\"AD_Q4:18\",\"AD_Q4:19\",\"AD_Q4:20\",\"AD_Q4:21\",\"AD_Q4:22\",\n",
    "                                 \"AD_Q4:23\",\"AD_Q4:24\",\"AD_Q5\",\"AD_Q6:1\",\"AD_Q6:2\",\"AD_Q6:3\",\"AD_Q6:4\",\"AD_Q6:5\",\"AD_Q6:6\",\"AD_Q6:7\",\n",
    "                                 \"AD_Q6:8\",\"AD_Q6:9\",\"AD_Q6:10\",\"AD_Q6:11\",\"AD_Q6:12\",\"AD_Q6:13\",\"AD_Q6:14\",\"AD_Q6:15\",\"AD_Q6:16\",\n",
    "                                 \"AD_Q6:17\",\"AD_Q6:18\",\"AD_Q6:19\",\"AD_Q6:20\",\"AD_Q6:21\",\"AD_Q6:22\",\"AD_Q6:23\",\"AD_Q6:24\",\"AD_Q7\",\n",
    "                                 \"AD_Q8\",\"AD_Q9:1\",\"AD_Q9:2\",\"AD_Q9:3\",\"AD_Q9:4\",\"AD_Q9:5\",\"AD_Q9:6\",\"AD_Q9:7\",\"AD_Q9:8\",\"AD_Q9:9\",\n",
    "                                 \"AD_Q9:10\",\"AD_Q9:11\",\"AD_Q9:12\",\"AD_Q9:13\",\"AD_Q9:14\",\"AD_Q9:15\",\"AD_Q9:16\",\"AD_Q9:17\",\"AD_Q9:18\",\n",
    "                                 \"AD_Q9:19\",\"AD_Q9:20\",\"AD_Q9:21\",\"AD_Q9:22\",\"AD_Q9:23\",\"AD_Q9:24\",\"AD_Q10\",\"AD_Q11:1\",\"AD_Q11:2\",\n",
    "                                 \"AD_Q11:3\",\"AD_Q11:4\",\"AD_Q11:5\",\"AD_Q11:6\",\"AD_Q11:7\",\"AD_Q11:8\",\"AD_Q11:9\",\"AD_Q11:10\",\"AD_Q11:11\",\n",
    "                                 \"AD_Q11:12\",\"AD_Q11:13\",\"AD_Q11:14\",\"AD_Q11:15\",\"AD_Q11:16\",\"AD_Q11:17\",\"AD_Q11:18\",\"AD_Q11:19\",\n",
    "                                 \"AD_Q11:20\",\"AD_Q11:21\",\"AD_Q11:22\",\"AD_Q11:23\",\"AD_Q11:24\",\"AD_Q12\",\"TIME_start\",\"TIME_end\",\n",
    "                                 \"TIME_total\"]\n",
    "\n",
    "# Sort records by participant and session number in ascending order\n",
    "AD_df_excel_data_part.sort_values(by=[\"Subject_Nr\", \"Session_Nr\"], kind='mergesort', inplace=True, ascending=True)\n",
    "AD_df_excel_data_part.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Ensure full column display and show the processed dataset\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "AD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1e6d7",
   "metadata": {},
   "source": [
    "### 2.2.2. Transformation of raw codes into meaningful attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca65972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Substitui os valores númericos que codificam horas nas colunas referentes às questões 4,6,9 11 pelos labels corretos (horas\n",
    "#correspondenes, p. ex., 04h00)\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:1'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:1'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:1'] = '00h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:2'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:2'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:2'] = '01h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:3'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:3'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:3'] = '02h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:4'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:4'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:4'] = '03h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:5'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:5'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:5'] = '04h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:6'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:6'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:6'] = '05h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:7'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:7'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:7'] = '06h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:8'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:8'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:8'] = '07h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:9'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:9'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:9'] = '08h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:10'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:10'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:10'] = '09h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:11'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:11'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:11'] = '10h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:12'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:12'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:12'] = '11h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:13'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:13'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:13'] = '12h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:14'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:14'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:14'] = '13h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:15'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:15'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:15'] = '14h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:16'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:16'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:16'] = '15h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:17'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:17'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:17'] = '16h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:18'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:18'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:18'] = '17h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:19'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:19'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:19'] = '18h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:20'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:20'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:20'] = '19h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:21'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:21'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:21'] = '20h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:22'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:22'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:22'] = '21h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:23'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:23'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:23'] = '22h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q4:24'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q4:24'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q4:24'] = '23h00'\n",
    "\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:1'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:1'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:1'] = '00h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:2'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:2'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:2'] = '01h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:3'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:3'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:3'] = '02h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:4'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:4'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:4'] = '03h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:5'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:5'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:5'] = '04h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:6'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:6'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:6'] = '05h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:7'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:7'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:7'] = '06h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:8'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:8'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:8'] = '07h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:9'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:9'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:9'] = '08h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:10'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:10'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:10'] = '09h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:11'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:11'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:11'] = '10h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:12'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:12'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:12'] = '11h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:13'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:13'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:13'] = '12h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:14'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:14'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:14'] = '13h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:15'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:15'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:15'] = '14h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:16'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:16'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:16'] = '15h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:17'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:17'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:17'] = '16h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:18'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:18'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:18'] = '17h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:19'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:19'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:19'] = '18h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:20'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:20'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:20'] = '19h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:21'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:21'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:21'] = '20h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:22'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:22'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:22'] = '21h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:23'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:23'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:23'] = '22h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q6:24'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q6:24'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q6:24'] = '23h00'\n",
    "\n",
    "\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:1'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:1'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:1'] = '00h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:2'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:2'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:2'] = '01h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:3'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:3'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:3'] = '02h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:4'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:4'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:4'] = '03h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:5'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:5'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:5'] = '04h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:6'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:6'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:6'] = '05h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:7'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:7'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:7'] = '06h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:8'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:8'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:8'] = '07h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:9'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:9'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:9'] = '08h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:10'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:10'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:10'] = '09h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:11'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:11'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:11'] = '10h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:12'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:12'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:12'] = '11h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:13'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:13'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:13'] = '12h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:14'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:14'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:14'] = '13h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:15'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:15'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:15'] = '14h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:16'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:16'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:16'] = '15h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:17'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:17'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:17'] = '16h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:18'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:18'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:18'] = '17h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:19'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:19'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:19'] = '18h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:20'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:20'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:20'] = '19h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:21'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:21'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:21'] = '20h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:22'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:22'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:22'] = '21h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:23'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:23'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:23'] = '22h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q9:24'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q9:24'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q9:24'] = '23h00'\n",
    "\n",
    "\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:1'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:1'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:1'] = '00h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:2'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:2'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:2'] = '01h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:3'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:3'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:3'] = '02h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:4'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:4'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:4'] = '03h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:5'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:5'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:5'] = '04h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:6'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:6'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:6'] = '05h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:7'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:7'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:7'] = '06h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:8'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:8'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:8'] = '07h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:9'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:9'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:9'] = '08h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:10'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:10'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:10'] = '09h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:11'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:11'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:11'] = '10h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:12'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:12'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:12'] = '11h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:13'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:13'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:13'] = '12h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:14'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:14'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:14'] = '13h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:15'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:15'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:15'] = '14h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:16'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:16'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:16'] = '15h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:17'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:17'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:17'] = '16h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:18'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:18'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:18'] = '17h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:19'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:19'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:19'] = '18h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:20'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:20'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:20'] = '19h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:21'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:21'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:21'] = '20h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:22'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:22'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:22'] = '21h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:23'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:23'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:23'] = '22h00'\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q11:24'])):\n",
    "    if AD_df_excel_data_part.loc[i,'AD_Q11:24'] == 1:\n",
    "        AD_df_excel_data_part.loc[i, 'AD_Q11:24'] = '23h00'\n",
    "\n",
    "\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q7'])):\n",
    "        if AD_df_excel_data_part.loc[i,'AD_Q7'] == 1:\n",
    "            AD_df_excel_data_part.loc[i,'AD_Q7'] = \"Sim\"\n",
    "        else:\n",
    "            AD_df_excel_data_part.loc[i, 'AD_Q7'] = \"Não\"\n",
    "\n",
    "for i in range(0,len(AD_df_excel_data_part['AD_Q10'])):\n",
    "        if AD_df_excel_data_part.loc[i,'AD_Q10'] == 1:\n",
    "            AD_df_excel_data_part.loc[i,'AD_Q10'] = \"Sim\"\n",
    "        else:\n",
    "            AD_df_excel_data_part.loc[i, 'AD_Q10'] = \"Não\"\n",
    "\n",
    "##Cria listas com os horários em que os participantes consumiram bebidas alcoólicas (Q4), bebidas cafeinadas (Q6), tomaram medicação\n",
    "##para dormir (Q9), ou fizeram exercício (Q11) e guarda essas listas em colunas na dataframe 'AD_df_excel_data_part'.\n",
    "major_list = []\n",
    "for j in range(0,len(AD_df_excel_data_part['participant'])):\n",
    "    temp_list = []\n",
    "    for i in range(7,31):\n",
    "        if AD_df_excel_data_part.iloc[j,i] != 0:\n",
    "            temp_list.append(AD_df_excel_data_part.iloc[j,i])\n",
    "    major_list.append(temp_list)\n",
    "for i in range(0,len(major_list)):\n",
    "    if major_list[i] == []:\n",
    "        major_list[i] = None\n",
    "AD_df_excel_data_part[\"AD_Q4\"] = major_list\n",
    "\n",
    "major_list = []\n",
    "for j in range(0,len(AD_df_excel_data_part['participant'])):\n",
    "    temp_list = []\n",
    "    for i in range(32,56):\n",
    "        if AD_df_excel_data_part.iloc[j,i] != 0:\n",
    "            temp_list.append(AD_df_excel_data_part.iloc[j,i])\n",
    "    major_list.append(temp_list)\n",
    "for i in range(0,len(major_list)):\n",
    "    if major_list[i] == []:\n",
    "        major_list[i] = None\n",
    "AD_df_excel_data_part[\"AD_Q6\"] = major_list\n",
    "\n",
    "major_list = []\n",
    "for j in range(0,len(AD_df_excel_data_part['participant'])):\n",
    "    temp_list = []\n",
    "    for i in range(58,82):\n",
    "        if AD_df_excel_data_part.iloc[j,i] != 0:\n",
    "            temp_list.append(AD_df_excel_data_part.iloc[j,i])\n",
    "    major_list.append(temp_list)\n",
    "for i in range(0,len(major_list)):\n",
    "    if major_list[i] == []:\n",
    "        major_list[i] = None\n",
    "AD_df_excel_data_part[\"AD_Q9\"] = major_list\n",
    "\n",
    "major_list = []\n",
    "for j in range(0,len(AD_df_excel_data_part['participant'])):\n",
    "    temp_list = []\n",
    "    for i in range(83,107):\n",
    "        if AD_df_excel_data_part.iloc[j,i] != 0:\n",
    "            temp_list.append(AD_df_excel_data_part.iloc[j,i])\n",
    "    major_list.append(temp_list)\n",
    "for i in range(0,len(major_list)):\n",
    "    if major_list[i] == []:\n",
    "        major_list[i] = None\n",
    "AD_df_excel_data_part[\"AD_Q11\"] = major_list\n",
    "\n",
    "##Elimina colunas que já não são úteis relacionadas com as questões 4, 6, 9 e 11.\n",
    "AD_df_excel_data_part.drop(AD_df_excel_data_part.columns[83:107],axis=1,inplace=True)\n",
    "AD_df_excel_data_part.drop(AD_df_excel_data_part.columns[58:82],axis=1,inplace=True)\n",
    "AD_df_excel_data_part.drop(AD_df_excel_data_part.columns[32:56],axis=1,inplace=True)\n",
    "AD_df_excel_data_part.drop(AD_df_excel_data_part.columns[7:31],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "cols_to_clean = [\"AD_Q4\",\"AD_Q6\", \"AD_Q11\"]  # columns where [''] appears\n",
    "for col in cols_to_clean:\n",
    "    AD_df_excel_data_part[col] = (\n",
    "        AD_df_excel_data_part[col]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[\\[\\]']\", \"\", regex=True)  # remove [, ], '\n",
    "        .str.strip()\n",
    "    )\n",
    "    \n",
    "AD_df_excel_data_part = AD_df_excel_data_part[\n",
    "    [\"Subject_Nr\", \"Session_Nr\",\"Data\", \"AD_Q1\",\"AD_Q2\",\"AD_Q3\",\"AD_Q4\", \"AD_Q5\",\"AD_Q6\", \"AD_Q7\",\"AD_Q8\",\"AD_Q9\",\"AD_Q10\", \n",
    "     \"AD_Q11\",\"AD_Q12\",\"TIME_start\", \"TIME_end\",\"TIME_total\"]\n",
    "]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "AD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af583587",
   "metadata": {},
   "source": [
    "### 2.2.3. View final activity diary database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904ae39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reorder columns to the canonical activity-diary layout\n",
    "AD_df_excel_data_part = AD_df_excel_data_part[['Subject_Nr', 'Session_Nr', 'Data', 'AD_Q1', 'AD_Q2', 'AD_Q3', 'AD_Q4',\n",
    "                                               'AD_Q5', 'AD_Q6', 'AD_Q7', 'AD_Q8', 'AD_Q9', 'AD_Q10', 'AD_Q11', 'AD_Q12', 'TIME_start',\n",
    "                                               'TIME_end', 'TIME_total']]\n",
    "\n",
    "# Show all columns without truncation\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the reordered dataframe\n",
    "AD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b460e5",
   "metadata": {},
   "source": [
    "# 3. Actigraphy database\n",
    "Generates DB with the actigraphy data collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f4701",
   "metadata": {},
   "source": [
    "## 3.1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de305a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resolve project root path (one directory up from /notebook)\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Build path to the day-summary CSV safely\n",
    "csv_part_data_path = PROJ_ROOT / \"Actigraphy\" / \"part5_daysummary_WW_L30M100V400_T5A5.csv\"\n",
    "\n",
    "# Load day-summary CSV into a DataFrame\n",
    "df_Actigraphy = pd.read_csv(csv_part_data_path, engine = 'python',sep = ',')\n",
    "\n",
    "# Create a view sorted by ID (no assignment; actual sort happens later)\n",
    "df_Actigraphy.sort_values(\"ID\")\n",
    "\n",
    "# Keep relevant columns from GeneActive day summary\n",
    "df_Actigraphy = df_Actigraphy[[\"ID\",\"filename\",\"sleeplog_used\",\"guider\",\"cleaningcode\",\"daysleeper\",\"night_number\",\"calendar_date\",\"weekday\",\n",
    "                                     \"nonwear_perc_day_spt\",\"sleeponset_ts\",\"wakeup_ts\",\"dur_day_spt_min\",\"dur_day_min\",\"dur_spt_min\",\n",
    "                                     \"dur_spt_wake_IN_min\",\"dur_spt_wake_LIG_min\",\"dur_spt_wake_MOD_min\",\"dur_spt_wake_VIG_min\",\"dur_day_IN_unbt_min\",\n",
    "                                     \"dur_day_LIG_unbt_min\",\"dur_day_MOD_unbt_min\",\"dur_day_VIG_unbt_min\",\"dur_spt_sleep_min\",\"sleep_efficiency\"]]\n",
    "\n",
    "\n",
    "######################################\n",
    "# Resolve project root path again (explicitly)\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Build path to the night-summary CSV safely\n",
    "csv_part_data_path2 = PROJ_ROOT / \"Actigraphy\" / \"part4_nightsummary_sleep_cleaned.csv\"\n",
    "\n",
    "# Load night-summary CSV into a DataFrame\n",
    "df_Actigraphy2 = pd.read_csv(csv_part_data_path2, engine = 'python',sep = ',')\n",
    "\n",
    "# Create a view sorted by ID (no assignment)\n",
    "df_Actigraphy2.sort_values(\"ID\")\n",
    "\n",
    "# Keep only first-night records\n",
    "df_Actigraphy2 = df_Actigraphy2[df_Actigraphy2['night'] == 1]\n",
    "\n",
    "# Preserve a deep copy of first-night data\n",
    "df_Actigraphy3 = df_Actigraphy2.copy(deep=True)\n",
    "\n",
    "# Select common metadata and timing columns\n",
    "df_Actigraphy2 = df_Actigraphy2[[\"ID\",\"filename\",\"sleeplog_used\",\"guider\",\"cleaningcode\",\"daysleeper\",\"night\",\"calendar_date\",\"weekday\",\n",
    "                                     \"nonwear_perc_spt\",\"sleeponset_ts\",\"wakeup_ts\"]]\n",
    "\n",
    "# Add missing day-summary columns (empty strings) to align schemas\n",
    "df_Actigraphy2 = df_Actigraphy2.assign(dur_day_spt_min=\"\",dur_day_min=\"\",dur_spt_min=\"\",dur_spt_wake_IN_min=\"\",\n",
    "                                             dur_spt_wake_LIG_min=\"\",dur_spt_wake_MOD_min=\"\",dur_spt_wake_VIG_min=\"\",\n",
    "                                             dur_day_IN_unbt_min=\"\",dur_day_LIG_unbt_min=\"\",dur_day_MOD_unbt_min=\"\",\n",
    "                                             dur_day_VIG_unbt_min=\"\",dur_spt_sleep_min=\"\",sleep_efficiency=\"\")\n",
    "\n",
    "# Harmonize column names with day-summary schema\n",
    "df_Actigraphy2.rename(columns={'night':'night_number','nonwear_perc_spt':'nonwear_perc_day_spt'},inplace=True)\n",
    "\n",
    "# Reorder columns to exactly match the day-summary layout\n",
    "df_Actigraphy2 = df_Actigraphy2[[\"ID\",\"filename\",\"sleeplog_used\",\"guider\",\"cleaningcode\",\"daysleeper\",\"night_number\",\"calendar_date\",\"weekday\",\n",
    "                                     \"nonwear_perc_day_spt\",\"sleeponset_ts\",\"wakeup_ts\",\"dur_day_spt_min\",\"dur_day_min\",\"dur_spt_min\",\n",
    "                                     \"dur_spt_wake_IN_min\",\"dur_spt_wake_LIG_min\",\"dur_spt_wake_MOD_min\",\"dur_spt_wake_VIG_min\",\"dur_day_IN_unbt_min\",\n",
    "                                     \"dur_day_LIG_unbt_min\",\"dur_day_MOD_unbt_min\",\"dur_day_VIG_unbt_min\",\"dur_spt_sleep_min\",\"sleep_efficiency\"]]\n",
    "\n",
    "# Gather day and night DataFrames for concatenation\n",
    "frames = [df_Actigraphy,df_Actigraphy2]\n",
    "\n",
    "# Concatenate day and first-night records into one DataFrame\n",
    "df_Actigraphy = pd.concat(frames)\n",
    "\n",
    "# Sort by participant ID and night number\n",
    "df_Actigraphy = df_Actigraphy.sort_values(by=['ID','night_number'],ascending=[True,True])\n",
    "\n",
    "# Reset index after sorting\n",
    "df_Actigraphy.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Regex pattern to capture text between underscores (first match)\n",
    "pattern2 = \"_(.*?)_\"\n",
    "\n",
    "# Prepare container for device location parsed from filename\n",
    "List_actigraph_location = []\n",
    "\n",
    "# Extract device location from each filename using regex and collect\n",
    "for i in range(len(df_Actigraphy[\"filename\"])):\n",
    "    substring = re.search(pattern2, df_Actigraphy[\"filename\"][i]).group(1)\n",
    "    List_actigraph_location.append(substring)\n",
    "\n",
    "# Convert collected locations to a Series\n",
    "LocationAcel = pd.Series(List_actigraph_location)\n",
    "\n",
    "# Insert parsed location as the third column\n",
    "df_Actigraphy.insert(2,\"LocationAcel\",LocationAcel)\n",
    "\n",
    "# Show all columns when displaying the DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_Actigraphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf9a25",
   "metadata": {},
   "source": [
    "## 3.2. Transformation of raw codes into meaningful attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns when printing DataFrames\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Show selected metadata columns from the Actigraphy DataFrame\n",
    "df_Actigraphy[[\"daysleeper\",\"sleeplog_used\",\"cleaningcode\",\"calendar_date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940ec52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map 0/1 in 'daysleeper' to human-readable labels across all rows\n",
    "for i in range(0,len(df_Actigraphy[\"daysleeper\"])):\n",
    "    # If coded as 0, label as day sleeper\n",
    "    if df_Actigraphy.loc[i,\"daysleeper\"] == 0:\n",
    "        df_Actigraphy.loc[i,\"daysleeper\"] = \"day sleeper\"\n",
    "    # If coded as 1, label as night sleeper\n",
    "    elif df_Actigraphy.loc[i,\"daysleeper\"] == 1:\n",
    "        df_Actigraphy.loc[i,\"daysleeper\"] = \"night sleeper\"\n",
    "\n",
    "# Convert 0/1 in 'sleeplog_used' to \"No\"/\"Yes\" across all rows\n",
    "for i in range(0,len(df_Actigraphy[\"sleeplog_used\"])):\n",
    "    # Replace 0 with \"No\"\n",
    "    if df_Actigraphy.loc[i,\"sleeplog_used\"] == 0:\n",
    "        df_Actigraphy.loc[i,\"sleeplog_used\"] = \"No\"\n",
    "    # Replace 1 with \"Yes\"\n",
    "    elif df_Actigraphy.loc[i,\"sleeplog_used\"] == 1:\n",
    "        df_Actigraphy.loc[i,\"sleeplog_used\"] = \"Yes\"\n",
    "\n",
    "# Expand 'cleaningcode' value 1 to its descriptive text across all rows\n",
    "for i in range(0,len(df_Actigraphy[\"cleaningcode\"])):\n",
    "    # If cleaning code is 1, set a detailed explanation\n",
    "    if df_Actigraphy.loc[i,\"cleaningcode\"] == 1:\n",
    "        df_Actigraphy.loc[i,\"cleaningcode\"] = \"1: GGIR sleep log was not used. Thus, HDCZA guider was used. Only Sleep Period Time (SPT) was identified \" \\\n",
    "                                                  \"(it was not possible to indentify Time in Bed (TIB)).\"\n",
    "\n",
    "# Reformat 'calendar_date' to zero-padded YYYY-MM-DD for night 1 rows\n",
    "for i in range(0,len(df_Actigraphy[\"calendar_date\"])):\n",
    "    # Only transform dates where night_number equals 1\n",
    "    if df_Actigraphy.loc[i,\"night_number\"] == 1:\n",
    "        # Split existing date by \"/\" into components\n",
    "        x = df_Actigraphy.loc[i,\"calendar_date\"].split(\"/\")\n",
    "        # Prepare reversed list for Y-M-D ordering\n",
    "        y = []\n",
    "        # Reverse component order\n",
    "        for j in x:\n",
    "            y.insert(0,j)\n",
    "        # Zero-pad single-digit components\n",
    "        for j in range(0,len(y)):\n",
    "            if int(y[j]) < 10:\n",
    "                y[j] = '0' + y[j]\n",
    "        # Convert list to string\n",
    "        y = str(y)\n",
    "        # Strip brackets\n",
    "        y = y.replace(\"[\",\"\")\n",
    "        y = y.replace(\"]\",\"\")\n",
    "        # Remove quotes\n",
    "        y = y.replace(\"'\",\"\")\n",
    "        # Remove spaces\n",
    "        y = y.replace(\" \",\"\")\n",
    "        # Replace commas with hyphens\n",
    "        y = y.replace(\",\",\"-\")\n",
    "        # Write back normalized date\n",
    "        df_Actigraphy.loc[i,\"calendar_date\"] = y\n",
    "\n",
    "# Build WASO list using night 1 from df_Actigraphy3, else sum wake mins within SPT\n",
    "List_WASO = []\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # For night 1, convert WASO hours to minutes and round\n",
    "    if df_Actigraphy.loc[i,\"night_number\"] == 1:\n",
    "        List_WASO.append(round(df_Actigraphy3.loc[i,\"WASO\"]*60,3))\n",
    "    # For other nights, sum wake durations in different intensity bands\n",
    "    else:\n",
    "        ind = df_Actigraphy.loc[i,\"dur_spt_wake_IN_min\"] + df_Actigraphy.loc[i,\"dur_spt_wake_LIG_min\"] + df_Actigraphy.loc[i,\"dur_spt_wake_MOD_min\"] + df_Actigraphy.loc[i,\"dur_spt_wake_VIG_min\"]\n",
    "        List_WASO.append(ind)\n",
    "\n",
    "# Get DataFrame shape to locate insertion position\n",
    "row,col = df_Actigraphy.shape\n",
    "# Convert accumulated WASO list to a Series\n",
    "List_WASO = pd.Series(List_WASO)\n",
    "# Insert WASO as a new column before the last existing column\n",
    "df_Actigraphy.insert(col-1,\"WASO\",List_WASO)\n",
    "\n",
    "# Ensure all columns display when printing\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Preview selected columns including the new WASO\n",
    "df_Actigraphy[[\"daysleeper\",\"sleeplog_used\",\"cleaningcode\",\"calendar_date\",\"WASO\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34add51d",
   "metadata": {},
   "source": [
    "## 3.3. Standardizing time variables and calculating sleep-related indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Actigraphy[[\"dur_day_spt_min\",\"dur_day_min\",\"dur_spt_min\",\"dur_spt_sleep_min\",\"dur_spt_wake_IN_min\",\n",
    "                    \"dur_spt_wake_LIG_min\",\"dur_spt_wake_MOD_min\",\"dur_spt_wake_VIG_min\",\"dur_day_IN_unbt_min\",\"dur_day_LIG_unbt_min\",\n",
    "                    \"dur_day_MOD_unbt_min\",\"dur_day_VIG_unbt_min\",\"WASO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c32af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the base datetime module\n",
    "import datetime\n",
    "# Import timedelta and datetime classes for time arithmetic and parsing\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Define target columns that will be converted to timedelta values\n",
    "List_target_col = [\"dur_day_spt_min\",\"dur_day_min\",\"dur_spt_min\",\"dur_spt_sleep_min\",\"dur_spt_wake_IN_min\",\n",
    "                    \"dur_spt_wake_LIG_min\",\"dur_spt_wake_MOD_min\",\"dur_spt_wake_VIG_min\",\"dur_day_IN_unbt_min\",\"dur_day_LIG_unbt_min\",\n",
    "                    \"dur_day_MOD_unbt_min\",\"dur_day_VIG_unbt_min\",\"WASO\"]\n",
    "\n",
    "# Loop over each target column to normalize values to timedeltas\n",
    "for i in List_target_col:\n",
    "    # Iterate over all rows in the actigraphy dataframe\n",
    "    for l in range(0,len(df_Actigraphy)):\n",
    "        # Process only rows that are not the first night\n",
    "        if df_Actigraphy.loc[l,\"night_number\"] != 1:\n",
    "            # If the value is zero, set it explicitly to a zero timedelta\n",
    "            if df_Actigraphy.loc[l,i] == 0:\n",
    "                df_Actigraphy.loc[l,i] = timedelta(hours=00, minutes=00, seconds=00)\n",
    "            # Otherwise convert minutes to a timedelta\n",
    "            else:\n",
    "                df_Actigraphy.loc[l,i] = timedelta(minutes=df_Actigraphy.loc[l,i])\n",
    "\n",
    "# Initialize storage for (wakeup - bedtime) durations for first-night rows\n",
    "wakeupminusbedtime = []\n",
    "# Iterate through rows to compute sleep duration and (wakeup - bedtime) for night 1\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # Operate only on first-night entries\n",
    "    if df_Actigraphy.loc[i,\"night_number\"] == 1:\n",
    "        # Parse sleep onset string to datetime, then to timedelta since midnight\n",
    "        aaa = datetime.strptime(df_Actigraphy.loc[i,\"sleeponset_ts\"], \"%H:%M:%S\")\n",
    "        aaa = timedelta(hours=aaa.hour, minutes=aaa.minute, seconds=aaa.second)\n",
    "        # If sleep onset is between 21:00 and 23:59:59, compute remaining day span to midnight+1s\n",
    "        if timedelta(hours=9,minutes=0,seconds=0) <= aaa <= timedelta(hours=23,minutes=59,seconds=59):\n",
    "            bbb = timedelta(hours=23,minutes=59,seconds=59) - aaa + timedelta(hours=0,minutes=0,seconds=1)\n",
    "        # Otherwise there is no cross-midnight portion\n",
    "        else:\n",
    "            bbb = timedelta(hours=00,minutes=0,seconds=0)\n",
    "        # Parse wakeup time string and convert to timedelta since midnight\n",
    "        ccc = datetime.strptime(df_Actigraphy.loc[i,\"wakeup_ts\"], \"%H:%M:%S\")\n",
    "        ccc = timedelta(hours=ccc.hour, minutes=ccc.minute, seconds=ccc.second)\n",
    "        # Convert WASO (in minutes) to a timedelta in seconds\n",
    "        ddd = df_Actigraphy.loc[i,\"WASO\"]*60\n",
    "        ddd = timedelta(seconds=ddd)\n",
    "        # If onset is in late evening, include cross-midnight span when computing sleep duration\n",
    "        if timedelta(hours=9,minutes=0,seconds=0) <= aaa <= timedelta(hours=23,minutes=59,seconds=59):\n",
    "            df_Actigraphy.loc[i,\"dur_spt_sleep_min\"] = ccc + bbb - ddd\n",
    "            wakeupminusbedtime.append(ccc + bbb)\n",
    "        # Otherwise compute simple difference without cross-midnight adjustment\n",
    "        else:\n",
    "            df_Actigraphy.loc[i,\"dur_spt_sleep_min\"] = ccc - aaa - ddd\n",
    "            wakeupminusbedtime.append(ccc - aaa)\n",
    "        # Overwrite WASO column with timedelta version\n",
    "        df_Actigraphy.loc[i,\"WASO\"] = ddd\n",
    "\n",
    "# Make deep copies of the participant daily logs dataframe for processing\n",
    "Daily_Logs_df_excel_data_part = SD_df_excel_data_part.copy(deep=True)\n",
    "# Second copy for potential separate use\n",
    "Daily_Logs_df_excel_data_part2 = SD_df_excel_data_part.copy(deep=True)\n",
    "\n",
    "# Keep only the bedtime and risetime columns\n",
    "Daily_Logs_df_excel_data_part = Daily_Logs_df_excel_data_part[[\"SD_Q2\",\"SD_Q7\"]]\n",
    "# List of daily log time columns to convert to timedeltas\n",
    "List_columns_Daily_Logs = [\"SD_Q2\",\"SD_Q7\"]\n",
    "# Convert each selected time field to a timedelta since midnight\n",
    "for i in List_columns_Daily_Logs:\n",
    "    # Iterate all rows to transform time objects into timedeltas\n",
    "    for l in range(0,len(Daily_Logs_df_excel_data_part)):\n",
    "        # Read time value and rebuild as timedelta for consistency\n",
    "        t = Daily_Logs_df_excel_data_part.loc[l,i]\n",
    "        Daily_Logs_df_excel_data_part.loc[l,i] = timedelta(hours=t.hour, minutes=t.minute, seconds=t.second)\n",
    "# Insert Bedtime timedelta into the actigraphy dataframe at column index 11\n",
    "df_Actigraphy.insert(11,\"Bedtime\",Daily_Logs_df_excel_data_part[\"SD_Q2\"])\n",
    "# Insert Risetime timedelta into the actigraphy dataframe at column index 14\n",
    "df_Actigraphy.insert(14,\"Risetime\",Daily_Logs_df_excel_data_part[\"SD_Q7\"])\n",
    "\n",
    "# Columns with HH:MM:SS strings to be converted into timedeltas\n",
    "List_columns_sleeponset_wakeup = [\"sleeponset_ts\",\"wakeup_ts\"]\n",
    "# Convert sleeponset and wakeup time strings to timedeltas\n",
    "for i in List_columns_sleeponset_wakeup:\n",
    "    # Iterate over rows to parse and convert each time string\n",
    "    for l in range(0,len(df_Actigraphy)):\n",
    "        # Parse string to datetime and convert to timedelta since midnight\n",
    "        t = datetime.strptime(df_Actigraphy.loc[l,i], \"%H:%M:%S\")\n",
    "        df_Actigraphy.loc[l,i] = timedelta(hours=t.hour, minutes=t.minute, seconds=t.second)\n",
    "\n",
    "# Define a zero timedelta for reuse\n",
    "aaa = timedelta(hours=00, minutes=00, seconds=00)\n",
    "# Define 07:00 threshold used to detect post-midnight times\n",
    "bbb = timedelta(hours=7, minutes=00, seconds=00)\n",
    "# Define 24:00 timedelta for day wrapping\n",
    "ccc = timedelta(hours=24, minutes=00, seconds=00)\n",
    "\n",
    "# Temporary holder for adjusted sleep onset per row\n",
    "temp_sleep_onset = aaa\n",
    "# Temporary holder for adjusted bedtime per row\n",
    "temp_Bedtime = aaa\n",
    "# Collector for Sleep Onset Latency (SOL) per row\n",
    "List_SOL = []\n",
    "# Ensure Bedtime is a timedelta, coerce invalids to NaT\n",
    "df_Actigraphy[\"Bedtime\"] = pd.to_timedelta(df_Actigraphy[\"Bedtime\"], errors=\"coerce\")\n",
    "# Ensure Risetime is a timedelta, coerce invalids to NaT\n",
    "df_Actigraphy[\"Risetime\"] = pd.to_timedelta(df_Actigraphy[\"Risetime\"], errors=\"coerce\")\n",
    "# Compute SOL per row, adjusting for cross-midnight where needed\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # If sleep onset is before 07:00, treat it as next-day by adding 24h\n",
    "    if df_Actigraphy.loc[i,\"sleeponset_ts\"] < bbb:\n",
    "        temp_sleep_onset = df_Actigraphy.loc[i,\"sleeponset_ts\"] + ccc\n",
    "    # Otherwise keep sleep onset as-is\n",
    "    else:\n",
    "        temp_sleep_onset = df_Actigraphy.loc[i,\"sleeponset_ts\"]\n",
    "    # If bedtime is before 07:00, treat it as next-day by adding 24h\n",
    "    if df_Actigraphy.loc[i,\"Bedtime\"] < bbb:\n",
    "        temp_Bedtime = df_Actigraphy.loc[i, \"Bedtime\"] + ccc\n",
    "    # Otherwise keep bedtime as-is\n",
    "    else:\n",
    "        temp_Bedtime = df_Actigraphy.loc[i, \"Bedtime\"]\n",
    "    # If adjusted sleep onset precedes adjusted bedtime, clamp onset to bedtime\n",
    "    if temp_sleep_onset < temp_Bedtime:\n",
    "        df_Actigraphy.loc[i, \"sleeponset_ts\"] = df_Actigraphy.loc[i, \"Bedtime\"]\n",
    "    # Compute SOL as (adjusted sleep onset - adjusted bedtime)\n",
    "    temp_SOL = temp_sleep_onset - temp_Bedtime\n",
    "    # Do not allow negative SOL; floor at zero\n",
    "    if temp_SOL < aaa:\n",
    "        temp_SOL = aaa\n",
    "    # Store computed SOL\n",
    "    List_SOL.append(temp_SOL)\n",
    "# Insert SOL column at provided index `col`\n",
    "df_Actigraphy.insert(col,\"SOL\",List_SOL)\n",
    "\n",
    "# If wakeup exceeds reported risetime, cap wakeup at risetime\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # Enforce wakeup <= risetime to avoid negative wake-after-wakeup\n",
    "    if df_Actigraphy.loc[i,\"wakeup_ts\"] > df_Actigraphy.loc[i,\"Risetime\"]:\n",
    "        df_Actigraphy.loc[i,\"wakeup_ts\"] = df_Actigraphy.loc[i,\"Risetime\"]\n",
    "\n",
    "# Prepare list for Time Awake Since Awakening (TASAFA)\n",
    "List_TASAFA = []\n",
    "# Compute TASAFA as risetime minus wakeup, floored at zero\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # Raw TASAFA difference\n",
    "    temp_TASAFA = df_Actigraphy.loc[i, \"Risetime\"] - df_Actigraphy.loc[i, \"wakeup_ts\"]\n",
    "    # Floor negative values to zero\n",
    "    if temp_TASAFA < aaa:\n",
    "        temp_TASAFA = aaa\n",
    "    # Store computed TASAFA\n",
    "    List_TASAFA.append(temp_TASAFA)\n",
    "# Insert TASAFA at `col+3`\n",
    "df_Actigraphy.insert(col+3,\"TASAFA\",List_TASAFA)\n",
    "\n",
    "# Prepare list for Daytime Sleep Episode (DSE) durations\n",
    "List_DSE = []\n",
    "# Compute DSE as SOL + sleep duration + WASO + TASAFA\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # Sum components to form DSE\n",
    "    temp_DSE = df_Actigraphy.loc[i,\"SOL\"] + df_Actigraphy.loc[i,\"dur_spt_sleep_min\"] + df_Actigraphy.loc[i,\"WASO\"] + df_Actigraphy.loc[i,\"TASAFA\"]\n",
    "    # Store computed DSE\n",
    "    List_DSE.append(temp_DSE)\n",
    "# Assign DSE column\n",
    "df_Actigraphy[\"DSE\"] = List_DSE\n",
    "\n",
    "# Prepare list for sleep efficiency based on TST(Act)/DSE(Act)\n",
    "List_SEF = []\n",
    "# Compute per-row sleep efficiency and round to 3 decimals\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # Sleep efficiency as ratio of TST to DSE\n",
    "    temp_SEF = round(df_Actigraphy.loc[i,\"dur_spt_sleep_min\"] / df_Actigraphy.loc[i,\"DSE\"],3)\n",
    "    # Store computed efficiency\n",
    "    List_SEF.append(temp_SEF)\n",
    "# Assign computed sleep efficiency column\n",
    "df_Actigraphy[\"Sleep Efficiency TST(Act)/DSE(Act)\"] = List_SEF\n",
    "\n",
    "# Desired column order for the actigraphy dataframe\n",
    "columns_re = [\"ID\",\"filename\",\"LocationAcel\",\"sleeplog_used\",\"guider\",\"cleaningcode\",\"daysleeper\",\"night_number\",\"calendar_date\",\n",
    "              \"weekday\",\"nonwear_perc_day_spt\",\"Bedtime\",\"sleeponset_ts\",\"wakeup_ts\",\"Risetime\",\"SOL\",\"dur_spt_sleep_min\",\"WASO\",\n",
    "              \"TASAFA\",\"DSE\",\"sleep_efficiency\",\"Sleep Efficiency TST(Act)/DSE(Act)\",\"dur_day_spt_min\",\"dur_day_min\",\"dur_spt_min\",\n",
    "              \"dur_spt_wake_IN_min\",\"dur_spt_wake_LIG_min\",\"dur_spt_wake_MOD_min\",\"dur_spt_wake_VIG_min\",\"dur_day_IN_unbt_min\",\n",
    "              \"dur_day_LIG_unbt_min\",\"dur_day_MOD_unbt_min\",\"dur_day_VIG_unbt_min\"]\n",
    "\n",
    "# Reindex dataframe to the desired column order\n",
    "df_Actigraphy = df_Actigraphy.reindex(columns=columns_re)\n",
    "# Human-readable column names for final output\n",
    "col_names = [\"ID\",\"filename\",\"LocationAcel\",\"sleeplog_used\",\"guider\",\"cleaningcode\",\"daysleeper\",\"night_number\",\"calendar_date\",\n",
    "              \"weekday\",\"nonwear_perc\",\"Bedtime\",\"Sleep Onset\",\"Wakeup Time\",\"Rise Time\",\"SOL\",\"TST\",\"WASO\",\n",
    "              \"TASAFA\",\"DSE\",\"Sleep Efficiency GGIR (TST/(Wakeup-Sleep Onset))\",\"Sleep Efficiency TST(Act)/DSE(Act)\",\n",
    "             \"Duration Day+Sleep episodes\",\"Duration Day Episode\",\"Duration Sleep Episode (Wakeup-Sleep Onset)\",\n",
    "             \"Duration Inactive in Sleep Episode\",\"Duration Light Activity in Sleep Episode\",\n",
    "             \"Duration Moderate Activity in Sleep Episode\",\"Duration Vigorous Activity in Sleep Episode\",\n",
    "             \"Duration Inactive in Day Episode\",\"Duration Light Activity in Day Episode\",\n",
    "             \"Duration Moderate Activity in Day Episode\",\"Duration Vigorous Activity in Day Episode\"]\n",
    "\n",
    "# Apply readable column names to the dataframe\n",
    "df_Actigraphy.columns = col_names\n",
    "\n",
    "# Index pointer for iterating through first-night wakeup-minus-bedtime values\n",
    "indexx = 0\n",
    "# Compute GGIR sleep efficiency and sleep episode durations row-wise\n",
    "for i in range(0,len(df_Actigraphy)):\n",
    "    # For first-night rows, use precomputed (wakeup - bedtime) duration\n",
    "    if df_Actigraphy.loc[i,\"night_number\"] == 1:\n",
    "        df_Actigraphy.loc[i,\"Sleep Efficiency GGIR (TST/(Wakeup-Sleep Onset))\"] = round(df_Actigraphy.loc[i,\"TST\"]/wakeupminusbedtime[indexx],3)\n",
    "        df_Actigraphy.loc[i,\"Duration Sleep Episode (Wakeup-Sleep Onset)\"] = wakeupminusbedtime[indexx]\n",
    "        indexx += 1\n",
    "    # For other nights, handle potential cross-midnight onset\n",
    "    else:\n",
    "        # If sleep onset is late evening, wrap to next day for total duration\n",
    "        if df_Actigraphy.loc[i,\"Sleep Onset\"] > bbb:\n",
    "            temp_sleep_onset = ccc - df_Actigraphy.loc[i,\"Sleep Onset\"]\n",
    "            df_Actigraphy.loc[i,\"Sleep Efficiency GGIR (TST/(Wakeup-Sleep Onset))\"] = round(df_Actigraphy.loc[i,\"TST\"]/(df_Actigraphy.loc[i,\"Wakeup Time\"] + temp_sleep_onset),3)\n",
    "        # Otherwise compute duration within the same day\n",
    "        else:\n",
    "            df_Actigraphy.loc[i, \"Sleep Efficiency GGIR (TST/(Wakeup-Sleep Onset))\"] = round(df_Actigraphy.loc[i, \"TST\"]/(df_Actigraphy.loc[i, \"Wakeup Time\"] - df_Actigraphy.loc[i,\"Sleep Onset\"]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow full column display in pandas output\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display selected duration-related actigraphy columns including WASO\n",
    "df_Actigraphy[[\"Duration Day+Sleep episodes\",\"Duration Day Episode\",\"Duration Sleep Episode (Wakeup-Sleep Onset)\",\n",
    "             \"Duration Inactive in Sleep Episode\",\"Duration Light Activity in Sleep Episode\",\n",
    "             \"Duration Moderate Activity in Sleep Episode\",\"Duration Vigorous Activity in Sleep Episode\",\n",
    "             \"Duration Inactive in Day Episode\",\"Duration Light Activity in Day Episode\",\n",
    "             \"Duration Moderate Activity in Day Episode\",\"Duration Vigorous Activity in Day Episode\",\"WASO\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3cc8b",
   "metadata": {},
   "source": [
    "## 3.4. Round time and sleep efficiency metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns containing time-like values that require formatting\n",
    "time_cols = [\n",
    "    \"Bedtime\",\n",
    "    \"Sleep Onset\",\n",
    "    \"Wakeup Time\",\n",
    "    \"Rise Time\",\n",
    "    \"SOL\",\n",
    "    \"TST\",\n",
    "    \"WASO\",\n",
    "    \"TASAFA\",\n",
    "    \"DSE\",\n",
    "    \"Duration Day+Sleep episodes\",\n",
    "    \"Duration Day Episode\",\n",
    "    \"Duration Sleep Episode (Wakeup-Sleep Onset)\",\n",
    "    \"Duration Inactive in Sleep Episode\",\n",
    "    \"Duration Light Activity in Sleep Episode\",\n",
    "    \"Duration Moderate Activity in Sleep Episode\",\n",
    "    \"Duration Vigorous Activity in Sleep Episode\",\n",
    "    \"Duration Inactive in Day Episode\",\n",
    "    \"Duration Light Activity in Day Episode\",\n",
    "    \"Duration Moderate Activity in Day Episode\",\n",
    "    \"Duration Vigorous Activity in Day Episode\"\n",
    "]\n",
    "\n",
    "# Convert selected columns to timedelta format, coercing invalid values\n",
    "df_Actigraphy[time_cols] = df_Actigraphy[time_cols].apply(pd.to_timedelta, errors=\"coerce\")\n",
    "\n",
    "# Loop through each time column to convert timedelta values into HH:MM:SS strings\n",
    "for col in time_cols:\n",
    "    df_Actigraphy[col] = df_Actigraphy[col].apply(\n",
    "        lambda x: (\n",
    "            f\"{int(x.total_seconds() // 3600):02d}:\"\n",
    "            f\"{int((x.total_seconds() % 3600) // 60):02d}:\"\n",
    "            f\"{int(x.total_seconds() % 60):02d}\"\n",
    "        ) if pd.notna(x) else \"\"\n",
    "    )\n",
    "\n",
    "# List of numeric columns that must be rounded to 2 decimal places\n",
    "cols_to_round = [\n",
    "    \"nonwear_perc\",\n",
    "    \"Sleep Efficiency GGIR (TST/(Wakeup-Sleep Onset))\",\n",
    "    \"Sleep Efficiency TST(Act)/DSE(Act)\"\n",
    "]\n",
    "\n",
    "# Round numeric percentage and efficiency columns\n",
    "df_Actigraphy[cols_to_round] = df_Actigraphy[cols_to_round].round(2)\n",
    "\n",
    "# Allow unlimited column width when printing the dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display selected duration-related columns including WASO\n",
    "df_Actigraphy[[\"Duration Day+Sleep episodes\",\"Duration Day Episode\",\"Duration Sleep Episode (Wakeup-Sleep Onset)\",\n",
    "             \"Duration Inactive in Sleep Episode\",\"Duration Light Activity in Sleep Episode\",\n",
    "             \"Duration Moderate Activity in Sleep Episode\",\"Duration Vigorous Activity in Sleep Episode\",\n",
    "             \"Duration Inactive in Day Episode\",\"Duration Light Activity in Day Episode\",\n",
    "             \"Duration Moderate Activity in Day Episode\",\"Duration Vigorous Activity in Day Episode\",\"WASO\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbbcc5",
   "metadata": {},
   "source": [
    "## 3.5. View actigraphy database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef65880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enable full display of all dataframe columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the entire df_Actigraphy dataframe\n",
    "df_Actigraphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83fc7d",
   "metadata": {},
   "source": [
    "# 4. Working memory (WM) tasks practice session database\n",
    "Generates DB with the performance data in the WM tasks collected in the practice sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaaa6f",
   "metadata": {},
   "source": [
    "## 4.1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b423216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file path utilities\n",
    "from pathlib import Path\n",
    "# Import pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Set project root directory one level above current script\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "# Define directory containing participant CSV files\n",
    "data_dir = PROJ_ROOT / \"data_participants_practice\"\n",
    "\n",
    "# Collect and sort all CSV files in the target directory\n",
    "csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Attempt to read a CSV file using multiple common encoding and delimiter configurations\n",
    "def try_read(p):\n",
    "    # List of parameter combinations to try for reading CSV files across OS/Excel formats\n",
    "    tries = [\n",
    "        dict(sep=None, engine=\"python\", encoding=\"utf-8-sig\"),\n",
    "        dict(sep=\";\",  engine=\"python\", encoding=\"utf-8-sig\"),\n",
    "        dict(sep=\",\",  engine=\"python\", encoding=\"utf-8-sig\"),\n",
    "\n",
    "        dict(sep=None, engine=\"python\", encoding=\"latin-1\"),\n",
    "        dict(sep=\";\",  engine=\"python\", encoding=\"latin-1\"),\n",
    "        dict(sep=\",\",  engine=\"python\", encoding=\"latin-1\"),\n",
    "\n",
    "        dict(sep=\"\\t\", engine=\"python\", encoding=\"utf-16\"),\n",
    "        dict(sep=\",\",  engine=\"python\", encoding=\"utf-16\"),\n",
    "        dict(sep=\";\",  engine=\"python\", encoding=\"utf-16\"),\n",
    "    ]\n",
    "\n",
    "    # Store most recent error to raise if all attempts fail\n",
    "    last_err = None\n",
    "    # Try reading the file with each configuration\n",
    "    for kw in tries:\n",
    "        try:\n",
    "            return pd.read_csv(str(p), error_bad_lines=False, warn_bad_lines=False, **kw)\n",
    "        except TypeError:\n",
    "            # Retry using only supported kwargs if pandas version is old\n",
    "            try:\n",
    "                return pd.read_csv(str(p), **{k:v for k,v in kw.items() if k in (\"sep\",\"engine\",\"encoding\")})\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    # Raise final error after exhausting all attempts\n",
    "    raise last_err\n",
    "\n",
    "# List to accumulate successfully read dataframes\n",
    "dfs = []\n",
    "# Iterate through all CSV file paths and read them into dataframes\n",
    "for p in csv_files:\n",
    "    df = try_read(p)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single combined dataframe\n",
    "df_total_part = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Reset index to ensure proper sequential indexing\n",
    "df_total_part.reset_index(inplace=True)\n",
    "\n",
    "# Replace occurrences of the string 'undefined' with missing values 'Nan'\n",
    "df_total_part = df_total_part.replace('undefined','Nan')\n",
    "\n",
    "# Move Task_Name column to first position and subject_nr to second position\n",
    "first_column = df_total_part.pop('Task_Name')\n",
    "second_column = df_total_part.pop('subject_nr')\n",
    "df_total_part.insert(0, 'Task_Name', first_column)\n",
    "df_total_part.insert(1, 'subject_nr', second_column)\n",
    "\n",
    "# Display all columns when printing dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display final combined participants dataframe\n",
    "df_total_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e28ea",
   "metadata": {},
   "source": [
    "## 4.2. Extracts and cleans practice-session data for each WM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497321b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter Reading Span practice trials\n",
    "df_Reading_Span_Practice = df_total_part[df_total_part['Task_Name'] == 'Reading Span']\n",
    "# Sort Reading Span rows by participant number\n",
    "df_Reading_Span_Practice = df_Reading_Span_Practice.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "\n",
    "# Filter Working Memory Updating Task practice trials\n",
    "df_WMU_Task_Practice = df_total_part[df_total_part['Task_Name'] == 'Working Memory Updating Task']\n",
    "# Sort WMU rows by participant number\n",
    "df_WMU_Task_Practice = df_WMU_Task_Practice.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "\n",
    "# Filter Symmetry Span practice trials\n",
    "df_Symmetry_Span_Practice = df_total_part[df_total_part['Task_Name'] == 'Symmetry Span']\n",
    "# Sort Symmetry Span rows by participant number\n",
    "df_Symmetry_Span_Practice = df_Symmetry_Span_Practice.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "\n",
    "# Filter Binding Task practice trials\n",
    "df_Binding_Task_Practice = df_total_part[df_total_part['Task_Name'] == 'Binding Task']\n",
    "# Sort Binding Task rows by participant number\n",
    "df_Binding_Task_Practice = df_Binding_Task_Practice.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "\n",
    "# Filter Operation Span practice trials\n",
    "df_Operation_Span_Practice = df_total_part[df_total_part['Task_Name'] == 'Operation Span']\n",
    "# Sort Operation Span rows by participant number\n",
    "df_Operation_Span_Practice = df_Operation_Span_Practice.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "\n",
    "########################################################################################################\n",
    "# Select relevant columns for Reading Span, convert formats, clean strings, and rename columns for clarity\n",
    "df_Reading_Span_Practice = df_Reading_Span_Practice[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'Frase', 'height', 'letter', 'List_Prev_Letter',\n",
    "     'List_responses_memory', 'live_row', 'logfile', 'response_average_time_memory', 'response_memory',\n",
    "     'response_processing', 'response_time_memory', 'response_time_processing', 'response_total_time_memory',\n",
    "     'RP_part_process_time', 'score_practice', 'score_reading_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4',\n",
    "     'score_subblock_5', 'score_subblock_6', 'Tipo', 'total_correct',\n",
    "     'total_response_time', 'total_responses', 'width']]\n",
    "# Replace commas with dots before numeric conversion\n",
    "df_Reading_Span_Practice[['acc', 'avg_rt']] = df_Reading_Span_Practice[['acc', 'avg_rt']].replace(',', '.')\n",
    "# Convert selected text columns to appropriate numeric/string types\n",
    "df_Reading_Span_Practice = df_Reading_Span_Practice.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "# Identify a recurring unwanted string in processing responses\n",
    "example_b = df_Reading_Span_Practice[\"response_processing\"].iloc[2]\n",
    "# Remove the identified unwanted string from the dataframe\n",
    "df_Reading_Span_Practice = df_Reading_Span_Practice.replace(example_b, '')\n",
    "# Rename all columns to practice-session–specific labels\n",
    "df_Reading_Span_Practice.columns = ['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco',\n",
    "                             'SubTaskName','acc_Practice_Sess','avg_rt_Practice_Sess','BlockChoice_Practice_Sess','correct_Practice_Sess','correct_response_Practice_Sess',\n",
    "                             'Frase_Practice_Sess','height_Practice_Sess','letter_Practice_Sess','List_Prev_Letter_Practice_Sess','List_responses_memory_Practice_Sess',\n",
    "                             'live_row_Practice_Sess','logfile_Practice_Sess','response_average_time_memory_Practice_Sess','response_memory_Practice_Sess',\n",
    "                             'response_processing_Practice_Sess','response_time_memory_Practice_Sess','response_time_processing_Practice_Sess',\n",
    "                             'response_total_time_memory_Practice_Sess','RP_part_process_time_Practice_Sess','score_practice_Practice_Sess',\n",
    "                             'score_reading_span_Practice_Sess','score_subblock_2_Practice_Sess','score_subblock_3_Practice_Sess','score_subblock_4_Practice_Sess',\n",
    "                             'score_subblock_5_Practice_Sess','score_subblock_6_Practice_Sess','Tipo_Practice_Sess','total_correct_Practice_Sess',\n",
    "                             'total_response_time_Practice_Sess','total_responses_Practice_Sess','width_Practice_Sess']\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "# Select WMU task columns, clean missing RTs, and rename columns for consistency\n",
    "df_WMU_Task_Practice = df_WMU_Task_Practice[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'correct1', 'correct2', 'correct3', 'digit1', 'digit2', 'digit3', 'height',\n",
    "     'Index_List', 'live_row', 'logfile', 'response1', 'response2', 'response3', 'response_time1', 'responseavgRT',\n",
    "     'total_correct_trial', 'TotalRtBlock',  'WMUExperimentalScore', 'WMUPracticeScore', 'width']]\n",
    "# Rename primary response time column\n",
    "df_WMU_Task_Practice = df_WMU_Task_Practice.rename(columns={'response_time1': 'response_time'})\n",
    "\n",
    "# Replace \"0\" average RT entries with empty strings\n",
    "for i in range(0, len(df_WMU_Task_Practice['responseavgRT'])):\n",
    "    if df_WMU_Task_Practice['responseavgRT'].iloc[i] == 0:\n",
    "        df_WMU_Task_Practice['responseavgRT'].iloc[i] = ''\n",
    "# Assign cleaned and standardized column names\n",
    "df_WMU_Task_Practice.columns = ['subject_nr','CB_ref','practice','TrialNumber','correct1_Practice_Sess',\n",
    "                         'correct2_Practice_Sess','correct3_Practice_Sess','digit1_Practice_Sess','digit2_Practice_Sess','digit3_Practice_Sess','height_Practice_Sess',\n",
    "                         'Index_List_Practice_Sess','live_row_Practice_Sess','logfile_Practice_Sess','response1_Practice_Sess','response2_Practice_Sess','response3_Practice_Sess',\n",
    "                         'response_time1_Practice_Sess','responseavgRT_Practice_Sess','total_correct_trial_Practice_Sess','TotalRtBlock_Practice_Sess',\n",
    "                         'WMUExperimentalScore_Practice_Sess','WMUPracticeScore_Practice_Sess','width_Practice_Sess']\n",
    "\n",
    "############################################################################################\n",
    "# Select Symmetry Span columns, clean text fields, and rename columns\n",
    "df_Symmetry_Span_Practice = df_Symmetry_Span_Practice[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName',\n",
    "     'aggregated_score_memory', 'average_response_time_processing', 'average_total_time_memory', 'correct',\n",
    "     'correct_response', 'countDys', 'countSym', 'height', 'LeftHalfPos', 'List_SS_button', 'List_SS_Pos', 'live_row',\n",
    "     'logfile', 'maxDys', 'maxSym', 'pressed_buttons', 'response_memory', 'response_processing', 'response_time_memory',\n",
    "     'response_time_processing', 'response_total_time_memory', 'response_total_time_memory_full_task', 'RightHalfPos',\n",
    "     'SP_part_process_time', 'SS_practice_score', 'score_symmetry_span', 'score_subblock_2', 'score_subblock_3',\n",
    "     'score_subblock_4', 'score_subblock_5', 'score_subblock_6', 'SymType',\n",
    "     'total_correct_processing', 'total_response_time_processing', 'width']]\n",
    "# Convert text-like response columns to string type\n",
    "df_Symmetry_Span_Practice = df_Symmetry_Span_Practice.astype({'correct_response': 'str', 'response_processing': 'str'})\n",
    "# Identify unwanted repeated processing string\n",
    "example_d = df_Symmetry_Span_Practice[\"response_processing\"].iloc[2]\n",
    "# Remove occurrences of the unwanted string\n",
    "df_Symmetry_Span_Practice = df_Symmetry_Span_Practice.replace(example_d, '')\n",
    "# Rename columns to practice-session format\n",
    "df_Symmetry_Span_Practice.columns = ['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco',\n",
    "                              'SubTaskName','aggregated_score_memory_Practice_Sess','average_response_time_processing_Practice_Sess',\n",
    "                              'average_total_time_memory_Practice_Sess','correct_Practice_Sess','correct_response_Practice_Sess','countDys_Practice_Sess',\n",
    "                              'countSym_Practice_Sess','height_Practice_Sess','LeftHalfPos_Practice_Sess','List_SS_button_Practice_Sess','List_SS_Pos_Practice_Sess',\n",
    "                              'live_row_Practice_Sess','logfile_Practice_Sess','maxDys_Practice_Sess','maxSym_Practice_Sess','pressed_buttons_Practice_Sess',\n",
    "                              'response_memory_Practice_Sess','response_processing_Practice_Sess','response_time_memory_Practice_Sess',\n",
    "                              'response_time_processing_Practice_Sess','response_total_time_memory_Practice_Sess',\n",
    "                              'response_total_time_memory_full_task_Practice_Sess','RightHalfPos_Practice_Sess','SP_part_process_time_Practice_Sess',\n",
    "                              'SS_practice_score_Practice_Sess','score_symmetry_span_Practice_Sess','score_subblock_2_Practice_Sess','score_subblock_3_Practice_Sess',\n",
    "                              'score_subblock_4_Practice_Sess','score_subblock_5_Practice_Sess','score_subblock_6_Practice_Sess','SymType_Practice_Sess',\n",
    "                              'total_correct_processing_Practice_Sess','total_response_time_processing_Practice_Sess','width_Practice_Sess']\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "# Select Binding Task columns, convert numeric formats, and rename for consistency\n",
    "df_Binding_Task_Practice = df_Binding_Task_Practice[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'acc', 'average_response_time',\n",
    "     'BindingRawScore', 'correct', 'correct_response', 'counter', 'Delay', 'eightsec_accuracy', 'FalseAlarms', 'height',\n",
    "     'Hits', 'live_row', 'logfile', 'match_1s_accuracy', 'match_1s_avg_rt', 'match_8s_accuracy', 'match_8s_avg_rt',\n",
    "     'mismatch_1s_accuracy', 'mismatch_1s_avg_rt', 'mismatch_8s_accuracy', 'mismatch_8s_avg_rt', 'NNonResponses',\n",
    "     'Omissions', 'onesec_accuracy', 'QuinetteAccuracyScore', 'QuinetteProcessingScore', 'response',\n",
    "     'response_time', 'ResponsesGiven', 'total_correct', 'total_match_1s_rt', 'total_match_8s_rt',\n",
    "     'total_mismatch_1s_rt', 'total_mismatch_8s_rt', 'total_response_time', 'total_responses', 'width']]\n",
    "# Replace commas with dots before float casting\n",
    "df_Binding_Task_Practice[['acc', 'average_response_time']] = df_Binding_Task_Practice[['acc', 'average_response_time']].replace(',', '.')\n",
    "# Convert numerical and text fields to proper types\n",
    "df_Binding_Task_Practice = df_Binding_Task_Practice.astype(\n",
    "    {'acc': 'float64', 'average_response_time': 'float64', 'correct_response': 'str', 'response': 'str'})\n",
    "# Rename Binding Task columns to practice-session format\n",
    "df_Binding_Task_Practice.columns = ['subject_nr','CB_ref','practice','TrialNumber','acc_Practice_Sess',\n",
    "                             'average_response_time_Practice_Sess','BindingRawScore_Practice_Sess','correct_Practice_Sess','correct_response_Practice_Sess',\n",
    "                             'counter_Practice_Sess','Delay_Practice_Sess','eightsec_accuracy_Practice_Sess','FalseAlarms_Practice_Sess','height_Practice_Sess','Hits_Practice_Sess',\n",
    "                             'live_row_Practice_Sess','logfile_Practice_Sess','match_1s_accuracy_Practice_Sess','match_1s_avg_rt_Practice_Sess','match_8s_accuracy_Practice_Sess',\n",
    "                             'match_8s_avg_rt_Practice_Sess','mismatch_1s_accuracy_Practice_Sess','mismatch_1s_avg_rt_Practice_Sess','mismatch_8s_accuracy_Practice_Sess',\n",
    "                             'mismatch_8s_avg_rt_Practice_Sess','NNonResponses_Practice_Sess','Omissions_Practice_Sess','onesec_accuracy_Practice_Sess',\n",
    "                             'QuinetteAccuracyScore_Practice_Sess','QuinetteProcessingScore_Practice_Sess','response_Practice_Sess','response_time_Practice_Sess',\n",
    "                             'ResponsesGiven_Practice_Sess','total_correct_Practice_Sess','total_match_1s_rt_Practice_Sess','total_match_8s_rt_Practice_Sess',\n",
    "                             'total_mismatch_1s_rt_Practice_Sess','total_mismatch_8s_rt_Practice_Sess','total_response_time_Practice_Sess','total_responses_Practice_Sess',\n",
    "                             'width_Practice_Sess']\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "# Select Operation Span columns, convert formats, remove unwanted strings, and rename\n",
    "df_Operation_Span_Practice = df_Operation_Span_Practice[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'height', 'letter', 'List_Prev_Letter', 'List_responses_memory',\n",
    "     'live_row', 'logfile', 'response_average_time_memory', 'response_memory', 'response_processing',\n",
    "     'response_time_memory', 'response_time_processing', 'response_total_time_memory', 'OP_part_process_time',\n",
    "     'score_practice', 'score_operation_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4', 'score_subblock_5',\n",
    "     'score_subblock_6', 'Tipo', 'total_correct', 'total_response_time',\n",
    "     'total_responses', 'width']]\n",
    "# Fix comma decimal separators before numeric conversion\n",
    "df_Operation_Span_Practice[['acc', 'avg_rt']] = df_Operation_Span_Practice[['acc', 'avg_rt']].replace(',', '.')\n",
    "# Convert selected fields to numeric or string types\n",
    "df_Operation_Span_Practice = df_Operation_Span_Practice.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "# Identify unwanted string in processing responses\n",
    "example_c = df_Operation_Span_Practice[\"response_processing\"].iloc[2]\n",
    "# Remove the unwanted string from the dataframe\n",
    "df_Operation_Span_Practice = df_Operation_Span_Practice.replace(example_c, '')\n",
    "# Rename Operation Span columns to practice-session format\n",
    "df_Operation_Span_Practice.columns = ['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco',\n",
    "                               'SubTaskName','acc_Practice_Sess','avg_rt_Practice_Sess','BlockChoice_Practice_Sess','correct_Practice_Sess',\n",
    "                               'correct_response_Practice_Sess','height_Practice_Sess','letter_Practice_Sess','List_Prev_Letter_Practice_Sess',\n",
    "                               'List_responses_memory_Practice_Sess','live_row_Practice_Sess','logfile_Practice_Sess','response_average_time_memory_Practice_Sess',\n",
    "                               'response_memory_Practice_Sess','response_processing_Practice_Sess','response_time_memory_Practice_Sess',\n",
    "                               'response_time_processing_Practice_Sess','response_total_time_memory_Practice_Sess','OP_part_process_time_Practice_Sess',\n",
    "                               'score_practice_Practice_Sess','score_operation_span_Practice_Sess','score_subblock_2_Practice_Sess','score_subblock_3_Practice_Sess',\n",
    "                               'score_subblock_4_Practice_Sess','score_subblock_5_Practice_Sess','score_subblock_6_Practice_Sess','Tipo_Practice_Sess',\n",
    "                               'total_correct_Practice_Sess','total_response_time_Practice_Sess','total_responses_Practice_Sess','width_Practice_Sess']\n",
    "\n",
    "# Reset index and drop the old index column\n",
    "df_Reading_Span_Practice = df_Reading_Span_Practice.reset_index(drop=True)\n",
    "\n",
    "# Configure pandas to show all columns when printing\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Display cleaned and formatted Reading Span practice dataframe\n",
    "df_Reading_Span_Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b7ea8",
   "metadata": {},
   "source": [
    "## 4.3. Generates database with raw scores in the five WM tasks (practice session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe to store raw scores for each task\n",
    "df_raw_scores = pd.DataFrame()\n",
    "\n",
    "# Insert subject number column into the raw scores dataframe\n",
    "subj_nr = df_total_part[\"subject_nr\"].unique()\n",
    "df_raw_scores.insert(0,'subject_nr',subj_nr)\n",
    "\n",
    "##############################################################################################################\n",
    "# Load temperature data and map temperature values to participants\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Build the path to the temperature Excel file\n",
    "excel_part_data_path = PROJ_ROOT / \"Temperature\" / \"Body_Temperature_Collection.xlsx\"\n",
    "Temperature = pd.read_excel(excel_part_data_path, sheet_name='TempPractice')\n",
    "\n",
    "# Convert subject numbers to integer for matching\n",
    "aa = df_raw_scores[\"subject_nr\"].astype(\"int\")\n",
    "\n",
    "# Iterate through temperature file and assign temperature values to matching subjects\n",
    "ii = 0\n",
    "k = 0\n",
    "for i in Temperature[\"Subject ID\"]:\n",
    "    if int(i) in aa.values:\n",
    "        df_raw_scores[\"Temperature (°C) Practice Sess\"] = Temperature[\"Temperature (°C) Practice Sess\"][ii]\n",
    "    ii += 1\n",
    "    k =+ 1\n",
    "\n",
    "########################################################################################################\n",
    "# Compute and insert raw scores for each WM task based on max performance per participant\n",
    "RawRS = list(df_Reading_Span_Practice.groupby(['subject_nr'], sort=True)['score_reading_span_Practice_Sess'].max() * 20)\n",
    "df_raw_scores[\"Reading Span Practice Session\"] = RawRS\n",
    "\n",
    "RawUT = list(df_WMU_Task_Practice.groupby(['subject_nr'], sort=True)['WMUExperimentalScore_Practice_Sess'].max())\n",
    "df_raw_scores[\"Updating Task Practice Session\"] = RawUT\n",
    "\n",
    "RawSS = list(df_Symmetry_Span_Practice.groupby(['subject_nr'], sort=True)['score_symmetry_span_Practice_Sess'].max() * 20)\n",
    "df_raw_scores[\"Symmetry Span Practice Session\"] = RawSS\n",
    "\n",
    "RawBT = list(df_Binding_Task_Practice.groupby(['subject_nr'], sort=True)['BindingRawScore_Practice_Sess'].max())\n",
    "df_raw_scores[\"Binding Task Practice Session\"] = RawBT\n",
    "\n",
    "RawOS = list(df_Operation_Span_Practice.groupby(['subject_nr'], sort=True)['score_operation_span_Practice_Sess'].max() * 20)\n",
    "df_raw_scores[\"Operation Span Practice Session\"] = RawOS\n",
    "\n",
    "# Convert subject number column to numeric and sort dataframe\n",
    "df_raw_scores[\"subject_nr\"] = pd.to_numeric(df_raw_scores[\"subject_nr\"], errors=\"coerce\")\n",
    "df_raw_scores = df_raw_scores.sort_values(by=\"subject_nr\").reset_index(drop=True)\n",
    "\n",
    "# Convert all columns except subject number and temperature to integers\n",
    "df_raw_scores = df_raw_scores.astype({\n",
    "    col: 'int' \n",
    "    for col in df_raw_scores.columns \n",
    "    if col not in ['subject_nr', 'Temperature (°C) Practice Sess']\n",
    "})\n",
    "\n",
    "# Create a sorted copy of the raw scores dataframe\n",
    "df_raw_scores_pract = df_raw_scores.sort_values(by=\"subject_nr\")\n",
    "\n",
    "# Show all columns when printing\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display final practice-session raw scores dataframe\n",
    "df_raw_scores_pract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f371a39",
   "metadata": {},
   "source": [
    "## 4.4. Generates database with normalized scores in the five WM tasks (practice session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949512d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################################################\n",
    "# Create dataframe containing normalized WM scores and socio-demographic data\n",
    "df_normalized_scores = df_raw_scores.copy(deep=True)\n",
    "\n",
    "# Normalize Reading Span scores by maximum possible score\n",
    "df_normalized_scores[\"Reading Span Practice Session\"] = df_normalized_scores[\"Reading Span Practice Session\"]/20\n",
    "# Normalize Updating Task scores by maximum score\n",
    "df_normalized_scores[\"Updating Task Practice Session\"] = df_normalized_scores[\"Updating Task Practice Session\"] / 36\n",
    "# Normalize Symmetry Span scores by maximum possible score\n",
    "df_normalized_scores[\"Symmetry Span Practice Session\"] = df_normalized_scores[\"Symmetry Span Practice Session\"] / 20\n",
    "# Normalize Binding Task scores by maximum possible score\n",
    "df_normalized_scores[\"Binding Task Practice Session\"] = df_normalized_scores[\"Binding Task Practice Session\"] / 12\n",
    "# Normalize Operation Span scores by maximum possible score\n",
    "df_normalized_scores[\"Operation Span Practice Session\"] = df_normalized_scores[\"Operation Span Practice Session\"] / 20\n",
    "\n",
    "# Columns requiring rounding after numeric conversion\n",
    "cols_to_round = [\n",
    "    \"Reading Span Practice Session\",\n",
    "    \"Updating Task Practice Session\",\n",
    "    \"Symmetry Span Practice Session\",\n",
    "    \"Binding Task Practice Session\",\n",
    "    \"Operation Span Practice Session\",\n",
    "]\n",
    "\n",
    "# Convert selected columns to numeric and round values to two decimals\n",
    "df_normalized_scores[cols_to_round] = (\n",
    "    df_normalized_scores[cols_to_round]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Ensure subject number is numeric and sort rows\n",
    "df_normalized_scores[\"subject_nr\"] = pd.to_numeric(df_normalized_scores[\"subject_nr\"], errors=\"coerce\")\n",
    "df_normalized_scores = df_normalized_scores.sort_values(by=\"subject_nr\").reset_index(drop=True)\n",
    "\n",
    "# Create sorted version of normalized scores dataframe\n",
    "df_normalized_scores_pract = df_normalized_scores.sort_values(by=\"subject_nr\")\n",
    "\n",
    "# Display all columns when printing\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Show normalized scores dataframe\n",
    "df_normalized_scores_pract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04662ead",
   "metadata": {},
   "source": [
    "# 5. Working memory (WM) tasks experimental session database\n",
    "Generates DB with the performance data in the WM tasks collected in the experimental sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848da14f",
   "metadata": {},
   "source": [
    "## 5.1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09b546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import utilities for filesystem paths\n",
    "from pathlib import Path\n",
    "# Import pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Define project root directory one level above current folder\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "# Define directory containing experimental participant CSV files\n",
    "data_dir = PROJ_ROOT / \"data_participants_experimental\"\n",
    "\n",
    "# Collect all CSV files in the directory, sorted alphabetically\n",
    "csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Attempt to read CSV files using several common encodings and delimiters\n",
    "def try_read(p):\n",
    "    # List of read attempts covering typical CSV formats across systems\n",
    "    tries = [\n",
    "        dict(sep=None, engine=\"python\", encoding=\"utf-8-sig\"),\n",
    "        dict(sep=\";\",  engine=\"python\", encoding=\"utf-8-sig\"),\n",
    "        dict(sep=\",\",  engine=\"python\", encoding=\"utf-8-sig\"),\n",
    "\n",
    "        dict(sep=None, engine=\"python\", encoding=\"latin-1\"),\n",
    "        dict(sep=\";\",  engine=\"python\", encoding=\"latin-1\"),\n",
    "        dict(sep=\",\",  engine=\"python\", encoding=\"latin-1\"),\n",
    "\n",
    "        dict(sep=\"\\t\", engine=\"python\", encoding=\"utf-16\"),\n",
    "        dict(sep=\",\",  engine=\"python\", encoding=\"utf-16\"),\n",
    "        dict(sep=\";\",  engine=\"python\", encoding=\"utf-16\"),\n",
    "    ]\n",
    "\n",
    "    # Track last error in case all attempts fail\n",
    "    last_err = None\n",
    "\n",
    "    # Try reading using each configuration\n",
    "    for kw in tries:\n",
    "        try:\n",
    "            return pd.read_csv(str(p), error_bad_lines=False, warn_bad_lines=False, **kw)\n",
    "        except TypeError:\n",
    "            # Retry with only compatible arguments if pandas version is older\n",
    "            try:\n",
    "                return pd.read_csv(str(p), **{k:v for k,v in kw.items() if k in (\"sep\",\"engine\",\"encoding\")})\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "\n",
    "    # Raise the last error if no attempt succeeded\n",
    "    raise last_err\n",
    "\n",
    "# List to collect all successfully loaded dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file and load it using try_read\n",
    "for p in csv_files:\n",
    "    df = try_read(p)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all participant dataframes into a single one\n",
    "df_total_part = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Reset the index to clean up the concatenation index\n",
    "df_total_part.reset_index(inplace=True)\n",
    "\n",
    "# Replace 'undefined' text entries with 'Nan' to mark missing values consistently\n",
    "df_total_part = df_total_part.replace('undefined','Nan')\n",
    "\n",
    "# Move Task_Name and subject_nr columns to be the first and second columns\n",
    "first_column = df_total_part.pop('Task_Name')\n",
    "second_column = df_total_part.pop('subject_nr')\n",
    "df_total_part.insert(0, 'Task_Name', first_column)\n",
    "df_total_part.insert(1, 'subject_nr', second_column)\n",
    "\n",
    "# Ensure all columns are fully visible when printing the dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the final combined experimental participants dataframe\n",
    "df_total_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6772b6",
   "metadata": {},
   "source": [
    "## 5.2. Extracts and cleans experimental-session data for each WM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0484a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Os seguintes 5 blocos de código criam 5 DataFrames distintas.\n",
    "#Cada uma das DataFrames vai conter a informação referente a cada uma das sete tarefas de memória de trabalho (reading span, symmetry\n",
    "# span, operation span, binding task e Updating Task) realizadas por todos os participantes.\n",
    "df_Reading_Span = df_total_part[df_total_part['Task_Name'] == 'Reading Span']\n",
    "df_Reading_Span = df_Reading_Span.sort_values(by=['selSNr'], kind='mergesort')\n",
    "df_Reading_Span = df_Reading_Span.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Reading_Span_1 = df_Reading_Span.query('selSNr == 1 and 1 <= subject_nr <= 5 or selSNr == 4 and 6 <= subject_nr <= 10 or selSNr == 3 and 11 <= subject_nr <= 15 or selSNr == 2 and 16 <= subject_nr <= 20 or selSNr == 1 and subject_nr == 21 or selSNr == 4 and subject_nr == 22 or selSNr == 3 and subject_nr == 23 or selSNr == 2 and subject_nr == 24 or selSNr == 1 and subject_nr == 25 or selSNr == 4 and subject_nr == 26 or selSNr == 3 and subject_nr == 27  or selSNr == 2 and subject_nr == 28')\n",
    "df_Reading_Span_1 = df_Reading_Span_1.reset_index(drop=True)\n",
    "df_Reading_Span_2 = df_Reading_Span.query('selSNr == 2 and 1 <= subject_nr <= 5 or selSNr == 1 and 6 <= subject_nr <= 10 or selSNr == 4 and 11 <= subject_nr <= 15 or selSNr == 3 and 16 <= subject_nr <= 20 or selSNr == 2 and subject_nr == 21 or selSNr == 1 and subject_nr == 22 or selSNr == 4 and subject_nr == 23 or selSNr == 3 and subject_nr == 24 or selSNr == 2 and subject_nr == 25 or selSNr == 1 and subject_nr == 26  or selSNr == 4 and subject_nr == 27  or selSNr == 3 and subject_nr == 28')\n",
    "df_Reading_Span_2 = df_Reading_Span_2.reset_index(drop=True)\n",
    "df_Reading_Span_3 = df_Reading_Span.query('selSNr == 3 and 1 <= subject_nr <= 5 or selSNr == 2 and 6 <= subject_nr <= 10 or selSNr == 1 and 11 <= subject_nr <= 15 or selSNr == 4 and 16 <= subject_nr <= 20 or selSNr == 3 and subject_nr == 21 or selSNr == 2 and subject_nr == 22 or selSNr == 1 and subject_nr == 23 or selSNr == 4 and subject_nr == 24 or selSNr == 3 and subject_nr == 25 or selSNr == 2 and subject_nr == 26  or selSNr == 1 and subject_nr == 27  or selSNr == 4 and subject_nr == 28')\n",
    "df_Reading_Span_3 = df_Reading_Span_3.reset_index(drop=True)\n",
    "df_Reading_Span_4 = df_Reading_Span.query('selSNr == 4 and 1 <= subject_nr <= 5 or selSNr == 3 and 6 <= subject_nr <= 10 or selSNr == 2 and 11 <= subject_nr <= 15 or selSNr == 1 and 16 <= subject_nr <= 20 or selSNr == 4 and subject_nr == 21 or selSNr == 3 and subject_nr == 22 or selSNr == 2 and subject_nr == 23 or selSNr == 1 and subject_nr == 24 or selSNr == 4 and subject_nr == 25 or selSNr == 3 and subject_nr == 26  or selSNr == 2 and subject_nr == 27  or selSNr == 1 and subject_nr == 28')\n",
    "df_Reading_Span_4 = df_Reading_Span_4.reset_index(drop=True)\n",
    "\n",
    "df_WMU_Task = df_total_part[df_total_part['Task_Name'] == 'Working Memory Updating Task']\n",
    "df_WMU_Task = df_WMU_Task.sort_values(by=['selSNr'], kind='mergesort')\n",
    "df_WMU_Task = df_WMU_Task.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_WMU_Task_1 = df_WMU_Task.query('selSNr == 1 and 1 <= subject_nr <= 5 or selSNr == 4 and 6 <= subject_nr <= 10 or selSNr == 3 and 11 <= subject_nr <= 15 or selSNr == 2 and 16 <= subject_nr <= 20 or selSNr == 1 and subject_nr == 21 or selSNr == 4 and subject_nr == 22 or selSNr == 3 and subject_nr == 23 or selSNr == 2 and subject_nr == 24 or selSNr == 1 and subject_nr == 25 or selSNr == 4 and subject_nr == 26  or selSNr == 3 and subject_nr == 27  or selSNr == 2 and subject_nr == 28')\n",
    "df_WMU_Task_1 = df_WMU_Task_1.reset_index(drop=True)\n",
    "df_WMU_Task_2 = df_WMU_Task.query('selSNr == 2 and 1 <= subject_nr <= 5 or selSNr == 1 and 6 <= subject_nr <= 10 or selSNr == 4 and 11 <= subject_nr <= 15 or selSNr == 3 and 16 <= subject_nr <= 20 or selSNr == 2 and subject_nr == 21 or selSNr == 1 and subject_nr == 22 or selSNr == 4 and subject_nr == 23 or selSNr == 3 and subject_nr == 24 or selSNr == 2 and subject_nr == 25 or selSNr == 1 and subject_nr == 26  or selSNr == 4 and subject_nr == 27  or selSNr == 3 and subject_nr == 28')\n",
    "df_WMU_Task_2 = df_WMU_Task_2.reset_index(drop=True)\n",
    "df_WMU_Task_3 = df_WMU_Task.query('selSNr == 3 and 1 <= subject_nr <= 5 or selSNr == 2 and 6 <= subject_nr <= 10 or selSNr == 1 and 11 <= subject_nr <= 15 or selSNr == 4 and 16 <= subject_nr <= 20 or selSNr == 3 and subject_nr == 21 or selSNr == 2 and subject_nr == 22 or selSNr == 1 and subject_nr == 23 or selSNr == 4 and subject_nr == 24 or selSNr == 3 and subject_nr == 25 or selSNr == 2 and subject_nr == 26  or selSNr == 1 and subject_nr == 27  or selSNr == 4 and subject_nr == 28')\n",
    "df_WMU_Task_3 = df_WMU_Task_3.reset_index(drop=True)\n",
    "df_WMU_Task_4 = df_WMU_Task.query('selSNr == 4 and 1 <= subject_nr <= 5 or selSNr == 3 and 6 <= subject_nr <= 10 or selSNr == 2 and 11 <= subject_nr <= 15 or selSNr == 1 and 16 <= subject_nr <= 20 or selSNr == 4 and subject_nr == 21 or selSNr == 3 and subject_nr == 22 or selSNr == 2 and subject_nr == 23 or selSNr == 1 and subject_nr == 24 or selSNr == 4 and subject_nr == 25 or selSNr == 3 and subject_nr == 26  or selSNr == 2 and subject_nr == 27  or selSNr == 1 and subject_nr == 28')\n",
    "df_WMU_Task_4 = df_WMU_Task_4.reset_index(drop=True)\n",
    "\n",
    "df_Symmetry_Span = df_total_part[df_total_part['Task_Name'] == 'Symmetry Span']\n",
    "df_Symmetry_Span = df_Symmetry_Span.sort_values(by=['selSNr'], kind='mergesort')\n",
    "df_Symmetry_Span = df_Symmetry_Span.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Symmetry_Span_1 = df_Symmetry_Span.query('selSNr == 1 and 1 <= subject_nr <= 5 or selSNr == 4 and 6 <= subject_nr <= 10 or selSNr == 3 and 11 <= subject_nr <= 15 or selSNr == 2 and 16 <= subject_nr <= 20 or selSNr == 1 and subject_nr == 21 or selSNr == 4 and subject_nr == 22 or selSNr == 3 and subject_nr == 23 or selSNr == 2 and subject_nr == 24 or selSNr == 1 and subject_nr == 25 or selSNr == 4 and subject_nr == 26 or selSNr == 3 and subject_nr == 27  or selSNr == 2 and subject_nr == 28')\n",
    "df_Symmetry_Span_1 = df_Symmetry_Span_1.reset_index(drop=True)\n",
    "df_Symmetry_Span_2 = df_Symmetry_Span.query('selSNr == 2 and 1 <= subject_nr <= 5 or selSNr == 1 and 6 <= subject_nr <= 10 or selSNr == 4 and 11 <= subject_nr <= 15 or selSNr == 3 and 16 <= subject_nr <= 20 or selSNr == 2 and subject_nr == 21 or selSNr == 1 and subject_nr == 22 or selSNr == 4 and subject_nr == 23 or selSNr == 3 and subject_nr == 24 or selSNr == 2 and subject_nr == 25 or selSNr == 1 and subject_nr == 26 or selSNr == 4 and subject_nr == 27  or selSNr == 3 and subject_nr == 28')\n",
    "df_Symmetry_Span_2 = df_Symmetry_Span_2.reset_index(drop=True)\n",
    "df_Symmetry_Span_3 = df_Symmetry_Span.query('selSNr == 3 and 1 <= subject_nr <= 5 or selSNr == 2 and 6 <= subject_nr <= 10 or selSNr == 1 and 11 <= subject_nr <= 15 or selSNr == 4 and 16 <= subject_nr <= 20 or selSNr == 3 and subject_nr == 21 or selSNr == 2 and subject_nr == 22 or selSNr == 1 and subject_nr == 23 or selSNr == 4 and subject_nr == 24 or selSNr == 3 and subject_nr == 25 or selSNr == 2 and subject_nr == 26 or selSNr == 1 and subject_nr == 27  or selSNr == 4 and subject_nr == 28')\n",
    "df_Symmetry_Span_3 = df_Symmetry_Span_3.reset_index(drop=True)\n",
    "df_Symmetry_Span_4 = df_Symmetry_Span.query('selSNr == 4 and 1 <= subject_nr <= 5 or selSNr == 3 and 6 <= subject_nr <= 10 or selSNr == 2 and 11 <= subject_nr <= 15 or selSNr == 1 and 16 <= subject_nr <= 20 or selSNr == 4 and subject_nr == 21 or selSNr == 3 and subject_nr == 22 or selSNr == 2 and subject_nr == 23 or selSNr == 1 and subject_nr == 24 or selSNr == 4 and subject_nr == 25 or selSNr == 3 and subject_nr == 26 or selSNr == 2 and subject_nr == 27  or selSNr == 1 and subject_nr == 28')\n",
    "df_Symmetry_Span_4 = df_Symmetry_Span_4.reset_index(drop=True)\n",
    "\n",
    "df_Binding_Task = df_total_part[df_total_part['Task_Name'] == 'Binding Task']\n",
    "df_Binding_Task = df_Binding_Task.sort_values(by=['selSNr'], kind='mergesort')\n",
    "df_Binding_Task = df_Binding_Task.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Binding_Task_1 = df_Binding_Task.query('selSNr == 1 and 1 <= subject_nr <= 5 or selSNr == 4 and 6 <= subject_nr <= 10 or selSNr == 3 and 11 <= subject_nr <= 15 or selSNr == 2 and 16 <= subject_nr <= 20 or selSNr == 1 and subject_nr == 21 or selSNr == 4 and subject_nr == 22 or selSNr == 3 and subject_nr == 23 or selSNr == 2 and subject_nr == 24 or selSNr == 1 and subject_nr == 25 or selSNr == 4 and subject_nr == 26 or selSNr == 3 and subject_nr == 27  or selSNr == 2 and subject_nr == 28')\n",
    "df_Binding_Task_1 = df_Binding_Task_1.reset_index(drop=True)\n",
    "df_Binding_Task_2 = df_Binding_Task.query('selSNr == 2 and 1 <= subject_nr <= 5 or selSNr == 1 and 6 <= subject_nr <= 10 or selSNr == 4 and 11 <= subject_nr <= 15 or selSNr == 3 and 16 <= subject_nr <= 20 or selSNr == 2 and subject_nr == 21 or selSNr == 1 and subject_nr == 22 or selSNr == 4 and subject_nr == 23 or selSNr == 3 and subject_nr == 24 or selSNr == 2 and subject_nr == 25 or selSNr == 1 and subject_nr == 26 or selSNr == 4 and subject_nr == 27  or selSNr == 3 and subject_nr == 28')\n",
    "df_Binding_Task_2 = df_Binding_Task_2.reset_index(drop=True)\n",
    "df_Binding_Task_3 = df_Binding_Task.query('selSNr == 3 and 1 <= subject_nr <= 5 or selSNr == 2 and 6 <= subject_nr <= 10 or selSNr == 1 and 11 <= subject_nr <= 15 or selSNr == 4 and 16 <= subject_nr <= 20 or selSNr == 3 and subject_nr == 21 or selSNr == 2 and subject_nr == 22 or selSNr == 1 and subject_nr == 23 or selSNr == 4 and subject_nr == 24 or selSNr == 3 and subject_nr == 25 or selSNr == 2 and subject_nr == 26 or selSNr == 1 and subject_nr == 27  or selSNr == 4 and subject_nr == 28')\n",
    "df_Binding_Task_3 = df_Binding_Task_3.reset_index(drop=True)\n",
    "df_Binding_Task_4 = df_Binding_Task.query('selSNr == 4 and 1 <= subject_nr <= 5 or selSNr == 3 and 6 <= subject_nr <= 10 or selSNr == 2 and 11 <= subject_nr <= 15 or selSNr == 1 and 16 <= subject_nr <= 20 or selSNr == 4 and subject_nr == 21 or selSNr == 3 and subject_nr == 22 or selSNr == 2 and subject_nr == 23 or selSNr == 1 and subject_nr == 24 or selSNr == 4 and subject_nr == 25 or selSNr == 3 and subject_nr == 26 or selSNr == 2 and subject_nr == 27  or selSNr == 1 and subject_nr == 28')\n",
    "df_Binding_Task_4 = df_Binding_Task_4.reset_index(drop=True)\n",
    "\n",
    "df_Operation_Span = df_total_part[df_total_part['Task_Name'] == 'Operation Span']\n",
    "df_Operation_Span = df_Operation_Span.sort_values(by=['selSNr'], kind='mergesort')\n",
    "df_Operation_Span = df_Operation_Span.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Operation_Span_1 = df_Operation_Span.query('selSNr == 1 and 1 <= subject_nr <= 5 or selSNr == 4 and 6 <= subject_nr <= 10 or selSNr == 3 and 11 <= subject_nr <= 15 or selSNr == 2 and 16 <= subject_nr <= 20 or selSNr == 1 and subject_nr == 21 or selSNr == 4 and subject_nr == 22 or selSNr == 3 and subject_nr == 23 or selSNr == 2 and subject_nr == 24 or selSNr == 1 and subject_nr == 25 or selSNr == 4 and subject_nr == 26 or selSNr == 3 and subject_nr == 27  or selSNr == 2 and subject_nr == 28')\n",
    "df_Operation_Span_1 = df_Operation_Span_1.reset_index(drop=True)\n",
    "df_Operation_Span_2 = df_Operation_Span.query('selSNr == 2 and 1 <= subject_nr <= 5 or selSNr == 1 and 6 <= subject_nr <= 10 or selSNr == 4 and 11 <= subject_nr <= 15 or selSNr == 3 and 16 <= subject_nr <= 20 or selSNr == 2 and subject_nr == 21 or selSNr == 1 and subject_nr == 22 or selSNr == 4 and subject_nr == 23 or selSNr == 3 and subject_nr == 24 or selSNr == 2 and subject_nr == 25 or selSNr == 1 and subject_nr == 26 or selSNr == 4 and subject_nr == 27  or selSNr == 3 and subject_nr == 28')\n",
    "df_Operation_Span_2 = df_Operation_Span_2.reset_index(drop=True)\n",
    "df_Operation_Span_3 = df_Operation_Span.query('selSNr == 3 and 1 <= subject_nr <= 5 or selSNr == 2 and 6 <= subject_nr <= 10 or selSNr == 1 and 11 <= subject_nr <= 15 or selSNr == 4 and 16 <= subject_nr <= 20 or selSNr == 3 and subject_nr == 21 or selSNr == 2 and subject_nr == 22 or selSNr == 1 and subject_nr == 23 or selSNr == 4 and subject_nr == 24 or selSNr == 3 and subject_nr == 25 or selSNr == 2 and subject_nr == 26 or selSNr == 1 and subject_nr == 27  or selSNr == 4 and subject_nr == 28')\n",
    "df_Operation_Span_3 = df_Operation_Span_3.reset_index(drop=True)\n",
    "df_Operation_Span_4 = df_Operation_Span.query('selSNr == 4 and 1 <= subject_nr <= 5 or selSNr == 3 and 6 <= subject_nr <= 10 or selSNr == 2 and 11 <= subject_nr <= 15 or selSNr == 1 and 16 <= subject_nr <= 20 or selSNr == 4 and subject_nr == 21 or selSNr == 3 and subject_nr == 22 or selSNr == 2 and subject_nr == 23 or selSNr == 1 and subject_nr == 24 or selSNr == 4 and subject_nr == 25 or selSNr == 3 and subject_nr == 26 or selSNr == 2 and subject_nr == 27  or selSNr == 1 and subject_nr == 28')\n",
    "df_Operation_Span_4 = df_Operation_Span_4.reset_index(drop=True)\n",
    "\n",
    "#Os próximos 5 blocos de código selecionam as colunas com informação relevante de cada WM task e guardam estas colunas em DataFrames\n",
    "#que só contêm informação relacionada com a mesma tarefa. Para além disso, estes nestes 5 blocos de código, são realizadas algumas\n",
    "#conversões no formato dos dados (e.g., string to float) e são alterados os nomes de algumas colunas de forma a ficarem mais percétiveis.\n",
    "#'selSNr',\n",
    "df_Reading_Span_1 = df_Reading_Span_1[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'Frase', 'height', 'letter', 'List_Prev_Letter',\n",
    "     'List_responses_memory', 'live_row', 'logfile', 'response_average_time_memory', 'response_memory',\n",
    "     'response_processing', 'response_time_memory', 'response_time_processing', 'response_total_time_memory',\n",
    "     'RP_part_process_time', 'score_practice', 'score_reading_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4',\n",
    "     'score_subblock_5', 'score_subblock_6', 'Tipo', 'total_correct',\n",
    "     'total_response_time', 'total_responses', 'width']]\n",
    "df_Reading_Span_1[['acc', 'avg_rt']] = df_Reading_Span_1[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Reading_Span_1 = df_Reading_Span_1.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_b = df_Reading_Span_1[\"response_processing\"].iloc[2]\n",
    "df_Reading_Span_1 = df_Reading_Span_1.replace(example_b, '')\n",
    "#df_Reading_Span_1 = df_Reading_Span_1.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Reading_Span_1 = df_Reading_Span_1.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Reading_Span_1.columns = ['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco',\n",
    "                             'SubTaskName','acc_Sess09h00','avg_rt_Sess09h00','BlockChoice_Sess09h00','correct_Sess09h00','correct_response_Sess09h00',\n",
    "                             'Frase_Sess09h00','height_Sess09h00','letter_Sess09h00','List_Prev_Letter_Sess09h00','List_responses_memory_Sess09h00',\n",
    "                             'live_row_Sess09h00','logfile_Sess09h00','response_average_time_memory_Sess09h00','response_memory_Sess09h00',\n",
    "                             'response_processing_Sess09h00','response_time_memory_Sess09h00','response_time_processing_Sess09h00',\n",
    "                             'response_total_time_memory_Sess09h00','RP_part_process_time_Sess09h00','score_practice_Sess09h00',\n",
    "                             'score_reading_span_Sess09h00','score_subblock_2_Sess09h00','score_subblock_3_Sess09h00','score_subblock_4_Sess09h00',\n",
    "                             'score_subblock_5_Sess09h00','score_subblock_6_Sess09h00','Tipo_Sess09h00','total_correct_Sess09h00',\n",
    "                             'total_response_time_Sess09h00','total_responses_Sess09h00','width_Sess09h00']\n",
    "#'selSNr',\n",
    "df_Reading_Span_2 = df_Reading_Span_2[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'Frase', 'height', 'letter', 'List_Prev_Letter',\n",
    "     'List_responses_memory', 'live_row', 'logfile', 'response_average_time_memory', 'response_memory',\n",
    "     'response_processing', 'response_time_memory', 'response_time_processing', 'response_total_time_memory',\n",
    "     'RP_part_process_time', 'score_practice', 'score_reading_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4',\n",
    "     'score_subblock_5', 'score_subblock_6', 'Tipo', 'total_correct',\n",
    "     'total_response_time', 'total_responses', 'width']]\n",
    "df_Reading_Span_2[['acc', 'avg_rt']] = df_Reading_Span_2[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Reading_Span_2 = df_Reading_Span_2.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_b = df_Reading_Span_2[\"response_processing\"].iloc[2]\n",
    "df_Reading_Span_2 = df_Reading_Span_2.replace(example_b, '')\n",
    "df_Reading_Span_2.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "#df_Reading_Span_2 = df_Reading_Span_2.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Reading_Span_2 = df_Reading_Span_2.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Reading_Span_2.columns = ['acc_Sess13h00','avg_rt_Sess13h00','BlockChoice_Sess13h00','correct_Sess13h00','correct_response_Sess13h00',\n",
    "                             'Frase_Sess13h00','height_Sess13h00','letter_Sess13h00','List_Prev_Letter_Sess13h00','List_responses_memory_Sess13h00',\n",
    "                             'live_row_Sess13h00','logfile_Sess13h00','response_average_time_memory_Sess13h00','response_memory_Sess13h00',\n",
    "                             'response_processing_Sess13h00','response_time_memory_Sess13h00','response_time_processing_Sess13h00',\n",
    "                             'response_total_time_memory_Sess13h00','RP_part_process_time_Sess13h00','score_practice_Sess13h00',\n",
    "                             'score_reading_span_Sess13h00','score_subblock_2_Sess13h00','score_subblock_3_Sess13h00','score_subblock_4_Sess13h00',\n",
    "                             'score_subblock_5_Sess13h00','score_subblock_6_Sess13h00','Tipo_Sess13h00','total_correct_Sess13h00',\n",
    "                             'total_response_time_Sess13h00','total_responses_Sess13h00','width_Sess13h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Reading_Span_3 = df_Reading_Span_3[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'Frase', 'height', 'letter', 'List_Prev_Letter',\n",
    "     'List_responses_memory', 'live_row', 'logfile', 'response_average_time_memory', 'response_memory',\n",
    "     'response_processing', 'response_time_memory', 'response_time_processing', 'response_total_time_memory',\n",
    "     'RP_part_process_time', 'score_practice', 'score_reading_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4',\n",
    "     'score_subblock_5', 'score_subblock_6', 'Tipo', 'total_correct',\n",
    "     'total_response_time', 'total_responses', 'width']]\n",
    "df_Reading_Span_3[['acc', 'avg_rt']] = df_Reading_Span_3[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Reading_Span_3 = df_Reading_Span_3.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_b = df_Reading_Span_3[\"response_processing\"].iloc[2]\n",
    "df_Reading_Span_3 = df_Reading_Span_3.replace(example_b, '')\n",
    "#df_Reading_Span_3 = df_Reading_Span_3.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Reading_Span_3 = df_Reading_Span_3.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Reading_Span_3.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Reading_Span_3.columns = ['acc_Sess17h00','avg_rt_Sess17h00','BlockChoice_Sess17h00','correct_Sess17h00','correct_response_Sess17h00',\n",
    "                             'Frase_Sess17h00','height_Sess17h00','letter_Sess17h00','List_Prev_Letter_Sess17h00','List_responses_memory_Sess17h00',\n",
    "                             'live_row_Sess17h00','logfile_Sess17h00','response_average_time_memory_Sess17h00','response_memory_Sess17h00',\n",
    "                             'response_processing_Sess17h00','response_time_memory_Sess17h00','response_time_processing_Sess17h00',\n",
    "                             'response_total_time_memory_Sess17h00','RP_part_process_time_Sess17h00','score_practice_Sess17h00',\n",
    "                             'score_reading_span_Sess17h00','score_subblock_2_Sess17h00','score_subblock_3_Sess17h00','score_subblock_4_Sess17h00',\n",
    "                             'score_subblock_5_Sess17h00','score_subblock_6_Sess17h00','Tipo_Sess17h00','total_correct_Sess17h00',\n",
    "                             'total_response_time_Sess17h00','total_responses_Sess17h00','width_Sess17h00']\n",
    "#'selSNr',\n",
    "df_Reading_Span_4 = df_Reading_Span_4[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'Frase', 'height', 'letter', 'List_Prev_Letter',\n",
    "     'List_responses_memory', 'live_row', 'logfile', 'response_average_time_memory', 'response_memory',\n",
    "     'response_processing', 'response_time_memory', 'response_time_processing', 'response_total_time_memory',\n",
    "     'RP_part_process_time', 'score_practice', 'score_reading_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4',\n",
    "     'score_subblock_5', 'score_subblock_6', 'Tipo', 'total_correct',\n",
    "     'total_response_time', 'total_responses', 'width']]\n",
    "df_Reading_Span_4[['acc', 'avg_rt']] = df_Reading_Span_4[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Reading_Span_4 = df_Reading_Span_4.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_b = df_Reading_Span_4[\"response_processing\"].iloc[2]\n",
    "df_Reading_Span_4 = df_Reading_Span_4.replace(example_b, '')\n",
    "#df_Reading_Span_4 = df_Reading_Span_4.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Reading_Span_4 = df_Reading_Span_4.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Reading_Span_4.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Reading_Span_4.columns = ['acc_Sess21h00','avg_rt_Sess21h00','BlockChoice_Sess21h00','correct_Sess21h00','correct_response_Sess21h00',\n",
    "                             'Frase_Sess21h00','height_Sess21h00','letter_Sess21h00','List_Prev_Letter_Sess21h00','List_responses_memory_Sess21h00',\n",
    "                             'live_row_Sess21h00','logfile_Sess21h00','response_average_time_memory_Sess21h00','response_memory_Sess21h00',\n",
    "                             'response_processing_Sess21h00','response_time_memory_Sess21h00','response_time_processing_Sess21h00',\n",
    "                             'response_total_time_memory_Sess21h00','RP_part_process_time_Sess21h00','score_practice_Sess21h00',\n",
    "                             'score_reading_span_Sess21h00','score_subblock_2_Sess21h00','score_subblock_3_Sess21h00','score_subblock_4_Sess21h00',\n",
    "                             'score_subblock_5_Sess21h00','score_subblock_6_Sess21h00','Tipo_Sess21h00','total_correct_Sess21h00',\n",
    "                             'total_response_time_Sess21h00','total_responses_Sess21h00','width_Sess21h00']\n",
    "\n",
    "#'selSNr',\n",
    "\n",
    "#print(df_Reading_Span_2.to_string())\n",
    "df_Reading_Span_Experimental = pd.concat([df_Reading_Span_1,df_Reading_Span_2,df_Reading_Span_3,df_Reading_Span_4],axis=1)\n",
    "\n",
    "df_Reading_Span_Experimental = df_Reading_Span_Experimental[['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName','acc_Sess09h00',\n",
    "                                                   'acc_Sess13h00','acc_Sess17h00','acc_Sess21h00','avg_rt_Sess09h00','avg_rt_Sess13h00','avg_rt_Sess17h00','avg_rt_Sess21h00',\n",
    "                                                   'BlockChoice_Sess09h00','BlockChoice_Sess13h00','BlockChoice_Sess17h00','BlockChoice_Sess21h00','correct_Sess09h00',\n",
    "                                                   'correct_Sess13h00','correct_Sess17h00','correct_Sess21h00','correct_response_Sess09h00','correct_response_Sess13h00',\n",
    "                                                   'correct_response_Sess17h00','correct_response_Sess21h00','Frase_Sess09h00','Frase_Sess13h00','Frase_Sess17h00','Frase_Sess21h00',\n",
    "                                                   'height_Sess09h00','height_Sess13h00','height_Sess17h00','height_Sess21h00','letter_Sess09h00','letter_Sess13h00','letter_Sess17h00',\n",
    "                                                   'letter_Sess21h00','List_Prev_Letter_Sess09h00','List_Prev_Letter_Sess13h00','List_Prev_Letter_Sess17h00',\n",
    "                                                   'List_Prev_Letter_Sess21h00','List_responses_memory_Sess09h00','List_responses_memory_Sess13h00',\n",
    "                                                   'List_responses_memory_Sess17h00','List_responses_memory_Sess21h00','live_row_Sess09h00','live_row_Sess13h00',\n",
    "                                                   'live_row_Sess17h00','live_row_Sess21h00','logfile_Sess09h00','logfile_Sess13h00','logfile_Sess17h00','logfile_Sess21h00',\n",
    "                                                   'response_average_time_memory_Sess09h00','response_average_time_memory_Sess13h00','response_average_time_memory_Sess17h00',\n",
    "                                                   'response_average_time_memory_Sess21h00','response_memory_Sess09h00','response_memory_Sess13h00','response_memory_Sess17h00',\n",
    "                                                   'response_memory_Sess21h00','response_processing_Sess09h00','response_processing_Sess13h00','response_processing_Sess17h00',\n",
    "                                                   'response_processing_Sess21h00','response_time_memory_Sess09h00','response_time_memory_Sess13h00',\n",
    "                                                   'response_time_memory_Sess17h00','response_time_memory_Sess21h00','response_time_processing_Sess09h00',\n",
    "                                                   'response_time_processing_Sess13h00','response_time_processing_Sess17h00','response_time_processing_Sess21h00',\n",
    "                                                   'response_total_time_memory_Sess09h00','response_total_time_memory_Sess13h00','response_total_time_memory_Sess17h00',\n",
    "                                                   'response_total_time_memory_Sess21h00','RP_part_process_time_Sess09h00','RP_part_process_time_Sess13h00',\n",
    "                                                   'RP_part_process_time_Sess17h00','RP_part_process_time_Sess21h00','score_practice_Sess09h00',\n",
    "                                                   'score_practice_Sess13h00','score_practice_Sess17h00','score_practice_Sess21h00','score_reading_span_Sess09h00',\n",
    "                                                   'score_reading_span_Sess13h00','score_reading_span_Sess17h00','score_reading_span_Sess21h00','score_subblock_2_Sess09h00',\n",
    "                                                   'score_subblock_2_Sess13h00','score_subblock_2_Sess17h00','score_subblock_2_Sess21h00','score_subblock_3_Sess09h00',\n",
    "                                                   'score_subblock_3_Sess13h00','score_subblock_3_Sess17h00','score_subblock_3_Sess21h00','score_subblock_4_Sess09h00',\n",
    "                                                   'score_subblock_4_Sess13h00','score_subblock_4_Sess17h00','score_subblock_4_Sess21h00','score_subblock_5_Sess09h00',\n",
    "                                                   'score_subblock_5_Sess13h00','score_subblock_5_Sess17h00','score_subblock_5_Sess21h00','score_subblock_6_Sess09h00',\n",
    "                                                   'score_subblock_6_Sess13h00','score_subblock_6_Sess17h00','score_subblock_6_Sess21h00','Tipo_Sess09h00','Tipo_Sess13h00',\n",
    "                                                   'Tipo_Sess17h00','Tipo_Sess21h00','total_correct_Sess09h00','total_correct_Sess13h00','total_correct_Sess17h00',\n",
    "                                                   'total_correct_Sess21h00','total_response_time_Sess09h00','total_response_time_Sess13h00','total_response_time_Sess17h00',\n",
    "                                                   'total_response_time_Sess21h00','total_responses_Sess09h00','total_responses_Sess13h00','total_responses_Sess17h00',\n",
    "                                                   'total_responses_Sess21h00','width_Sess09h00','width_Sess13h00','width_Sess17h00','width_Sess21h00']]\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#'toUpdate1_1', 'toUpdate1_2', 'toUpdate1_3', 'toUpdate2_1', 'toUpdate2_2','toUpdate2_3','correct_response1', 'correct_response2', 'correct_response3',\n",
    "#'selSNr',\n",
    "df_WMU_Task_1 = df_WMU_Task_1[\n",
    "    ['subject_nr',  'CB_ref', 'practice', 'TrialNumber', 'correct1', 'correct2', 'correct3', 'digit1', 'digit2', 'digit3', 'height',\n",
    "     'Index_List', 'live_row', 'logfile', 'response1', 'response2', 'response3', 'response_time1', 'responseavgRT',\n",
    "     'total_correct_trial', 'TotalRtBlock',  'WMUExperimentalScore', 'WMUPracticeScore', 'width']]\n",
    "df_WMU_Task_1 = df_WMU_Task_1.rename(columns={'response_time1': 'response_time'})\n",
    "#df_WMU_Task_1 = df_WMU_Task_1.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_WMU_Task_1 = df_WMU_Task_1.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "for i in range(0, len(df_WMU_Task_1['responseavgRT'])):\n",
    "    if df_WMU_Task_1['responseavgRT'].iloc[i] == 0:\n",
    "        df_WMU_Task_1['responseavgRT'].iloc[i] = ''\n",
    "df_WMU_Task_1.columns = ['subject_nr','CB_ref','practice','TrialNumber','correct1_Sess09h00',\n",
    "                         'correct2_Sess09h00','correct3_Sess09h00','digit1_Sess09h00','digit2_Sess09h00','digit3_Sess09h00','height_Sess09h00',\n",
    "                         'Index_List_Sess09h00','live_row_Sess09h00','logfile_Sess09h00','response1_Sess09h00','response2_Sess09h00','response3_Sess09h00',\n",
    "                         'response_time1_Sess09h00','responseavgRT_Sess09h00','total_correct_trial_Sess09h00','TotalRtBlock_Sess09h00',\n",
    "                         'WMUExperimentalScore_Sess09h00','WMUPracticeScore_Sess09h00','width_Sess09h00']\n",
    "\n",
    "#WMU_cast_lis = ['toUpdate1_1', 'toUpdate1_2', 'toUpdate1_3', 'toUpdate2_1', 'toUpdate2_2', 'toUpdate2_3']\n",
    "#for i in WMU_cast_lis:\n",
    "#    for j in range(0, len(df_WMU_Task[i])):\n",
    "#        if df_WMU_Task[i].iloc[j] > 0:\n",
    "#            df_WMU_Task[i].iloc[j] = '+' + str(int(df_WMU_Task[i].iloc[j]))\n",
    "#        else:\n",
    "#            df_WMU_Task[i].iloc[j] = str(int(df_WMU_Task[i].iloc[j]))\n",
    "\n",
    "#'selSNr',\n",
    "df_WMU_Task_2 = df_WMU_Task_2[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'correct1', 'correct2', 'correct3', 'digit1', 'digit2', 'digit3', 'height',\n",
    "     'Index_List', 'live_row', 'logfile', 'response1', 'response2', 'response3', 'response_time1', 'responseavgRT',\n",
    "     'total_correct_trial', 'TotalRtBlock',  'WMUExperimentalScore', 'WMUPracticeScore', 'width']]\n",
    "df_WMU_Task_2 = df_WMU_Task_2.rename(columns={'response_time1': 'response_time'})\n",
    "#df_WMU_Task_2 = df_WMU_Task_2.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_WMU_Task_2 = df_WMU_Task_2.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "for i in range(0, len(df_WMU_Task_2['responseavgRT'])):\n",
    "    if df_WMU_Task_2['responseavgRT'].iloc[i] == 0:\n",
    "        df_WMU_Task_2['responseavgRT'].iloc[i] = ''\n",
    "df_WMU_Task_2.drop(['subject_nr','CB_ref','practice','TrialNumber'],axis=1,inplace=True)\n",
    "df_WMU_Task_2.columns = ['correct1_Sess13h00','correct2_Sess13h00','correct3_Sess13h00','digit1_Sess13h00','digit2_Sess13h00','digit3_Sess13h00','height_Sess13h00',\n",
    "                         'Index_List_Sess13h00','live_row_Sess13h00','logfile_Sess13h00','response1_Sess13h00','response2_Sess13h00','response3_Sess13h00',\n",
    "                         'response_time1_Sess13h00','responseavgRT_Sess13h00','total_correct_trial_Sess13h00','TotalRtBlock_Sess13h00',\n",
    "                         'WMUExperimentalScore_Sess13h00','WMUPracticeScore_Sess13h00','width_Sess13h00']\n",
    "#WMU_cast_lis = ['toUpdate1_1', 'toUpdate1_2', 'toUpdate1_3', 'toUpdate2_1', 'toUpdate2_2', 'toUpdate2_3']\n",
    "#for i in WMU_cast_lis:\n",
    "#    for j in range(0, len(df_WMU_Task[i])):\n",
    "#        if df_WMU_Task[i].iloc[j] > 0:\n",
    "#            df_WMU_Task[i].iloc[j] = '+' + str(int(df_WMU_Task[i].iloc[j]))\n",
    "#        else:\n",
    "#            df_WMU_Task[i].iloc[j] = str(int(df_WMU_Task[i].iloc[j]))\n",
    "\n",
    "#'selSNr',\n",
    "df_WMU_Task_3 = df_WMU_Task_3[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'correct1', 'correct2', 'correct3', 'digit1', 'digit2', 'digit3', 'height',\n",
    "     'Index_List', 'live_row', 'logfile', 'response1', 'response2', 'response3', 'response_time1', 'responseavgRT',\n",
    "     'total_correct_trial', 'TotalRtBlock',  'WMUExperimentalScore', 'WMUPracticeScore', 'width']]\n",
    "df_WMU_Task_3 = df_WMU_Task_3.rename(columns={'response_time1': 'response_time'})\n",
    "#df_WMU_Task_3 = df_WMU_Task_3.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_WMU_Task_3 = df_WMU_Task_3.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "for i in range(0, len(df_WMU_Task_3['responseavgRT'])):\n",
    "    if df_WMU_Task_3['responseavgRT'].iloc[i] == 0:\n",
    "        df_WMU_Task_3['responseavgRT'].iloc[i] = ''\n",
    "df_WMU_Task_3.drop(['subject_nr','CB_ref','practice','TrialNumber'],axis=1,inplace=True)\n",
    "df_WMU_Task_3.columns = ['correct1_Sess17h00','correct2_Sess17h00','correct3_Sess17h00','digit1_Sess17h00','digit2_Sess17h00','digit3_Sess17h00','height_Sess17h00',\n",
    "                         'Index_List_Sess17h00','live_row_Sess17h00','logfile_Sess17h00','response1_Sess17h00','response2_Sess17h00','response3_Sess17h00',\n",
    "                         'response_time1_Sess17h00','responseavgRT_Sess17h00','total_correct_trial_Sess17h00','TotalRtBlock_Sess17h00',\n",
    "                         'WMUExperimentalScore_Sess17h00','WMUPracticeScore_Sess17h00','width_Sess17h00']\n",
    "#WMU_cast_lis = ['toUpdate1_1', 'toUpdate1_2', 'toUpdate1_3', 'toUpdate2_1', 'toUpdate2_2', 'toUpdate2_3']\n",
    "#for i in WMU_cast_lis:\n",
    "#    for j in range(0, len(df_WMU_Task[i])):\n",
    "#        if df_WMU_Task[i].iloc[j] > 0:\n",
    "#            df_WMU_Task[i].iloc[j] = '+' + str(int(df_WMU_Task[i].iloc[j]))\n",
    "#        else:\n",
    "#            df_WMU_Task[i].iloc[j] = str(int(df_WMU_Task[i].iloc[j]))\n",
    "\n",
    "#'selSNr',\n",
    "df_WMU_Task_4 = df_WMU_Task_4[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'correct1', 'correct2', 'correct3', 'digit1', 'digit2', 'digit3', 'height',\n",
    "     'Index_List', 'live_row', 'logfile', 'response1', 'response2', 'response3', 'response_time1', 'responseavgRT',\n",
    "     'total_correct_trial', 'TotalRtBlock',  'WMUExperimentalScore', 'WMUPracticeScore', 'width']]\n",
    "df_WMU_Task_4 = df_WMU_Task_4.rename(columns={'response_time1': 'response_time'})\n",
    "#df_WMU_Task_4 = df_WMU_Task_4.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_WMU_Task_4 = df_WMU_Task_4.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "for i in range(0, len(df_WMU_Task_4['responseavgRT'])):\n",
    "    if df_WMU_Task_4['responseavgRT'].iloc[i] == 0:\n",
    "        df_WMU_Task_4['responseavgRT'].iloc[i] = ''\n",
    "df_WMU_Task_4.drop(['subject_nr','CB_ref','practice','TrialNumber'],axis=1,inplace=True)\n",
    "df_WMU_Task_4.columns = ['correct1_Sess21h00','correct2_Sess21h00','correct3_Sess21h00','digit1_Sess21h00','digit2_Sess21h00','digit3_Sess21h00','height_Sess21h00',\n",
    "                         'Index_List_Sess21h00','live_row_Sess21h00','logfile_Sess21h00','response1_Sess21h00','response2_Sess21h00','response3_Sess21h00',\n",
    "                         'response_time1_Sess21h00','responseavgRT_Sess21h00','total_correct_trial_Sess21h00','TotalRtBlock_Sess21h00',\n",
    "                         'WMUExperimentalScore_Sess21h00','WMUPracticeScore_Sess21h00','width_Sess21h00']\n",
    "#WMU_cast_lis = ['toUpdate1_1', 'toUpdate1_2', 'toUpdate1_3', 'toUpdate2_1', 'toUpdate2_2', 'toUpdate2_3']\n",
    "#for i in WMU_cast_lis:\n",
    "#    for j in range(0, len(df_WMU_Task[i])):\n",
    "#        if df_WMU_Task[i].iloc[j] > 0:\n",
    "#            df_WMU_Task[i].iloc[j] = '+' + str(int(df_WMU_Task[i].iloc[j]))\n",
    "#        else:\n",
    "#            df_WMU_Task[i].iloc[j] = str(int(df_WMU_Task[i].iloc[j]))\n",
    "\n",
    "df_WMU_Task_Experimental = pd.concat([df_WMU_Task_1,df_WMU_Task_2,df_WMU_Task_3,df_WMU_Task_4],axis=1)\n",
    "\n",
    "df_WMU_Task_Experimental = df_WMU_Task_Experimental[['subject_nr','CB_ref','practice','TrialNumber','correct1_Sess09h00','correct1_Sess13h00',\n",
    "                                       'correct1_Sess17h00','correct1_Sess21h00','correct2_Sess09h00','correct2_Sess13h00','correct2_Sess17h00',\n",
    "                                       'correct2_Sess21h00','correct3_Sess09h00','correct3_Sess13h00','correct3_Sess17h00','correct3_Sess21h00',\n",
    "                                       'digit1_Sess09h00','digit1_Sess13h00','digit1_Sess17h00','digit1_Sess21h00','digit2_Sess09h00','digit2_Sess13h00',\n",
    "                                       'digit2_Sess17h00','digit2_Sess21h00','digit3_Sess09h00','digit3_Sess13h00','digit3_Sess17h00','digit3_Sess21h00',\n",
    "                                       'height_Sess09h00','height_Sess13h00','height_Sess17h00','height_Sess21h00','Index_List_Sess09h00',\n",
    "                                       'Index_List_Sess13h00','Index_List_Sess17h00','Index_List_Sess21h00','live_row_Sess09h00','live_row_Sess13h00',\n",
    "                                       'live_row_Sess17h00','live_row_Sess21h00','logfile_Sess09h00','logfile_Sess13h00','logfile_Sess17h00','logfile_Sess21h00',\n",
    "                                       'response1_Sess09h00','response1_Sess13h00','response1_Sess17h00','response1_Sess21h00','response2_Sess09h00','response2_Sess13h00',\n",
    "                                       'response2_Sess17h00','response2_Sess21h00','response3_Sess09h00','response3_Sess13h00','response3_Sess17h00','response3_Sess21h00',\n",
    "                                       'response_time1_Sess09h00','response_time1_Sess13h00','response_time1_Sess17h00','response_time1_Sess21h00',\n",
    "                                       'responseavgRT_Sess09h00','responseavgRT_Sess13h00','responseavgRT_Sess17h00','responseavgRT_Sess21h00',\n",
    "                                       'total_correct_trial_Sess09h00','total_correct_trial_Sess13h00','total_correct_trial_Sess17h00','total_correct_trial_Sess21h00',\n",
    "                                       'TotalRtBlock_Sess09h00','TotalRtBlock_Sess13h00','TotalRtBlock_Sess17h00','TotalRtBlock_Sess21h00',\n",
    "                                       'WMUExperimentalScore_Sess09h00', 'WMUExperimentalScore_Sess13h00','WMUExperimentalScore_Sess17h00', 'WMUExperimentalScore_Sess21h00',\n",
    "                                       'WMUPracticeScore_Sess09h00','WMUPracticeScore_Sess13h00','WMUPracticeScore_Sess17h00','WMUPracticeScore_Sess21h00',\n",
    "                                       'width_Sess09h00','width_Sess13h00','width_Sess17h00','width_Sess21h00']]\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#'selSNr',\n",
    "df_Symmetry_Span_1 = df_Symmetry_Span_1[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName',\n",
    "     'aggregated_score_memory', 'average_response_time_processing', 'average_total_time_memory', 'correct',\n",
    "     'correct_response', 'countDys', 'countSym', 'height', 'LeftHalfPos', 'List_SS_button', 'List_SS_Pos', 'live_row',\n",
    "     'logfile', 'maxDys', 'maxSym', 'pressed_buttons', 'response_memory', 'response_processing', 'response_time_memory',\n",
    "     'response_time_processing', 'response_total_time_memory', 'response_total_time_memory_full_task', 'RightHalfPos',\n",
    "     'SP_part_process_time', 'SS_practice_score', 'score_symmetry_span', 'score_subblock_2', 'score_subblock_3',\n",
    "     'score_subblock_4', 'score_subblock_5', 'score_subblock_6', 'SymType',\n",
    "     'total_correct_processing', 'total_response_time_processing', 'width']]\n",
    "df_Symmetry_Span_1 = df_Symmetry_Span_1.astype({'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_d = df_Symmetry_Span_1[\"response_processing\"].iloc[2]\n",
    "df_Symmetry_Span_1 = df_Symmetry_Span_1.replace(example_d, '')\n",
    "#df_Symmetry_Span_1 = df_Symmetry_Span_1.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Symmetry_Span_1 = df_Symmetry_Span_1.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Symmetry_Span_1.columns = ['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco',\n",
    "                              'SubTaskName','aggregated_score_memory_Sess09h00','average_response_time_processing_Sess09h00',\n",
    "                              'average_total_time_memory_Sess09h00','correct_Sess09h00','correct_response_Sess09h00','countDys_Sess09h00',\n",
    "                              'countSym_Sess09h00','height_Sess09h00','LeftHalfPos_Sess09h00','List_SS_button_Sess09h00','List_SS_Pos_Sess09h00',\n",
    "                              'live_row_Sess09h00','logfile_Sess09h00','maxDys_Sess09h00','maxSym_Sess09h00','pressed_buttons_Sess09h00',\n",
    "                              'response_memory_Sess09h00','response_processing_Sess09h00','response_time_memory_Sess09h00',\n",
    "                              'response_time_processing_Sess09h00','response_total_time_memory_Sess09h00',\n",
    "                              'response_total_time_memory_full_task_Sess09h00','RightHalfPos_Sess09h00','SP_part_process_time_Sess09h00',\n",
    "                              'SS_practice_score_Sess09h00','score_symmetry_span_Sess09h00','score_subblock_2_Sess09h00','score_subblock_3_Sess09h00',\n",
    "                              'score_subblock_4_Sess09h00','score_subblock_5_Sess09h00','score_subblock_6_Sess09h00','SymType_Sess09h00',\n",
    "                              'total_correct_processing_Sess09h00','total_response_time_processing_Sess09h00','width_Sess09h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Symmetry_Span_2 = df_Symmetry_Span_2[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName',\n",
    "     'aggregated_score_memory', 'average_response_time_processing', 'average_total_time_memory', 'correct',\n",
    "     'correct_response', 'countDys', 'countSym', 'height', 'LeftHalfPos', 'List_SS_button', 'List_SS_Pos', 'live_row',\n",
    "     'logfile', 'maxDys', 'maxSym', 'pressed_buttons', 'response_memory', 'response_processing', 'response_time_memory',\n",
    "     'response_time_processing', 'response_total_time_memory', 'response_total_time_memory_full_task', 'RightHalfPos',\n",
    "     'SP_part_process_time', 'SS_practice_score', 'score_symmetry_span', 'score_subblock_2', 'score_subblock_3',\n",
    "     'score_subblock_4', 'score_subblock_5', 'score_subblock_6', 'SymType',\n",
    "     'total_correct_processing', 'total_response_time_processing', 'width']]\n",
    "df_Symmetry_Span_2 = df_Symmetry_Span_2.astype({'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_d = df_Symmetry_Span_2[\"response_processing\"].iloc[2]\n",
    "df_Symmetry_Span_2 = df_Symmetry_Span_2.replace(example_d, '')\n",
    "#df_Symmetry_Span_2 = df_Symmetry_Span_2.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Symmetry_Span_2 = df_Symmetry_Span_2.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Symmetry_Span_2.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Symmetry_Span_2.columns = ['aggregated_score_memory_Sess13h00','average_response_time_processing_Sess13h00',\n",
    "                              'average_total_time_memory_Sess13h00','correct_Sess13h00','correct_response_Sess13h00','countDys_Sess13h00',\n",
    "                              'countSym_Sess13h00','height_Sess13h00','LeftHalfPos_Sess13h00','List_SS_button_Sess13h00','List_SS_Pos_Sess13h00',\n",
    "                              'live_row_Sess13h00','logfile_Sess13h00','maxDys_Sess13h00','maxSym_Sess13h00','pressed_buttons_Sess13h00',\n",
    "                              'response_memory_Sess13h00','response_processing_Sess13h00','response_time_memory_Sess13h00',\n",
    "                              'response_time_processing_Sess13h00','response_total_time_memory_Sess13h00',\n",
    "                              'response_total_time_memory_full_task_Sess13h00','RightHalfPos_Sess13h00','SP_part_process_time_Sess13h00',\n",
    "                              'SS_practice_score_Sess13h00','score_symmetry_span_Sess13h00','score_subblock_2_Sess13h00','score_subblock_3_Sess13h00',\n",
    "                              'score_subblock_4_Sess13h00','score_subblock_5_Sess13h00','score_subblock_6_Sess13h00','SymType_Sess13h00',\n",
    "                              'total_correct_processing_Sess13h00','total_response_time_processing_Sess13h00','width_Sess13h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Symmetry_Span_3 = df_Symmetry_Span_3[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName',\n",
    "     'aggregated_score_memory', 'average_response_time_processing', 'average_total_time_memory', 'correct',\n",
    "     'correct_response', 'countDys', 'countSym', 'height', 'LeftHalfPos', 'List_SS_button', 'List_SS_Pos', 'live_row',\n",
    "     'logfile', 'maxDys', 'maxSym', 'pressed_buttons', 'response_memory', 'response_processing', 'response_time_memory',\n",
    "     'response_time_processing', 'response_total_time_memory', 'response_total_time_memory_full_task', 'RightHalfPos',\n",
    "     'SP_part_process_time', 'SS_practice_score', 'score_symmetry_span', 'score_subblock_2', 'score_subblock_3',\n",
    "     'score_subblock_4', 'score_subblock_5', 'score_subblock_6', 'SymType',\n",
    "     'total_correct_processing', 'total_response_time_processing', 'width']]\n",
    "df_Symmetry_Span_3 = df_Symmetry_Span_3.astype({'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_d = df_Symmetry_Span_3[\"response_processing\"].iloc[2]\n",
    "df_Symmetry_Span_3 = df_Symmetry_Span_3.replace(example_d, '')\n",
    "#df_Symmetry_Span_3 = df_Symmetry_Span_3.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Symmetry_Span_3 = df_Symmetry_Span_3.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Symmetry_Span_3.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Symmetry_Span_3.columns = ['aggregated_score_memory_Sess17h00','average_response_time_processing_Sess17h00',\n",
    "                              'average_total_time_memory_Sess17h00','correct_Sess17h00','correct_response_Sess17h00','countDys_Sess17h00',\n",
    "                              'countSym_Sess17h00','height_Sess17h00','LeftHalfPos_Sess17h00','List_SS_button_Sess17h00','List_SS_Pos_Sess17h00',\n",
    "                              'live_row_Sess17h00','logfile_Sess17h00','maxDys_Sess17h00','maxSym_Sess17h00','pressed_buttons_Sess17h00',\n",
    "                              'response_memory_Sess17h00','response_processing_Sess17h00','response_time_memory_Sess17h00',\n",
    "                              'response_time_processing_Sess17h00','response_total_time_memory_Sess17h00',\n",
    "                              'response_total_time_memory_full_task_Sess17h00','RightHalfPos_Sess17h00','SP_part_process_time_Sess17h00',\n",
    "                              'SS_practice_score_Sess17h00','score_symmetry_span_Sess17h00','score_subblock_2_Sess17h00','score_subblock_3_Sess17h00',\n",
    "                              'score_subblock_4_Sess17h00','score_subblock_5_Sess17h00','score_subblock_6_Sess17h00','SymType_Sess17h00',\n",
    "                              'total_correct_processing_Sess17h00','total_response_time_processing_Sess17h00','width_Sess17h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Symmetry_Span_4 = df_Symmetry_Span_4[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName',\n",
    "     'aggregated_score_memory', 'average_response_time_processing', 'average_total_time_memory', 'correct',\n",
    "     'correct_response', 'countDys', 'countSym', 'height', 'LeftHalfPos', 'List_SS_button', 'List_SS_Pos', 'live_row',\n",
    "     'logfile', 'maxDys', 'maxSym', 'pressed_buttons', 'response_memory', 'response_processing', 'response_time_memory',\n",
    "     'response_time_processing', 'response_total_time_memory', 'response_total_time_memory_full_task', 'RightHalfPos',\n",
    "     'SP_part_process_time', 'SS_practice_score', 'score_symmetry_span', 'score_subblock_2', 'score_subblock_3',\n",
    "     'score_subblock_4', 'score_subblock_5', 'score_subblock_6', 'SymType',\n",
    "     'total_correct_processing', 'total_response_time_processing', 'width']]\n",
    "df_Symmetry_Span_4 = df_Symmetry_Span_4.astype({'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_d = df_Symmetry_Span_4[\"response_processing\"].iloc[2]\n",
    "df_Symmetry_Span_4 = df_Symmetry_Span_4.replace(example_d, '')\n",
    "#df_Symmetry_Span_4 = df_Symmetry_Span_4.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Symmetry_Span_4 = df_Symmetry_Span_4.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Symmetry_Span_4.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Symmetry_Span_4.columns = ['aggregated_score_memory_Sess21h00','average_response_time_processing_Sess21h00',\n",
    "                              'average_total_time_memory_Sess21h00','correct_Sess21h00','correct_response_Sess21h00','countDys_Sess21h00',\n",
    "                              'countSym_Sess21h00','height_Sess21h00','LeftHalfPos_Sess21h00','List_SS_button_Sess21h00','List_SS_Pos_Sess21h00',\n",
    "                              'live_row_Sess21h00','logfile_Sess21h00','maxDys_Sess21h00','maxSym_Sess21h00','pressed_buttons_Sess21h00',\n",
    "                              'response_memory_Sess21h00','response_processing_Sess21h00','response_time_memory_Sess21h00',\n",
    "                              'response_time_processing_Sess21h00','response_total_time_memory_Sess21h00',\n",
    "                              'response_total_time_memory_full_task_Sess21h00','RightHalfPos_Sess21h00','SP_part_process_time_Sess21h00',\n",
    "                              'SS_practice_score_Sess21h00','score_symmetry_span_Sess21h00','score_subblock_2_Sess21h00','score_subblock_3_Sess21h00',\n",
    "                              'score_subblock_4_Sess21h00','score_subblock_5_Sess21h00','score_subblock_6_Sess21h00','SymType_Sess21h00',\n",
    "                              'total_correct_processing_Sess21h00','total_response_time_processing_Sess21h00','width_Sess21h00']\n",
    "\n",
    "df_Symmetry_Span_Experimental = pd.concat([df_Symmetry_Span_1,df_Symmetry_Span_2,df_Symmetry_Span_3,df_Symmetry_Span_4],axis=1)\n",
    "\n",
    "df_Symmetry_Span_Experimental = df_Symmetry_Span_Experimental[['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName',\n",
    "                                                 'aggregated_score_memory_Sess09h00','aggregated_score_memory_Sess13h00',\n",
    "                                                 'aggregated_score_memory_Sess17h00','aggregated_score_memory_Sess21h00',\n",
    "                                                 'average_response_time_processing_Sess09h00','average_response_time_processing_Sess13h00',\n",
    "                                                 'average_response_time_processing_Sess17h00','average_response_time_processing_Sess21h00',\n",
    "                                                 'average_total_time_memory_Sess09h00','average_total_time_memory_Sess13h00',\n",
    "                                                 'average_total_time_memory_Sess17h00','average_total_time_memory_Sess21h00',\n",
    "                                                 'correct_Sess09h00','correct_Sess13h00','correct_Sess17h00','correct_Sess21h00',\n",
    "                                                 'correct_response_Sess09h00','correct_response_Sess13h00','correct_response_Sess17h00',\n",
    "                                                 'correct_response_Sess21h00','countDys_Sess09h00','countDys_Sess13h00','countDys_Sess17h00','countDys_Sess21h00',\n",
    "                                                 'countSym_Sess09h00','countSym_Sess13h00','countSym_Sess17h00','countSym_Sess21h00','height_Sess09h00','height_Sess13h00','height_Sess17h00','height_Sess21h00',\n",
    "                                                 'LeftHalfPos_Sess09h00','LeftHalfPos_Sess13h00','LeftHalfPos_Sess17h00','LeftHalfPos_Sess21h00',\n",
    "                                                 'List_SS_button_Sess09h00','List_SS_button_Sess13h00','List_SS_button_Sess17h00','List_SS_button_Sess21h00',\n",
    "                                                 'List_SS_Pos_Sess09h00','List_SS_Pos_Sess13h00','List_SS_Pos_Sess17h00','List_SS_Pos_Sess21h00','live_row_Sess09h00',\n",
    "                                                 'live_row_Sess13h00','live_row_Sess17h00','live_row_Sess21h00','logfile_Sess09h00','logfile_Sess13h00','logfile_Sess17h00',\n",
    "                                                 'logfile_Sess21h00','maxDys_Sess09h00','maxDys_Sess13h00','maxDys_Sess17h00','maxDys_Sess21h00','maxSym_Sess09h00',\n",
    "                                                 'maxSym_Sess13h00','maxSym_Sess17h00','maxSym_Sess21h00','pressed_buttons_Sess09h00','pressed_buttons_Sess13h00',\n",
    "                                                 'pressed_buttons_Sess17h00','pressed_buttons_Sess21h00','response_memory_Sess09h00',\n",
    "                                                 'response_memory_Sess13h00','response_memory_Sess17h00','response_memory_Sess21h00',\n",
    "                                                 'response_processing_Sess09h00','response_processing_Sess13h00','response_processing_Sess17h00',\n",
    "                                                 'response_processing_Sess21h00','response_time_memory_Sess09h00','response_time_memory_Sess13h00','response_time_memory_Sess17h00',\n",
    "                                                 'response_time_memory_Sess21h00','response_time_processing_Sess09h00','response_time_processing_Sess13h00',\n",
    "                                                 'response_time_processing_Sess17h00','response_time_processing_Sess21h00','response_total_time_memory_Sess09h00',\n",
    "                                                 'response_total_time_memory_Sess13h00','response_total_time_memory_Sess17h00','response_total_time_memory_Sess21h00',\n",
    "                                                 'response_total_time_memory_full_task_Sess09h00','response_total_time_memory_full_task_Sess13h00',\n",
    "                                                 'response_total_time_memory_full_task_Sess17h00','response_total_time_memory_full_task_Sess21h00',\n",
    "                                                 'RightHalfPos_Sess09h00','RightHalfPos_Sess13h00','RightHalfPos_Sess17h00','RightHalfPos_Sess21h00',\n",
    "                                                 'SP_part_process_time_Sess09h00','SP_part_process_time_Sess13h00','SP_part_process_time_Sess17h00',\n",
    "                                                 'SP_part_process_time_Sess21h00','SS_practice_score_Sess09h00','SS_practice_score_Sess13h00','SS_practice_score_Sess17h00',\n",
    "                                                 'SS_practice_score_Sess21h00','score_symmetry_span_Sess09h00','score_symmetry_span_Sess13h00',\n",
    "                                                 'score_symmetry_span_Sess17h00','score_symmetry_span_Sess21h00','score_subblock_2_Sess09h00','score_subblock_2_Sess13h00',\n",
    "                                                 'score_subblock_2_Sess17h00','score_subblock_2_Sess21h00','score_subblock_3_Sess09h00','score_subblock_3_Sess13h00',\n",
    "                                                 'score_subblock_3_Sess17h00','score_subblock_3_Sess21h00','score_subblock_4_Sess09h00','score_subblock_4_Sess13h00',\n",
    "                                                 'score_subblock_4_Sess17h00','score_subblock_4_Sess21h00','score_subblock_5_Sess09h00','score_subblock_5_Sess13h00',\n",
    "                                                 'score_subblock_5_Sess17h00','score_subblock_5_Sess21h00','score_subblock_6_Sess09h00','score_subblock_6_Sess13h00',\n",
    "                                                 'score_subblock_6_Sess17h00','score_subblock_6_Sess21h00','SymType_Sess09h00','SymType_Sess13h00','SymType_Sess17h00',\n",
    "                                                 'SymType_Sess21h00','total_correct_processing_Sess09h00','total_correct_processing_Sess13h00','total_correct_processing_Sess17h00',\n",
    "                                                 'total_correct_processing_Sess21h00','total_response_time_processing_Sess09h00','total_response_time_processing_Sess13h00',\n",
    "                                                 'total_response_time_processing_Sess17h00','total_response_time_processing_Sess21h00','width_Sess09h00','width_Sess13h00',\n",
    "                                                 'width_Sess17h00','width_Sess21h00']]\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#'Probe', 'Target',\n",
    "#'selSNr',\n",
    "df_Binding_Task_1 = df_Binding_Task_1[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'acc', 'average_response_time',\n",
    "     'BindingRawScore', 'correct', 'correct_response', 'counter', 'Delay', 'eightsec_accuracy', 'FalseAlarms', 'height',\n",
    "     'Hits', 'live_row', 'logfile', 'match_1s_accuracy', 'match_1s_avg_rt', 'match_8s_accuracy', 'match_8s_avg_rt',\n",
    "     'mismatch_1s_accuracy', 'mismatch_1s_avg_rt', 'mismatch_8s_accuracy', 'mismatch_8s_avg_rt', 'NNonResponses',\n",
    "     'Omissions', 'onesec_accuracy', 'QuinetteAccuracyScore', 'QuinetteProcessingScore', 'response',\n",
    "     'response_time', 'ResponsesGiven', 'total_correct', 'total_match_1s_rt', 'total_match_8s_rt',\n",
    "     'total_mismatch_1s_rt', 'total_mismatch_8s_rt', 'total_response_time', 'total_responses', 'width']]\n",
    "df_Binding_Task_1[['acc', 'average_response_time']] = df_Binding_Task_1[['acc', 'average_response_time']].replace(',', '.')\n",
    "df_Binding_Task_1 = df_Binding_Task_1.astype(\n",
    "    {'acc': 'float64', 'average_response_time': 'float64', 'correct_response': 'str', 'response': 'str'})\n",
    "#df_Binding_Task_1 = df_Binding_Task_1.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Binding_Task_1 = df_Binding_Task_1.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Binding_Task_1.columns = ['subject_nr','CB_ref','practice','TrialNumber','acc_Sess09h00',\n",
    "                             'average_response_time_Sess09h00','BindingRawScore_Sess09h00','correct_Sess09h00','correct_response_Sess09h00',\n",
    "                             'counter_Sess09h00','Delay_Sess09h00','eightsec_accuracy_Sess09h00','FalseAlarms_Sess09h00','height_Sess09h00','Hits_Sess09h00',\n",
    "                             'live_row_Sess09h00','logfile_Sess09h00','match_1s_accuracy_Sess09h00','match_1s_avg_rt_Sess09h00','match_8s_accuracy_Sess09h00',\n",
    "                             'match_8s_avg_rt_Sess09h00','mismatch_1s_accuracy_Sess09h00','mismatch_1s_avg_rt_Sess09h00','mismatch_8s_accuracy_Sess09h00',\n",
    "                             'mismatch_8s_avg_rt_Sess09h00','NNonResponses_Sess09h00','Omissions_Sess09h00','onesec_accuracy_Sess09h00',\n",
    "                             'QuinetteAccuracyScore_Sess09h00','QuinetteProcessingScore_Sess09h00','response_Sess09h00','response_time_Sess09h00',\n",
    "                             'ResponsesGiven_Sess09h00','total_correct_Sess09h00','total_match_1s_rt_Sess09h00','total_match_8s_rt_Sess09h00',\n",
    "                             'total_mismatch_1s_rt_Sess09h00','total_mismatch_8s_rt_Sess09h00','total_response_time_Sess09h00','total_responses_Sess09h00',\n",
    "                             'width_Sess09h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Binding_Task_2 = df_Binding_Task_2[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'acc', 'average_response_time',\n",
    "     'BindingRawScore', 'correct', 'correct_response', 'counter', 'Delay', 'eightsec_accuracy', 'FalseAlarms', 'height',\n",
    "     'Hits', 'live_row', 'logfile', 'match_1s_accuracy', 'match_1s_avg_rt', 'match_8s_accuracy', 'match_8s_avg_rt',\n",
    "     'mismatch_1s_accuracy', 'mismatch_1s_avg_rt', 'mismatch_8s_accuracy', 'mismatch_8s_avg_rt', 'NNonResponses',\n",
    "     'Omissions', 'onesec_accuracy', 'QuinetteAccuracyScore', 'QuinetteProcessingScore', 'response',\n",
    "     'response_time', 'ResponsesGiven', 'total_correct', 'total_match_1s_rt', 'total_match_8s_rt',\n",
    "     'total_mismatch_1s_rt', 'total_mismatch_8s_rt', 'total_response_time', 'total_responses', 'width']]\n",
    "df_Binding_Task_2[['acc', 'average_response_time']] = df_Binding_Task_2[['acc', 'average_response_time']].replace(',', '.')\n",
    "df_Binding_Task_2 = df_Binding_Task_2.astype(\n",
    "    {'acc': 'float64', 'average_response_time': 'float64', 'correct_response': 'str', 'response': 'str'})\n",
    "#df_Binding_Task_2 = df_Binding_Task_2.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Binding_Task_2 = df_Binding_Task_2.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Binding_Task_2.drop(['subject_nr','CB_ref','practice','TrialNumber'],axis=1,inplace=True)\n",
    "df_Binding_Task_2.columns = ['acc_Sess13h00','average_response_time_Sess13h00','BindingRawScore_Sess13h00','correct_Sess13h00','correct_response_Sess13h00',\n",
    "                             'counter_Sess13h00','Delay_Sess13h00','eightsec_accuracy_Sess13h00','FalseAlarms_Sess13h00','height_Sess13h00','Hits_Sess13h00',\n",
    "                             'live_row_Sess13h00','logfile_Sess13h00','match_1s_accuracy_Sess13h00','match_1s_avg_rt_Sess13h00','match_8s_accuracy_Sess13h00',\n",
    "                             'match_8s_avg_rt_Sess13h00','mismatch_1s_accuracy_Sess13h00','mismatch_1s_avg_rt_Sess13h00','mismatch_8s_accuracy_Sess13h00',\n",
    "                             'mismatch_8s_avg_rt_Sess13h00','NNonResponses_Sess13h00','Omissions_Sess13h00','onesec_accuracy_Sess13h00',\n",
    "                             'QuinetteAccuracyScore_Sess13h00','QuinetteProcessingScore_Sess13h00','response_Sess13h00','response_time_Sess13h00',\n",
    "                             'ResponsesGiven_Sess13h00','total_correct_Sess13h00','total_match_1s_rt_Sess13h00','total_match_8s_rt_Sess13h00',\n",
    "                             'total_mismatch_1s_rt_Sess13h00','total_mismatch_8s_rt_Sess13h00','total_response_time_Sess13h00','total_responses_Sess13h00',\n",
    "                             'width_Sess13h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Binding_Task_3 = df_Binding_Task_3[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'acc', 'average_response_time',\n",
    "     'BindingRawScore', 'correct', 'correct_response', 'counter', 'Delay', 'eightsec_accuracy', 'FalseAlarms', 'height',\n",
    "     'Hits', 'live_row', 'logfile', 'match_1s_accuracy', 'match_1s_avg_rt', 'match_8s_accuracy', 'match_8s_avg_rt',\n",
    "     'mismatch_1s_accuracy', 'mismatch_1s_avg_rt', 'mismatch_8s_accuracy', 'mismatch_8s_avg_rt', 'NNonResponses',\n",
    "     'Omissions', 'onesec_accuracy', 'QuinetteAccuracyScore', 'QuinetteProcessingScore', 'response',\n",
    "     'response_time', 'ResponsesGiven', 'total_correct', 'total_match_1s_rt', 'total_match_8s_rt',\n",
    "     'total_mismatch_1s_rt', 'total_mismatch_8s_rt', 'total_response_time', 'total_responses', 'width']]\n",
    "df_Binding_Task_3[['acc', 'average_response_time']] = df_Binding_Task_3[['acc', 'average_response_time']].replace(',', '.')\n",
    "df_Binding_Task_3 = df_Binding_Task_3.astype(\n",
    "    {'acc': 'float64', 'average_response_time': 'float64', 'correct_response': 'str', 'response': 'str'})\n",
    "#df_Binding_Task_3 = df_Binding_Task_3.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Binding_Task_3 = df_Binding_Task_3.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Binding_Task_3.drop(['subject_nr','CB_ref','practice','TrialNumber'],axis=1,inplace=True)\n",
    "df_Binding_Task_3.columns = ['acc_Sess17h00','average_response_time_Sess17h00','BindingRawScore_Sess17h00','correct_Sess17h00','correct_response_Sess17h00',\n",
    "                             'counter_Sess17h00','Delay_Sess17h00','eightsec_accuracy_Sess17h00','FalseAlarms_Sess17h00','height_Sess17h00','Hits_Sess17h00',\n",
    "                             'live_row_Sess17h00','logfile_Sess17h00','match_1s_accuracy_Sess17h00','match_1s_avg_rt_Sess17h00','match_8s_accuracy_Sess17h00',\n",
    "                             'match_8s_avg_rt_Sess17h00','mismatch_1s_accuracy_Sess17h00','mismatch_1s_avg_rt_Sess17h00','mismatch_8s_accuracy_Sess17h00',\n",
    "                             'mismatch_8s_avg_rt_Sess17h00','NNonResponses_Sess17h00','Omissions_Sess17h00','onesec_accuracy_Sess17h00',\n",
    "                             'QuinetteAccuracyScore_Sess17h00','QuinetteProcessingScore_Sess17h00','response_Sess17h00','response_time_Sess17h00',\n",
    "                             'ResponsesGiven_Sess17h00','total_correct_Sess17h00','total_match_1s_rt_Sess17h00','total_match_8s_rt_Sess17h00',\n",
    "                             'total_mismatch_1s_rt_Sess17h00','total_mismatch_8s_rt_Sess17h00','total_response_time_Sess17h00','total_responses_Sess17h00',\n",
    "                             'width_Sess17h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Binding_Task_4 = df_Binding_Task_4[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'acc', 'average_response_time',\n",
    "     'BindingRawScore', 'correct', 'correct_response', 'counter', 'Delay', 'eightsec_accuracy', 'FalseAlarms', 'height',\n",
    "     'Hits', 'live_row', 'logfile', 'match_1s_accuracy', 'match_1s_avg_rt', 'match_8s_accuracy', 'match_8s_avg_rt',\n",
    "     'mismatch_1s_accuracy', 'mismatch_1s_avg_rt', 'mismatch_8s_accuracy', 'mismatch_8s_avg_rt', 'NNonResponses',\n",
    "     'Omissions', 'onesec_accuracy', 'QuinetteAccuracyScore', 'QuinetteProcessingScore', 'response',\n",
    "     'response_time', 'ResponsesGiven', 'total_correct', 'total_match_1s_rt', 'total_match_8s_rt',\n",
    "     'total_mismatch_1s_rt', 'total_mismatch_8s_rt', 'total_response_time', 'total_responses', 'width']]\n",
    "df_Binding_Task_4[['acc', 'average_response_time']] = df_Binding_Task_4[['acc', 'average_response_time']].replace(',', '.')\n",
    "df_Binding_Task_4 = df_Binding_Task_4.astype(\n",
    "    {'acc': 'float64', 'average_response_time': 'float64', 'correct_response': 'str', 'response': 'str'})\n",
    "#df_Binding_Task_4 = df_Binding_Task_4.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Binding_Task_4 = df_Binding_Task_4.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Binding_Task_4.drop(['subject_nr','CB_ref','practice','TrialNumber'],axis=1,inplace=True)\n",
    "df_Binding_Task_4.columns = ['acc_Sess21h00','average_response_time_Sess21h00','BindingRawScore_Sess21h00','correct_Sess21h00','correct_response_Sess21h00',\n",
    "                             'counter_Sess21h00','Delay_Sess21h00','eightsec_accuracy_Sess21h00','FalseAlarms_Sess21h00','height_Sess21h00','Hits_Sess21h00',\n",
    "                             'live_row_Sess21h00','logfile_Sess21h00','match_1s_accuracy_Sess21h00','match_1s_avg_rt_Sess21h00','match_8s_accuracy_Sess21h00',\n",
    "                             'match_8s_avg_rt_Sess21h00','mismatch_1s_accuracy_Sess21h00','mismatch_1s_avg_rt_Sess21h00','mismatch_8s_accuracy_Sess21h00',\n",
    "                             'mismatch_8s_avg_rt_Sess21h00','NNonResponses_Sess21h00','Omissions_Sess21h00','onesec_accuracy_Sess21h00',\n",
    "                             'QuinetteAccuracyScore_Sess21h00','QuinetteProcessingScore_Sess21h00','response_Sess21h00','response_time_Sess21h00',\n",
    "                             'ResponsesGiven_Sess21h00','total_correct_Sess21h00','total_match_1s_rt_Sess21h00','total_match_8s_rt_Sess21h00',\n",
    "                             'total_mismatch_1s_rt_Sess21h00','total_mismatch_8s_rt_Sess21h00','total_response_time_Sess21h00','total_responses_Sess21h00',\n",
    "                             'width_Sess21h00']\n",
    "\n",
    "df_Binding_Task_Experimental = pd.concat([df_Binding_Task_1,df_Binding_Task_2,df_Binding_Task_3,df_Binding_Task_4],axis=1)\n",
    "\n",
    "df_Binding_Task_Experimental = df_Binding_Task_Experimental[['subject_nr','CB_ref','practice','TrialNumber','acc_Sess09h00','acc_Sess13h00','acc_Sess17h00','acc_Sess21h00',\n",
    "                                         'average_response_time_Sess09h00','average_response_time_Sess13h00','average_response_time_Sess17h00','average_response_time_Sess21h00',\n",
    "                                         'BindingRawScore_Sess09h00','BindingRawScore_Sess13h00','BindingRawScore_Sess17h00','BindingRawScore_Sess21h00','correct_Sess09h00',\n",
    "                                         'correct_Sess13h00','correct_Sess17h00','correct_Sess21h00','correct_response_Sess09h00','correct_response_Sess13h00',\n",
    "                                         'correct_response_Sess17h00','correct_response_Sess21h00','counter_Sess09h00','counter_Sess13h00','counter_Sess17h00','counter_Sess21h00',\n",
    "                                         'Delay_Sess09h00','Delay_Sess13h00','Delay_Sess17h00','Delay_Sess21h00','eightsec_accuracy_Sess09h00','eightsec_accuracy_Sess13h00',\n",
    "                                         'eightsec_accuracy_Sess17h00','eightsec_accuracy_Sess21h00','FalseAlarms_Sess09h00','FalseAlarms_Sess13h00','FalseAlarms_Sess17h00',\n",
    "                                         'FalseAlarms_Sess21h00','height_Sess09h00','height_Sess13h00','height_Sess17h00','height_Sess21h00','Hits_Sess09h00','Hits_Sess13h00',\n",
    "                                         'Hits_Sess17h00','Hits_Sess21h00','live_row_Sess09h00','live_row_Sess13h00','live_row_Sess17h00','live_row_Sess21h00','logfile_Sess09h00',\n",
    "                                         'logfile_Sess13h00','logfile_Sess17h00','logfile_Sess21h00','match_1s_accuracy_Sess09h00','match_1s_accuracy_Sess13h00',\n",
    "                                         'match_1s_accuracy_Sess17h00','match_1s_accuracy_Sess21h00','match_1s_avg_rt_Sess09h00','match_1s_avg_rt_Sess13h00','match_1s_avg_rt_Sess17h00',\n",
    "                                         'match_1s_avg_rt_Sess21h00','match_8s_accuracy_Sess09h00','match_8s_accuracy_Sess13h00','match_8s_accuracy_Sess17h00',\n",
    "                                         'match_8s_accuracy_Sess21h00','match_8s_avg_rt_Sess09h00','match_8s_avg_rt_Sess13h00','match_8s_avg_rt_Sess17h00',\n",
    "                                         'match_8s_avg_rt_Sess21h00','mismatch_1s_accuracy_Sess09h00','mismatch_1s_accuracy_Sess13h00','mismatch_1s_accuracy_Sess17h00',\n",
    "                                         'mismatch_1s_accuracy_Sess21h00','mismatch_1s_avg_rt_Sess09h00','mismatch_1s_avg_rt_Sess13h00','mismatch_1s_avg_rt_Sess17h00',\n",
    "                                         'mismatch_1s_avg_rt_Sess21h00','mismatch_8s_accuracy_Sess09h00','mismatch_8s_accuracy_Sess13h00','mismatch_8s_accuracy_Sess17h00',\n",
    "                                         'mismatch_8s_accuracy_Sess21h00','mismatch_8s_avg_rt_Sess09h00','mismatch_8s_avg_rt_Sess13h00','mismatch_8s_avg_rt_Sess17h00',\n",
    "                                         'mismatch_8s_avg_rt_Sess21h00','NNonResponses_Sess09h00','NNonResponses_Sess13h00','NNonResponses_Sess17h00',\n",
    "                                         'NNonResponses_Sess21h00','Omissions_Sess09h00','Omissions_Sess13h00','Omissions_Sess17h00',\n",
    "                                         'Omissions_Sess21h00','onesec_accuracy_Sess09h00','onesec_accuracy_Sess13h00','onesec_accuracy_Sess17h00',\n",
    "                                         'onesec_accuracy_Sess21h00','QuinetteAccuracyScore_Sess09h00','QuinetteAccuracyScore_Sess13h00','QuinetteAccuracyScore_Sess17h00',\n",
    "                                         'QuinetteAccuracyScore_Sess21h00','QuinetteProcessingScore_Sess09h00','QuinetteProcessingScore_Sess13h00',\n",
    "                                         'QuinetteProcessingScore_Sess17h00','QuinetteProcessingScore_Sess21h00','response_Sess09h00',\n",
    "                                         'response_Sess13h00','response_Sess17h00','response_Sess21h00','response_time_Sess09h00','response_time_Sess13h00',\n",
    "                                         'response_time_Sess17h00','response_time_Sess21h00','ResponsesGiven_Sess09h00','ResponsesGiven_Sess13h00',\n",
    "                                         'ResponsesGiven_Sess17h00','ResponsesGiven_Sess21h00','total_correct_Sess09h00','total_correct_Sess13h00','total_correct_Sess17h00',\n",
    "                                         'total_correct_Sess21h00','total_match_1s_rt_Sess09h00','total_match_1s_rt_Sess13h00','total_match_1s_rt_Sess17h00',\n",
    "                                         'total_match_1s_rt_Sess21h00','total_match_8s_rt_Sess09h00','total_match_8s_rt_Sess13h00','total_match_8s_rt_Sess17h00',\n",
    "                                         'total_match_8s_rt_Sess21h00','total_mismatch_1s_rt_Sess09h00','total_mismatch_1s_rt_Sess13h00','total_mismatch_1s_rt_Sess17h00',\n",
    "                                         'total_mismatch_1s_rt_Sess21h00','total_mismatch_8s_rt_Sess09h00','total_mismatch_8s_rt_Sess13h00','total_mismatch_8s_rt_Sess17h00',\n",
    "                                         'total_mismatch_8s_rt_Sess21h00','total_response_time_Sess09h00','total_response_time_Sess13h00','total_response_time_Sess17h00',\n",
    "                                         'total_response_time_Sess21h00','total_responses_Sess09h00','total_responses_Sess13h00','total_responses_Sess17h00','total_responses_Sess21h00',\n",
    "                                         'width_Sess09h00','width_Sess13h00','width_Sess17h00','width_Sess21h00']]\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#'selSNr',\n",
    "df_Operation_Span_1 = df_Operation_Span_1[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'height', 'letter', 'List_Prev_Letter', 'List_responses_memory',\n",
    "     'live_row', 'logfile', 'response_average_time_memory', 'response_memory', 'response_processing',\n",
    "     'response_time_memory', 'response_time_processing', 'response_total_time_memory', 'OP_part_process_time',\n",
    "     'score_practice', 'score_operation_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4', 'score_subblock_5',\n",
    "     'score_subblock_6', 'Tipo', 'total_correct', 'total_response_time',\n",
    "     'total_responses', 'width']]\n",
    "df_Operation_Span_1[['acc', 'avg_rt']] = df_Operation_Span_1[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Operation_Span_1 = df_Operation_Span_1.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_c = df_Operation_Span_1[\"response_processing\"].iloc[2]\n",
    "df_Operation_Span_1 = df_Operation_Span_1.replace(example_c, '')\n",
    "#df_Operation_Span_1 = df_Operation_Span_1.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Operation_Span_1 = df_Operation_Span_1.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Operation_Span_1.columns = ['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco',\n",
    "                               'SubTaskName','acc_Sess09h00','avg_rt_Sess09h00','BlockChoice_Sess09h00','correct_Sess09h00',\n",
    "                               'correct_response_Sess09h00','height_Sess09h00','letter_Sess09h00','List_Prev_Letter_Sess09h00',\n",
    "                               'List_responses_memory_Sess09h00','live_row_Sess09h00','logfile_Sess09h00','response_average_time_memory_Sess09h00',\n",
    "                               'response_memory_Sess09h00','response_processing_Sess09h00','response_time_memory_Sess09h00',\n",
    "                               'response_time_processing_Sess09h00','response_total_time_memory_Sess09h00','OP_part_process_time_Sess09h00',\n",
    "                               'score_practice_Sess09h00','score_operation_span_Sess09h00','score_subblock_2_Sess09h00','score_subblock_3_Sess09h00',\n",
    "                               'score_subblock_4_Sess09h00','score_subblock_5_Sess09h00','score_subblock_6_Sess09h00','Tipo_Sess09h00',\n",
    "                               'total_correct_Sess09h00','total_response_time_Sess09h00','total_responses_Sess09h00','width_Sess09h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Operation_Span_2 = df_Operation_Span_2[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'height', 'letter', 'List_Prev_Letter', 'List_responses_memory',\n",
    "     'live_row', 'logfile', 'response_average_time_memory', 'response_memory', 'response_processing',\n",
    "     'response_time_memory', 'response_time_processing', 'response_total_time_memory', 'OP_part_process_time',\n",
    "     'score_practice', 'score_operation_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4', 'score_subblock_5',\n",
    "     'score_subblock_6', 'Tipo', 'total_correct', 'total_response_time',\n",
    "     'total_responses', 'width']]\n",
    "df_Operation_Span_2[['acc', 'avg_rt']] = df_Operation_Span_2[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Operation_Span_2 = df_Operation_Span_2.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_c = df_Operation_Span_2[\"response_processing\"].iloc[2]\n",
    "df_Operation_Span_2 = df_Operation_Span_2.replace(example_c, '')\n",
    "#df_Operation_Span_2 = df_Operation_Span_2.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Operation_Span_2 = df_Operation_Span_2.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Operation_Span_2.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Operation_Span_2.columns = ['acc_Sess13h00','avg_rt_Sess13h00','BlockChoice_Sess13h00','correct_Sess13h00',\n",
    "                               'correct_response_Sess13h00','height_Sess13h00','letter_Sess13h00','List_Prev_Letter_Sess13h00',\n",
    "                               'List_responses_memory_Sess13h00','live_row_Sess13h00','logfile_Sess13h00','response_average_time_memory_Sess13h00',\n",
    "                               'response_memory_Sess13h00','response_processing_Sess13h00','response_time_memory_Sess13h00',\n",
    "                               'response_time_processing_Sess13h00','response_total_time_memory_Sess13h00','OP_part_process_time_Sess13h00',\n",
    "                               'score_practice_Sess13h00','score_operation_span_Sess13h00','score_subblock_2_Sess13h00','score_subblock_3_Sess13h00',\n",
    "                               'score_subblock_4_Sess13h00','score_subblock_5_Sess13h00','score_subblock_6_Sess13h00','Tipo_Sess13h00',\n",
    "                               'total_correct_Sess13h00','total_response_time_Sess13h00','total_responses_Sess13h00','width_Sess13h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Operation_Span_3 = df_Operation_Span_3[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'height', 'letter', 'List_Prev_Letter', 'List_responses_memory',\n",
    "     'live_row', 'logfile', 'response_average_time_memory', 'response_memory', 'response_processing',\n",
    "     'response_time_memory', 'response_time_processing', 'response_total_time_memory', 'OP_part_process_time',\n",
    "     'score_practice', 'score_operation_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4', 'score_subblock_5',\n",
    "     'score_subblock_6', 'Tipo', 'total_correct', 'total_response_time',\n",
    "     'total_responses', 'width']]\n",
    "df_Operation_Span_3[['acc', 'avg_rt']] = df_Operation_Span_3[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Operation_Span_3 = df_Operation_Span_3.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_c = df_Operation_Span_3[\"response_processing\"].iloc[2]\n",
    "df_Operation_Span_3 = df_Operation_Span_3.replace(example_c, '')\n",
    "#df_Operation_Span_3 = df_Operation_Span_3.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Operation_Span_3 = df_Operation_Span_3.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Operation_Span_3.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Operation_Span_3.columns = ['acc_Sess17h00','avg_rt_Sess17h00','BlockChoice_Sess17h00','correct_Sess17h00',\n",
    "                               'correct_response_Sess17h00','height_Sess17h00','letter_Sess17h00','List_Prev_Letter_Sess17h00',\n",
    "                               'List_responses_memory_Sess17h00','live_row_Sess17h00','logfile_Sess17h00','response_average_time_memory_Sess17h00',\n",
    "                               'response_memory_Sess17h00','response_processing_Sess17h00','response_time_memory_Sess17h00',\n",
    "                               'response_time_processing_Sess17h00','response_total_time_memory_Sess17h00','OP_part_process_time_Sess17h00',\n",
    "                               'score_practice_Sess17h00','score_operation_span_Sess17h00','score_subblock_2_Sess17h00','score_subblock_3_Sess17h00',\n",
    "                               'score_subblock_4_Sess17h00','score_subblock_5_Sess17h00','score_subblock_6_Sess17h00','Tipo_Sess17h00',\n",
    "                               'total_correct_Sess17h00','total_response_time_Sess17h00','total_responses_Sess17h00','width_Sess17h00']\n",
    "\n",
    "#'selSNr',\n",
    "df_Operation_Span_4 = df_Operation_Span_4[\n",
    "    ['subject_nr', 'CB_ref', 'practice', 'TrialNumber', 'Sub_bloco', 'SubTaskName', 'acc', 'avg_rt',\n",
    "     'BlockChoice', 'correct', 'correct_response', 'height', 'letter', 'List_Prev_Letter', 'List_responses_memory',\n",
    "     'live_row', 'logfile', 'response_average_time_memory', 'response_memory', 'response_processing',\n",
    "     'response_time_memory', 'response_time_processing', 'response_total_time_memory', 'OP_part_process_time',\n",
    "     'score_practice', 'score_operation_span', 'score_subblock_2', 'score_subblock_3', 'score_subblock_4', 'score_subblock_5',\n",
    "     'score_subblock_6', 'Tipo', 'total_correct', 'total_response_time',\n",
    "     'total_responses', 'width']]\n",
    "df_Operation_Span_4[['acc', 'avg_rt']] = df_Operation_Span_4[['acc', 'avg_rt']].replace(',', '.')\n",
    "df_Operation_Span_4 = df_Operation_Span_4.astype(\n",
    "    {'acc': 'float64', 'avg_rt': 'float64', 'correct_response': 'str', 'response_processing': 'str'})\n",
    "example_c = df_Operation_Span_4[\"response_processing\"].iloc[2]\n",
    "df_Operation_Span_4 = df_Operation_Span_4.replace(example_c, '')\n",
    "#df_Operation_Span_4 = df_Operation_Span_4.sort_values(by=['selSNr'], kind='mergesort')\n",
    "#df_Operation_Span_4 = df_Operation_Span_4.sort_values(by=['subject_nr'], kind='mergesort')\n",
    "df_Operation_Span_4.drop(['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName'],axis=1,inplace=True)\n",
    "df_Operation_Span_4.columns = ['acc_Sess21h00','avg_rt_Sess21h00','BlockChoice_Sess21h00','correct_Sess21h00',\n",
    "                               'correct_response_Sess21h00','height_Sess21h00','letter_Sess21h00','List_Prev_Letter_Sess21h00',\n",
    "                               'List_responses_memory_Sess21h00','live_row_Sess21h00','logfile_Sess21h00','response_average_time_memory_Sess21h00',\n",
    "                               'response_memory_Sess21h00','response_processing_Sess21h00','response_time_memory_Sess21h00',\n",
    "                               'response_time_processing_Sess21h00','response_total_time_memory_Sess21h00','OP_part_process_time_Sess21h00',\n",
    "                               'score_practice_Sess21h00','score_operation_span_Sess21h00','score_subblock_2_Sess21h00','score_subblock_3_Sess21h00',\n",
    "                               'score_subblock_4_Sess21h00','score_subblock_5_Sess21h00','score_subblock_6_Sess21h00','Tipo_Sess21h00',\n",
    "                               'total_correct_Sess21h00','total_response_time_Sess21h00','total_responses_Sess21h00','width_Sess21h00']\n",
    "\n",
    "df_Operation_Span_Experimental = pd.concat([df_Operation_Span_1,df_Operation_Span_2,df_Operation_Span_3,df_Operation_Span_4],axis=1)\n",
    "\n",
    "\n",
    "df_Operation_Span_Experimental = df_Operation_Span_Experimental[['subject_nr','CB_ref','practice','TrialNumber','Sub_bloco','SubTaskName',\n",
    "                                                   'acc_Sess09h00','acc_Sess13h00','acc_Sess17h00','acc_Sess21h00','avg_rt_Sess09h00','avg_rt_Sess13h00','avg_rt_Sess17h00','avg_rt_Sess21h00',\n",
    "                                                   'BlockChoice_Sess09h00','BlockChoice_Sess13h00','BlockChoice_Sess17h00','BlockChoice_Sess21h00','correct_Sess09h00',\n",
    "                                                   'correct_Sess13h00','correct_Sess17h00','correct_Sess21h00','correct_response_Sess09h00','correct_response_Sess13h00',\n",
    "                                                   'correct_response_Sess17h00','correct_response_Sess21h00','height_Sess09h00','height_Sess13h00','height_Sess17h00','height_Sess21h00',\n",
    "                                                   'letter_Sess09h00','letter_Sess13h00','letter_Sess17h00','letter_Sess21h00','List_Prev_Letter_Sess09h00','List_Prev_Letter_Sess13h00',\n",
    "                                                   'List_Prev_Letter_Sess17h00','List_Prev_Letter_Sess21h00','List_responses_memory_Sess09h00','List_responses_memory_Sess13h00',\n",
    "                                                   'List_responses_memory_Sess17h00','List_responses_memory_Sess21h00','live_row_Sess09h00','live_row_Sess13h00','live_row_Sess17h00',\n",
    "                                                   'live_row_Sess21h00','logfile_Sess09h00','logfile_Sess13h00','logfile_Sess17h00','logfile_Sess21h00',\n",
    "                                                   'response_average_time_memory_Sess09h00','response_average_time_memory_Sess13h00','response_average_time_memory_Sess17h00',\n",
    "                                                   'response_average_time_memory_Sess21h00','response_memory_Sess09h00','response_memory_Sess13h00','response_memory_Sess17h00',\n",
    "                                                   'response_memory_Sess21h00','response_processing_Sess09h00','response_processing_Sess13h00','response_processing_Sess17h00',\n",
    "                                                   'response_processing_Sess21h00','response_time_memory_Sess09h00','response_time_memory_Sess13h00','response_time_memory_Sess17h00',\n",
    "                                                   'response_time_memory_Sess21h00','response_time_processing_Sess09h00','response_time_processing_Sess13h00',\n",
    "                                                   'response_time_processing_Sess17h00','response_time_processing_Sess21h00','response_total_time_memory_Sess09h00',\n",
    "                                                   'response_total_time_memory_Sess13h00','response_total_time_memory_Sess17h00','response_total_time_memory_Sess21h00',\n",
    "                                                   'OP_part_process_time_Sess09h00','OP_part_process_time_Sess13h00','OP_part_process_time_Sess17h00',\n",
    "                                                   'OP_part_process_time_Sess21h00','score_practice_Sess09h00','score_practice_Sess13h00','score_practice_Sess17h00',\n",
    "                                                   'score_practice_Sess21h00','score_operation_span_Sess09h00','score_operation_span_Sess13h00','score_operation_span_Sess17h00',\n",
    "                                                   'score_operation_span_Sess21h00','score_subblock_2_Sess09h00','score_subblock_2_Sess13h00','score_subblock_2_Sess17h00',\n",
    "                                                   'score_subblock_2_Sess21h00','score_subblock_3_Sess09h00','score_subblock_3_Sess13h00','score_subblock_3_Sess17h00',\n",
    "                                                   'score_subblock_3_Sess21h00','score_subblock_4_Sess09h00','score_subblock_4_Sess13h00','score_subblock_4_Sess17h00',\n",
    "                                                   'score_subblock_4_Sess21h00','score_subblock_5_Sess09h00','score_subblock_5_Sess13h00','score_subblock_5_Sess17h00',\n",
    "                                                   'score_subblock_5_Sess21h00','score_subblock_6_Sess09h00','score_subblock_6_Sess13h00','score_subblock_6_Sess17h00',\n",
    "                                                   'score_subblock_6_Sess21h00','Tipo_Sess09h00','Tipo_Sess13h00','Tipo_Sess17h00','Tipo_Sess21h00','total_correct_Sess09h00',\n",
    "                                                   'total_correct_Sess13h00','total_correct_Sess17h00','total_correct_Sess21h00','total_response_time_Sess09h00',\n",
    "                                                   'total_response_time_Sess13h00','total_response_time_Sess17h00','total_response_time_Sess21h00','total_responses_Sess09h00',\n",
    "                                                   'total_responses_Sess13h00','total_responses_Sess17h00','total_responses_Sess21h00','width_Sess09h00','width_Sess13h00',\n",
    "                                                   'width_Sess17h00','width_Sess21h00']]\n",
    "\n",
    "\n",
    "# Reset index and drop the old index column\n",
    "df_Reading_Span_Experimental = df_Reading_Span_Experimental.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all dataframe columns when printing\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the experimental Reading Span dataframe\n",
    "df_Reading_Span_Experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e0e7f",
   "metadata": {},
   "source": [
    "## 5.3. Generates database with raw scores in the five WM tasks (experimental session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb661ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to aggregate all raw scores\n",
    "df_raw_scores = pd.DataFrame()\n",
    "\n",
    "# Get unique subject IDs from the total participants table\n",
    "subj_nr = df_total_part[\"subject_nr\"].unique()\n",
    "\n",
    "# Add subject IDs as the first column of the raw scores DataFrame\n",
    "df_raw_scores.insert(0,'subject_nr',subj_nr)\n",
    "\n",
    "# Define project root and load experimental body temperature data\n",
    "PROJ_ROOT = Path(\"..\").resolve()\n",
    "excel_part_data_path = PROJ_ROOT / \"Temperature\" / \"Body_Temperature_Collection.xlsx\"\n",
    "Temperature = pd.read_excel(excel_part_data_path,sheet_name='TempExperimental')\n",
    "\n",
    "# Remove redundant subject identifier from the temperature sheet\n",
    "Temperature.drop(columns=\"Subject ID\", inplace=True)\n",
    "\n",
    "# Add 09h00 temperature values to the raw scores table\n",
    "df_raw_scores[\"Temperature (°C) Sess 09h00\"] = list(Temperature[\"Temperature (°C) Sess 09h00\"])\n",
    "\n",
    "# Add 13h00 temperature values to the raw scores table\n",
    "df_raw_scores[\"Temperature (°C) Sess 13h00\"] = list(Temperature[\"Temperature (°C) Sess 13h00\"])\n",
    "\n",
    "# Add 17h00 temperature values to the raw scores table\n",
    "df_raw_scores[\"Temperature (°C) Sess 17h00\"] = list(Temperature[\"Temperature (°C) Sess 17h00\"])\n",
    "\n",
    "# Add 21h00 temperature values to the raw scores table\n",
    "df_raw_scores[\"Temperature (°C) Sess 21h00\"] = list(Temperature[\"Temperature (°C) Sess 21h00\"])\n",
    "\n",
    "# Compute max Reading Span (09h00) per subject (scaled ×20) and store\n",
    "RawRS1 = list(df_Reading_Span_1.groupby(['subject_nr'], sort=True)['score_reading_span_Sess09h00'].max() * 20)\n",
    "\n",
    "# Insert Reading Span 09h00 scores into the aggregate table\n",
    "df_raw_scores[\"Reading Span Session 09h00\"] = RawRS1\n",
    "\n",
    "# Define metadata columns to mirror into subsequent Reading Span DataFrames\n",
    "cols_to_add = [\n",
    "    'subject_nr', 'CB_ref', 'practice', \n",
    "    'TrialNumber', 'Sub_bloco', 'SubTaskName'\n",
    "]\n",
    "\n",
    "# Copy metadata columns from RS1 to RS2 before computing RS2 scores\n",
    "for col in cols_to_add:\n",
    "    df_Reading_Span_2.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Reading_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Reading Span (13h00) per subject (scaled ×20) and store\n",
    "RawRS2 = list(df_Reading_Span_2.groupby(['subject_nr'], sort=True)['score_reading_span_Sess13h00'].max() * 20)\n",
    "\n",
    "# Insert Reading Span 13h00 scores into the aggregate table\n",
    "df_raw_scores[\"Reading Span Session 13h00\"] = RawRS2\n",
    "\n",
    "# Reuse the same metadata set for RS3\n",
    "cols_to_add = [\n",
    "    'subject_nr', 'CB_ref', 'practice', \n",
    "    'TrialNumber', 'Sub_bloco', 'SubTaskName'\n",
    "]\n",
    "\n",
    "# Copy metadata columns from RS1 to RS3 before computing RS3 scores\n",
    "for col in cols_to_add:\n",
    "    df_Reading_Span_3.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Reading_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Reading Span (17h00) per subject (scaled ×20) and store\n",
    "RawRS3 = list(df_Reading_Span_3.groupby(['subject_nr'], sort=True)['score_reading_span_Sess17h00'].max() * 20)\n",
    "\n",
    "# Insert Reading Span 17h00 scores into the aggregate table\n",
    "df_raw_scores[\"Reading Span Session 17h00\"] = RawRS3\n",
    "\n",
    "# Reuse the same metadata set for RS4\n",
    "cols_to_add = [\n",
    "    'subject_nr', 'CB_ref', 'practice', \n",
    "    'TrialNumber', 'Sub_bloco', 'SubTaskName'\n",
    "]\n",
    "\n",
    "# Copy metadata columns from RS1 to RS4 before computing RS4 scores\n",
    "for col in cols_to_add:\n",
    "    df_Reading_Span_4.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Reading_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Reading Span (21h00) per subject (scaled ×20) and store\n",
    "RawRS4 = list(df_Reading_Span_4.groupby(['subject_nr'], sort=True)['score_reading_span_Sess21h00'].max() * 20)\n",
    "\n",
    "# Insert Reading Span 21h00 scores into the aggregate table\n",
    "df_raw_scores[\"Reading Span Session 21h00\"] = RawRS4\n",
    "\n",
    "# Compute max Updating Task (09h00) per subject and store\n",
    "RawUT1 = list(df_WMU_Task_1.groupby(['subject_nr'], sort=True)['WMUExperimentalScore_Sess09h00'].max())\n",
    "\n",
    "# Insert Updating Task 09h00 scores into the aggregate table\n",
    "df_raw_scores[\"Updating Task Session 09h00\"] = RawUT1\n",
    "\n",
    "# Define minimal metadata to mirror for WMU tasks\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from WMU1 to WMU2 before computing WMU2 scores\n",
    "for col in cols_to_add:\n",
    "    df_WMU_Task_2.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_WMU_Task_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Updating Task (13h00) per subject and store\n",
    "RawUT2 = list(df_WMU_Task_2.groupby(['subject_nr'], sort=True)['WMUExperimentalScore_Sess13h00'].max())\n",
    "\n",
    "# Insert Updating Task 13h00 scores into the aggregate table\n",
    "df_raw_scores[\"Updating Task Session 13h00\"] = RawUT2\n",
    "\n",
    "# Reuse subject ID mirroring for WMU3\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from WMU1 to WMU3 before computing WMU3 scores\n",
    "for col in cols_to_add:\n",
    "    df_WMU_Task_3.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_WMU_Task_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Updating Task (17h00) per subject and store\n",
    "RawUT3 = list(df_WMU_Task_3.groupby(['subject_nr'], sort=True)['WMUExperimentalScore_Sess17h00'].max())\n",
    "\n",
    "# Insert Updating Task 17h00 scores into the aggregate table\n",
    "df_raw_scores[\"Updating Task Session 17h00\"] = RawUT3\n",
    "\n",
    "# Reuse subject ID mirroring for WMU4\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from WMU1 to WMU4 before computing WMU4 scores\n",
    "for col in cols_to_add:\n",
    "    df_WMU_Task_4.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_WMU_Task_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Updating Task (21h00) per subject and store\n",
    "RawUT4 = list(df_WMU_Task_4.groupby(['subject_nr'], sort=True)['WMUExperimentalScore_Sess21h00'].max())\n",
    "\n",
    "# Insert Updating Task 21h00 scores into the aggregate table\n",
    "df_raw_scores[\"Updating Task Session 21h00\"] = RawUT4\n",
    "\n",
    "# Compute max Symmetry Span (09h00) per subject (scaled ×20) and store\n",
    "RawSS1 = list(df_Symmetry_Span_1.groupby(['subject_nr'], sort=True)['score_symmetry_span_Sess09h00'].max() * 20)\n",
    "\n",
    "# Insert Symmetry Span 09h00 scores into the aggregate table\n",
    "df_raw_scores[\"Symmetry Span Session 09h00\"] = RawSS1\n",
    "\n",
    "# Define minimal metadata to mirror for Symmetry Span tasks\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from SS1 to SS2 before computing SS2 scores\n",
    "for col in cols_to_add:\n",
    "    df_Symmetry_Span_2.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Symmetry_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Symmetry Span (13h00) per subject (scaled ×20) and store\n",
    "RawSS2 = list(df_Symmetry_Span_2.groupby(['subject_nr'], sort=True)['score_symmetry_span_Sess13h00'].max() * 20)\n",
    "\n",
    "# Insert Symmetry Span 13h00 scores into the aggregate table\n",
    "df_raw_scores[\"Symmetry Span Session 13h00\"] = RawSS2\n",
    "\n",
    "# Reuse subject ID mirroring for SS3\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from SS1 to SS3 before computing SS3 scores\n",
    "for col in cols_to_add:\n",
    "    df_Symmetry_Span_3.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Symmetry_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Symmetry Span (17h00) per subject (scaled ×20) and store\n",
    "RawSS3 = list(df_Symmetry_Span_3.groupby(['subject_nr'], sort=True)['score_symmetry_span_Sess17h00'].max() * 20)\n",
    "\n",
    "# Insert Symmetry Span 17h00 scores into the aggregate table\n",
    "df_raw_scores[\"Symmetry Span Session 17h00\"] = RawSS3\n",
    "\n",
    "# Reuse subject ID mirroring for SS4\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from SS1 to SS4 before computing SS4 scores\n",
    "for col in cols_to_add:\n",
    "    df_Symmetry_Span_4.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Symmetry_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Symmetry Span (21h00) per subject (scaled ×20) and store\n",
    "RawSS4 = list(df_Symmetry_Span_4.groupby(['subject_nr'], sort=True)['score_symmetry_span_Sess21h00'].max() * 20)\n",
    "\n",
    "# Insert Symmetry Span 21h00 scores into the aggregate table\n",
    "df_raw_scores[\"Symmetry Span Session 21h00\"] = RawSS4\n",
    "\n",
    "# Compute max Binding Task (09h00) per subject and store\n",
    "RawBT1 = list(df_Binding_Task_1.groupby(['subject_nr'], sort=True)['BindingRawScore_Sess09h00'].max())\n",
    "\n",
    "# Insert Binding Task 09h00 scores into the aggregate table\n",
    "df_raw_scores[\"Binding Task Session 09h00\"] = RawBT1\n",
    "\n",
    "# Define minimal metadata to mirror for Binding tasks\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from BT1 to BT2 before computing BT2 scores\n",
    "for col in cols_to_add:\n",
    "    df_Binding_Task_2.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Binding_Task_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Binding Task (13h00) per subject and store\n",
    "RawBT2 = list(df_Binding_Task_2.groupby(['subject_nr'], sort=True)['BindingRawScore_Sess13h00'].max())\n",
    "\n",
    "# Insert Binding Task 13h00 scores into the aggregate table\n",
    "df_raw_scores[\"Binding Task Session 13h00\"] = RawBT2\n",
    "\n",
    "# Reuse subject ID mirroring for BT3\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from BT1 to BT3 before computing BT3 scores\n",
    "for col in cols_to_add:\n",
    "    df_Binding_Task_3.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Binding_Task_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Binding Task (17h00) per subject and store\n",
    "RawBT3 = list(df_Binding_Task_3.groupby(['subject_nr'], sort=True)['BindingRawScore_Sess17h00'].max())\n",
    "\n",
    "# Insert Binding Task 17h00 scores into the aggregate table\n",
    "df_raw_scores[\"Binding Task Session 17h00\"] = RawBT3\n",
    "\n",
    "# Reuse subject ID mirroring for BT4\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from BT1 to BT4 before computing BT4 scores\n",
    "for col in cols_to_add:\n",
    "    df_Binding_Task_4.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Binding_Task_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Binding Task (21h00) per subject and store\n",
    "RawBT4 = list(df_Binding_Task_4.groupby(['subject_nr'], sort=True)['BindingRawScore_Sess21h00'].max())\n",
    "\n",
    "# Insert Binding Task 21h00 scores into the aggregate table\n",
    "df_raw_scores[\"Binding Task Session 21h00\"] = RawBT4\n",
    "\n",
    "# Compute max Operation Span (09h00) per subject (scaled ×20) and store\n",
    "RawOS1 = list(df_Operation_Span_1.groupby(['subject_nr'], sort=True)['score_operation_span_Sess09h00'].max() * 20)\n",
    "\n",
    "# Insert Operation Span 09h00 scores into the aggregate table\n",
    "df_raw_scores[\"Operation Span Session 09h00\"] = RawOS1\n",
    "\n",
    "# Define minimal metadata to mirror for Operation Span tasks\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from OS1 to OS2 before computing OS2 scores\n",
    "for col in cols_to_add:\n",
    "    df_Operation_Span_2.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Operation_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Operation Span (13h00) per subject (scaled ×20) and store\n",
    "RawOS2 = list(df_Operation_Span_2.groupby(['subject_nr'], sort=True)['score_operation_span_Sess13h00'].max() * 20)\n",
    "\n",
    "# Insert Operation Span 13h00 scores into the aggregate table\n",
    "df_raw_scores[\"Operation Span Session 13h00\"] = RawOS2\n",
    "\n",
    "# Reuse subject ID mirroring for OS3\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from OS1 to OS3 before computing OS3 scores\n",
    "for col in cols_to_add:\n",
    "    df_Operation_Span_3.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Operation_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Operation Span (17h00) per subject (scaled ×20) and store\n",
    "RawOS3 = list(df_Operation_Span_3.groupby(['subject_nr'], sort=True)['score_operation_span_Sess17h00'].max() * 20)\n",
    "\n",
    "# Insert Operation Span 17h00 scores into the aggregate table\n",
    "df_raw_scores[\"Operation Span Session 17h00\"] = RawOS3\n",
    "\n",
    "# Reuse subject ID mirroring for OS4\n",
    "cols_to_add = [\n",
    "    'subject_nr'\n",
    "]\n",
    "\n",
    "# Copy subject IDs from OS1 to OS4 before computing OS4 scores\n",
    "for col in cols_to_add:\n",
    "    df_Operation_Span_4.insert(\n",
    "        loc=0,                          # insert at the beginning\n",
    "        column=col, \n",
    "        value=df_Operation_Span_1[col].values\n",
    "    )\n",
    "\n",
    "# Compute max Operation Span (21h00) per subject (scaled ×20) and store\n",
    "RawOS4 = list(df_Operation_Span_4.groupby(['subject_nr'], sort=True)['score_operation_span_Sess21h00'].max() * 20)\n",
    "\n",
    "# Insert Operation Span 21h00 scores into the aggregate table\n",
    "df_raw_scores[\"Operation Span Session 21h00\"] = RawOS4\n",
    "\n",
    "# Determine each subject’s counterbalancing order based on ID ranges/specific IDs\n",
    "list_counterbalancing = []\n",
    "for i in range(0,len(df_raw_scores)):\n",
    "    if 1 <= df_raw_scores.loc[i,'subject_nr'] <= 5 or df_raw_scores.loc[i,'subject_nr'] == 21 or df_raw_scores.loc[i,'subject_nr'] == 25:\n",
    "        aaa = '09h00,13h00,17h00,21h00'\n",
    "        list_counterbalancing.append(aaa)\n",
    "    elif 6 <= df_raw_scores.loc[i,'subject_nr'] <= 10 or df_raw_scores.loc[i,'subject_nr'] == 22 or df_raw_scores.loc[i,'subject_nr'] == 26:\n",
    "        aaa = '13h00,17h00,21h00,09h00'\n",
    "        list_counterbalancing.append(aaa)\n",
    "    elif 11 <= df_raw_scores.loc[i,'subject_nr'] <= 15 or df_raw_scores.loc[i,'subject_nr'] == 23  or df_raw_scores.loc[i,'subject_nr'] == 27:\n",
    "        aaa = '17h00,21h00,09h00,13h00'\n",
    "        list_counterbalancing.append(aaa)\n",
    "    elif 16 <= df_raw_scores.loc[i,'subject_nr'] <= 20 or df_raw_scores.loc[i,'subject_nr'] == 24  or df_raw_scores.loc[i,'subject_nr'] == 28:\n",
    "        aaa = '21h00,09h00,13h00,17h00'\n",
    "        list_counterbalancing.append(aaa)\n",
    "\n",
    "# Insert the derived counterbalancing sequence as the second column\n",
    "df_raw_scores.insert(1, 'Counterbalancing', list_counterbalancing)\n",
    "\n",
    "# Cast all non-ID, non-text, non-temperature columns to integer type\n",
    "df_raw_scores = df_raw_scores.astype({\n",
    "    col: 'int' \n",
    "    for col in df_raw_scores.columns \n",
    "    if col not in ['subject_nr', 'Counterbalancing','Temperature (°C) Sess 09h00','Temperature (°C) Sess 13h00','Temperature (°C) Sess 17h00',\n",
    "                   'Temperature (°C) Sess 21h00']\n",
    "})\n",
    "\n",
    "# Sort the final experimental table by subject ID\n",
    "df_raw_scores_exper = df_raw_scores.sort_values(by=\"subject_nr\")\n",
    "\n",
    "# Display the sorted table\n",
    "df_raw_scores_exper\n",
    "\n",
    "# Ensure all columns are visible when displaying DataFrames\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the sorted table again with the updated display option\n",
    "df_raw_scores_exper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207c787",
   "metadata": {},
   "source": [
    "## 5.4. Generates database with normalized scores in the five WM tasks (experimental session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c51a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a deep copy of the raw scores table to store normalized values\n",
    "df_normalized_scores_exper = df_raw_scores_exper.copy(deep=True)\n",
    "\n",
    "# Normalize Reading Span 09h00 scores by dividing by maximum possible value\n",
    "df_normalized_scores_exper[\"Reading Span Session 09h00\"] = df_normalized_scores_exper[\"Reading Span Session 09h00\"]/20\n",
    "# Normalize Reading Span 13h00 scores\n",
    "df_normalized_scores_exper[\"Reading Span Session 13h00\"] = df_normalized_scores_exper[\"Reading Span Session 13h00\"]/20\n",
    "# Normalize Reading Span 17h00 scores\n",
    "df_normalized_scores_exper[\"Reading Span Session 17h00\"] = df_normalized_scores_exper[\"Reading Span Session 17h00\"]/20\n",
    "# Normalize Reading Span 21h00 scores\n",
    "df_normalized_scores_exper[\"Reading Span Session 21h00\"] = df_normalized_scores_exper[\"Reading Span Session 21h00\"]/20\n",
    "\n",
    "# Normalize Updating Task 09h00 scores by dividing by maximum possible value\n",
    "df_normalized_scores_exper[\"Updating Task Session 09h00\"] = df_normalized_scores_exper[\"Updating Task Session 09h00\"] / 36\n",
    "# Normalize Updating Task 13h00 scores\n",
    "df_normalized_scores_exper[\"Updating Task Session 13h00\"] = df_normalized_scores_exper[\"Updating Task Session 13h00\"] / 36\n",
    "# Normalize Updating Task 17h00 scores\n",
    "df_normalized_scores_exper[\"Updating Task Session 17h00\"] = df_normalized_scores_exper[\"Updating Task Session 17h00\"] / 36\n",
    "# Normalize Updating Task 21h00 scores\n",
    "df_normalized_scores_exper[\"Updating Task Session 21h00\"] = df_normalized_scores_exper[\"Updating Task Session 21h00\"] / 36\n",
    "\n",
    "# Normalize Symmetry Span 09h00 scores by dividing by maximum possible value\n",
    "df_normalized_scores_exper[\"Symmetry Span Session 09h00\"] = df_normalized_scores_exper[\"Symmetry Span Session 09h00\"] / 20\n",
    "# Normalize Symmetry Span 13h00 scores\n",
    "df_normalized_scores_exper[\"Symmetry Span Session 13h00\"] = df_normalized_scores_exper[\"Symmetry Span Session 13h00\"] / 20\n",
    "# Normalize Symmetry Span 17h00 scores\n",
    "df_normalized_scores_exper[\"Symmetry Span Session 17h00\"] = df_normalized_scores_exper[\"Symmetry Span Session 17h00\"] / 20\n",
    "# Normalize Symmetry Span 21h00 scores\n",
    "df_normalized_scores_exper[\"Symmetry Span Session 21h00\"] = df_normalized_scores_exper[\"Symmetry Span Session 21h00\"] / 20\n",
    "\n",
    "# Normalize Binding Task 09h00 scores by dividing by maximum possible value\n",
    "df_normalized_scores_exper[\"Binding Task Session 09h00\"] = df_normalized_scores_exper[\"Binding Task Session 09h00\"] / 12\n",
    "# Normalize Binding Task 13h00 scores\n",
    "df_normalized_scores_exper[\"Binding Task Session 13h00\"] = df_normalized_scores_exper[\"Binding Task Session 13h00\"] / 12\n",
    "# Normalize Binding Task 17h00 scores\n",
    "df_normalized_scores_exper[\"Binding Task Session 17h00\"] = df_normalized_scores_exper[\"Binding Task Session 17h00\"] / 12\n",
    "# Normalize Binding Task 21h00 scores\n",
    "df_normalized_scores_exper[\"Binding Task Session 21h00\"] = df_normalized_scores_exper[\"Binding Task Session 21h00\"] / 12\n",
    "\n",
    "# Normalize Operation Span 09h00 scores by dividing by maximum possible value\n",
    "df_normalized_scores_exper[\"Operation Span Session 09h00\"] = df_normalized_scores_exper[\"Operation Span Session 09h00\"] / 20\n",
    "# Normalize Operation Span 13h00 scores\n",
    "df_normalized_scores_exper[\"Operation Span Session 13h00\"] = df_normalized_scores_exper[\"Operation Span Session 13h00\"] / 20\n",
    "# Normalize Operation Span 17h00 scores\n",
    "df_normalized_scores_exper[\"Operation Span Session 17h00\"] = df_normalized_scores_exper[\"Operation Span Session 17h00\"] / 20\n",
    "# Normalize Operation Span 21h00 scores\n",
    "df_normalized_scores_exper[\"Operation Span Session 21h00\"] = df_normalized_scores_exper[\"Operation Span Session 21h00\"] / 20\n",
    "\n",
    "# List of normalized score columns that need to be rounded\n",
    "cols_to_round = [\n",
    "    \"Reading Span Session 09h00\",\n",
    "    \"Reading Span Session 13h00\",\n",
    "    \"Reading Span Session 17h00\",\n",
    "    \"Reading Span Session 21h00\",\n",
    "    \"Updating Task Session 09h00\",\n",
    "    \"Updating Task Session 13h00\",\n",
    "    \"Updating Task Session 17h00\",\n",
    "    \"Updating Task Session 21h00\",\n",
    "    \"Symmetry Span Session 09h00\",\n",
    "    \"Symmetry Span Session 13h00\",\n",
    "    \"Symmetry Span Session 17h00\",\n",
    "    \"Symmetry Span Session 21h00\",\n",
    "    \"Binding Task Session 09h00\",\n",
    "    \"Binding Task Session 13h00\",\n",
    "    \"Binding Task Session 17h00\",\n",
    "    \"Binding Task Session 21h00\",\n",
    "    \"Operation Span Session 09h00\",\n",
    "    \"Operation Span Session 13h00\",\n",
    "    \"Operation Span Session 17h00\",\n",
    "    \"Operation Span Session 21h00\",\n",
    "]\n",
    "\n",
    "# Convert selected columns to numeric and round them to two decimals\n",
    "df_normalized_scores_exper[cols_to_round] = (\n",
    "    df_normalized_scores_exper[cols_to_round]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")  # safely convert to numeric\n",
    "    .round(2)  # round to 2 decimal places\n",
    ")\n",
    "\n",
    "# Ensure full column visibility when displaying DataFrames\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the final normalized table\n",
    "df_normalized_scores_exper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c29f1",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Viewing and/or downloading the final processed databases\n",
    "Downloading of viewing the final processed DBs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986fef6",
   "metadata": {},
   "source": [
    "## 6.1. Downloading the final processed databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FileLink to allow generating a downloadable link for the Excel file\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a multi-sheet Excel file and write all datasets into separate sheets\n",
    "with pd.ExcelWriter('BD_all_data_combined.xlsx') as writer:\n",
    "    df_excel_Screening.to_excel(writer, sheet_name='Screening_Part', index=False)\n",
    "    SD_df_excel_data_part.to_excel(writer, sheet_name='Sleep Diary', index=False)\n",
    "    AD_df_excel_data_part.to_excel(writer, sheet_name='Activity Diary', index=False)\n",
    "    df_Actigraphy.to_excel(writer,sheet_name=\"Actigraphy\", index=False)\n",
    "    df_raw_scores_exper.to_excel(writer, sheet_name='Raw Scores', index=False)\n",
    "    df_normalized_scores_exper.to_excel(writer, sheet_name='Normalized Scores', index=False)\n",
    "    df_Reading_Span_Experimental.to_excel(writer, sheet_name='Reading Span', index=False)\n",
    "    df_WMU_Task_Experimental.to_excel(writer, sheet_name='Updating Task', index=False)\n",
    "    df_Symmetry_Span_Experimental.to_excel(writer, sheet_name='Symmetry Span', index=False)\n",
    "    df_Binding_Task_Experimental.to_excel(writer, sheet_name='Binding Task', index=False)\n",
    "    df_Operation_Span_Experimental.to_excel(writer, sheet_name='Operation Span', index=False)\n",
    "    df_raw_scores_pract.to_excel(writer, sheet_name='Practice Raw Scores', index=False)\n",
    "    df_normalized_scores_pract.to_excel(writer, sheet_name='Practice Normalized Scores', index=False)\n",
    "    df_Reading_Span_Practice.to_excel(writer, sheet_name='Practice RS', index=False)\n",
    "    df_WMU_Task_Practice.to_excel(writer, sheet_name='Practice UT', index=False)\n",
    "    df_Symmetry_Span_Practice.to_excel(writer, sheet_name='Practice SS', index=False)\n",
    "    df_Binding_Task_Practice.to_excel(writer, sheet_name='Practice BT', index=False)\n",
    "    df_Operation_Span_Practice.to_excel(writer, sheet_name='Practice OS', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94864f60",
   "metadata": {},
   "source": [
    "## 6.2. Viewing the final processed databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12b9e8",
   "metadata": {},
   "source": [
    "### 6.2.1 Screening database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df_excel_Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c25942",
   "metadata": {},
   "source": [
    "### 6.2.2. Sleep diary database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45024f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "SD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96291799",
   "metadata": {},
   "source": [
    "### 6.2.3. Activity diary database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "AD_df_excel_data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec074fec",
   "metadata": {},
   "source": [
    "### 6.2.4. Actigraphy database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08958d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df_Actigraphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66ce4e",
   "metadata": {},
   "source": [
    "### 6.2.5. WM tasks raw scores experimental sessions database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df_raw_scores_exper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bc14f",
   "metadata": {},
   "source": [
    "### 6.2.6. WM tasks normalized scores experimental sessions database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df_normalized_scores_exper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601ca9c",
   "metadata": {},
   "source": [
    "### 6.2.7. WM tasks raw scores practice sessions database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df_raw_scores_pract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f187e9",
   "metadata": {},
   "source": [
    "### 6.2.8. WM tasks normalized scores practice sessions database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "df_normalized_scores_pract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
